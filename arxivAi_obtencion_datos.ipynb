{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRP-tSaD4sml",
        "outputId": "178dcec5-2da2-4a2f-e479-cbeffd8e7cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creacion de dataset y obtencion de datos\n"
      ],
      "metadata": {
        "id": "wESeph0T6N-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.indexing import item_from_zerodim\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "import json,re\n",
        "\n",
        "# Ruta de la carpeta principal\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "lista_part=[\"dev/parsed_pdfs/\",\"test/parsed_pdfs/\",\"train/parsed_pdfs/\"]\n",
        "lista_conf=[\"arxiv.cs.ai_2007-2017\"]\n",
        "rutas_archivos_json = []\n",
        "secc = {}\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    #texto = re.sub(r'\\n\\d+\\n', '\\n', texto)\n",
        "    #texto = re.sub(r'[×/]', '', texto)\n",
        "    #texto = re.sub(r'\\b\\d+\\b', '', texto)\n",
        "    #texto = re.sub(r'\\n+', '\\n', texto).strip()\n",
        "    # Reemplazar caracteres problemáticos con un espacio en blanco\n",
        "    #texto_limpio = ''.join(char if ord(char) < 128 else ' ' for char in texto)\n",
        "    #return texto_limpio\n",
        "    # Reemplazar caracteres problemáticos con un espacio en blanco\n",
        "    texto_limpio = ''.join(char if char.isprintable() else ' ' for char in texto)\n",
        "    return texto_limpio\n",
        "\n",
        "\n",
        "intro_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(introduction)', re.IGNORECASE)\n",
        "conclusion_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(conclusion)', re.IGNORECASE)\n",
        "\n",
        "for confe in lista_conf:\n",
        "  print(confe)\n",
        "  for particion in lista_part:\n",
        "    #print(particion)\n",
        "    ruta=(ruta_carpeta_principal+\"/\"+confe+\"/\"+particion)\n",
        "    #print(\"Explorando ruta:\", ruta)\n",
        "    archivos=glob.glob(ruta+\"/*.json\")\n",
        "    #print(archivos)\n",
        "    particion_sin_ruta = particion.strip('/').split('/')[0]\n",
        "\n",
        "    for archivo in archivos:\n",
        "      with open(archivo, 'r') as f:\n",
        "        contenido = json.load(f)\n",
        "        #print(contenido.keys())\n",
        "        nombre=contenido[\"name\"]\n",
        "        diccontent=contenido[\"metadata\"]\n",
        "        #print(diccontent)\n",
        "        t = diccontent[\"title\"]\n",
        "        #print(t)\n",
        "        e = diccontent[\"emails\"]\n",
        "        #print(e)\n",
        "        autores=diccontent[\"authors\"]\n",
        "        #print(autores)\n",
        "        dicsecciones=diccontent[\"sections\"]\n",
        "\n",
        "        referencias_archivo = diccontent[\"references\"]\n",
        "        c = 0\n",
        "        abstract= diccontent[\"abstractText\"]\n",
        "        #print(dicsecciones)\n",
        "        cadena=\"\"\n",
        "        introduccion=\"\"\n",
        "        conclusion=\"\"\n",
        "        contenido=\"\"\n",
        "\n",
        "\n",
        "        #print(dicsecciones)\n",
        "        if dicsecciones!=None:\n",
        "\n",
        "          for item in dicsecciones:\n",
        "            if item['heading'] == None:\n",
        "                cadena = \"\"\n",
        "                text=\"\"\n",
        "            else:\n",
        "                cadena = cadena + item['heading'] + \"\\n\"\n",
        "                cadena = limpiar_texto(cadena) + \"\"\n",
        "\n",
        "                # Verificar si el heading coincide con el patron de introduccion y concllu\n",
        "                if re.search(intro_pattern, item['heading']):\n",
        "                  introduccion = limpiar_texto(item['text'])\n",
        "                elif re.search(conclusion_pattern, item['heading']):\n",
        "                  conclusion = limpiar_texto(item['text'])\n",
        "                else:\n",
        "                  text= limpiar_texto(item['text'])\n",
        "                  contenido += f\"**{item['heading']}**\\n\"+ text+ \"\\n\"\n",
        "\n",
        "            texto= item['text'] #re.sub(r'(\\n\\d+)+\\n', '',item['text'])\n",
        "            texto = limpiar_texto(texto)\n",
        "            cadena=cadena+texto+\"\\n\"\n",
        "\n",
        "          rutas_archivos_json.append({\n",
        "                      \"Conferencia\":confe,\n",
        "                      \"Particion\":particion_sin_ruta,\n",
        "                      \"Archivo\": nombre,\n",
        "                      \"Title\": t,\n",
        "                      \"Emails\": e,\n",
        "                      \"Autores\": autores,\n",
        "                      \"Texto Completo\": cadena,\n",
        "                      \"Abstract\": abstract,\n",
        "                      \"Introducción\": introduccion,\n",
        "                      \"Contenido\": contenido,\n",
        "                      \"Conclusion\": conclusion,\n",
        "                      \"Referencias\": referencias_archivo\n",
        "                  })\n",
        "\n",
        "        else:\n",
        "          print(\"Archivo \"+nombre +\" sin contenido\")\n",
        "\n",
        "          rutas_archivos_json.append({\n",
        "                      \"Conferencia\":confe,\n",
        "                      \"Particion\":particion_sin_ruta,\n",
        "                      \"Archivo\": nombre,\n",
        "                      \"Title\": t,\n",
        "                      \"Emails\": e,\n",
        "                      \"Autores\": autores,\n",
        "                      \"Texto Completo\": \"\",\n",
        "                      \"Abstract\": abstract,\n",
        "                      \"Introducción\": \"\",\n",
        "                      \"Contenido\": \"\",\n",
        "                      \"Conclusion\": \"\",\n",
        "                      \"Referencias\": referencias_archivo\n",
        "                  })\n",
        "\n",
        "\n",
        "df = pd.DataFrame(rutas_archivos_json)\n",
        "df.to_csv(ruta_carpeta_principal+\"/\"+\"arxivAi_primer_ciclo.csv\", sep=',', index=False, encoding='utf-8-sig')\n",
        "print(\"Datos guardados en CSV:\", ruta_carpeta_principal+\"/\"+\"arxivAi_primer_ciclo.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UN1mqL4J5qph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2e3d3f-19d7-44f0-c6d5-093396490783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arxiv.cs.ai_2007-2017\n",
            "Archivo 1302.1525.pdf sin contenido\n",
            "Archivo 1302.6791.pdf sin contenido\n",
            "Archivo 1302.1557.pdf sin contenido\n",
            "Archivo 1302.1574.pdf sin contenido\n",
            "Archivo 1206.5266.pdf sin contenido\n",
            "Archivo 1301.2296.pdf sin contenido\n",
            "Archivo 1301.2289.pdf sin contenido\n",
            "Archivo 1302.1540.pdf sin contenido\n",
            "Archivo 1301.2295.pdf sin contenido\n",
            "Archivo 1301.2312.pdf sin contenido\n",
            "Archivo 1206.5294.pdf sin contenido\n",
            "Archivo 1301.2305.pdf sin contenido\n",
            "Archivo 1206.5249.pdf sin contenido\n",
            "Archivo 1302.1551.pdf sin contenido\n",
            "Archivo 1302.6823.pdf sin contenido\n",
            "Archivo 1302.6802.pdf sin contenido\n",
            "Archivo 1302.6826.pdf sin contenido\n",
            "Archivo 1301.2281.pdf sin contenido\n",
            "Archivo 1302.6815.pdf sin contenido\n",
            "Archivo 1302.1559.pdf sin contenido\n",
            "Archivo 1301.2306.pdf sin contenido\n",
            "Archivo 1301.2313.pdf sin contenido\n",
            "Archivo 1302.1523.pdf sin contenido\n",
            "Archivo 1301.2256.pdf sin contenido\n",
            "Archivo 1708.06816.pdf sin contenido\n",
            "Archivo 1302.6818.pdf sin contenido\n",
            "Archivo 1301.2287.pdf sin contenido\n",
            "Archivo 1301.2288.pdf sin contenido\n",
            "Archivo 1302.6821.pdf sin contenido\n",
            "Archivo 1206.5258.pdf sin contenido\n",
            "Archivo 1302.1558.pdf sin contenido\n",
            "Archivo 1302.1567.pdf sin contenido\n",
            "Archivo 1301.2273.pdf sin contenido\n",
            "Archivo 1302.6794.pdf sin contenido\n",
            "Archivo 1302.6842.pdf sin contenido\n",
            "Archivo 1302.6839.pdf sin contenido\n",
            "Archivo 1509.05463.pdf sin contenido\n",
            "Archivo 1301.2279.pdf sin contenido\n",
            "Archivo 1301.2290.pdf sin contenido\n",
            "Archivo 1302.6833.pdf sin contenido\n",
            "Archivo 1302.1524.pdf sin contenido\n",
            "Archivo 1302.6800.pdf sin contenido\n",
            "Archivo 1302.6855.pdf sin contenido\n",
            "Archivo 1302.6792.pdf sin contenido\n",
            "Archivo 1302.6796.pdf sin contenido\n",
            "Archivo 1302.6829.pdf sin contenido\n",
            "Archivo 1302.6812.pdf sin contenido\n",
            "Archivo 1603.07442.pdf sin contenido\n",
            "Archivo 1203.4855.pdf sin contenido\n",
            "Archivo 1302.6832.pdf sin contenido\n",
            "Archivo 1302.1560.pdf sin contenido\n",
            "Archivo 1302.6779.pdf sin contenido\n",
            "Archivo 1206.5268.pdf sin contenido\n",
            "Archivo 1302.6781.pdf sin contenido\n",
            "Archivo 1609.08925.pdf sin contenido\n",
            "Archivo 1206.5253.pdf sin contenido\n",
            "Archivo 1302.6811.pdf sin contenido\n",
            "Archivo 1301.2263.pdf sin contenido\n",
            "Archivo 1302.6827.pdf sin contenido\n",
            "Archivo 1302.6801.pdf sin contenido\n",
            "Archivo 1302.6807.pdf sin contenido\n",
            "Archivo 1206.5288.pdf sin contenido\n",
            "Archivo 1301.2291.pdf sin contenido\n",
            "Archivo 1302.6830.pdf sin contenido\n",
            "Archivo 1302.6850.pdf sin contenido\n",
            "Archivo 1206.5257.pdf sin contenido\n",
            "Archivo 1206.5287.pdf sin contenido\n",
            "Archivo 1302.6810.pdf sin contenido\n",
            "Archivo 1206.5273.pdf sin contenido\n",
            "Archivo 1302.6797.pdf sin contenido\n",
            "Archivo 1302.1534.pdf sin contenido\n",
            "Archivo 1302.6844.pdf sin contenido\n",
            "Archivo 1302.1530.pdf sin contenido\n",
            "Archivo 1301.2298.pdf sin contenido\n",
            "Archivo 1302.6782.pdf sin contenido\n",
            "Archivo 1302.6847.pdf sin contenido\n",
            "Archivo 1302.1548.pdf sin contenido\n",
            "Archivo 1206.5271.pdf sin contenido\n",
            "Archivo 1302.6805.pdf sin contenido\n",
            "Archivo 1206.5279.pdf sin contenido\n",
            "Archivo 1302.6831.pdf sin contenido\n",
            "Archivo 1301.2304.pdf sin contenido\n",
            "Archivo 1301.2272.pdf sin contenido\n",
            "Archivo 1206.5295.pdf sin contenido\n",
            "Archivo 1302.6795.pdf sin contenido\n",
            "Archivo 1302.6824.pdf sin contenido\n",
            "Archivo 1302.6809.pdf sin contenido\n",
            "Archivo 1302.1541.pdf sin contenido\n",
            "Archivo 1302.1555.pdf sin contenido\n",
            "Archivo 1302.6835.pdf sin contenido\n",
            "Archivo 1301.2265.pdf sin contenido\n",
            "Archivo 1302.6780.pdf sin contenido\n",
            "Archivo 1302.1544.pdf sin contenido\n",
            "Archivo 1302.1573.pdf sin contenido\n",
            "Archivo 1301.2297.pdf sin contenido\n",
            "Archivo 1206.5242.pdf sin contenido\n",
            "Archivo 1301.2282.pdf sin contenido\n",
            "Archivo 1301.2271.pdf sin contenido\n",
            "Archivo 1301.2314.pdf sin contenido\n",
            "Archivo 1302.1527.pdf sin contenido\n",
            "Archivo 1301.2274.pdf sin contenido\n",
            "Archivo 1302.1547.pdf sin contenido\n",
            "Archivo 1301.2299.pdf sin contenido\n",
            "Archivo 1301.2293.pdf sin contenido\n",
            "Archivo 1302.1532.pdf sin contenido\n",
            "Archivo 1302.6825.pdf sin contenido\n",
            "Archivo 1206.5276.pdf sin contenido\n",
            "Archivo 1302.1520.pdf sin contenido\n",
            "Archivo 1302.6853.pdf sin contenido\n",
            "Archivo 1302.6784.pdf sin contenido\n",
            "Archivo 1301.2285.pdf sin contenido\n",
            "Archivo 1302.6846.pdf sin contenido\n",
            "Archivo 1302.1554.pdf sin contenido\n",
            "Archivo 1302.6793.pdf sin contenido\n",
            "Archivo 1302.1535.pdf sin contenido\n",
            "Archivo 1302.1522.pdf sin contenido\n",
            "Archivo 1302.1521.pdf sin contenido\n",
            "Archivo 1301.2260.pdf sin contenido\n",
            "Archivo 1302.6840.pdf sin contenido\n",
            "Archivo 1302.6798.pdf sin contenido\n",
            "Archivo 1302.6836.pdf sin contenido\n",
            "Archivo 1302.6816.pdf sin contenido\n",
            "Archivo 1302.6814.pdf sin contenido\n",
            "Archivo 1302.6820.pdf sin contenido\n",
            "Archivo 1302.1563.pdf sin contenido\n",
            "Archivo 1301.2259.pdf sin contenido\n",
            "Archivo 1301.2253.pdf sin contenido\n",
            "Archivo 1301.2319.pdf sin contenido\n",
            "Archivo 1302.6841.pdf sin contenido\n",
            "Archivo 1301.2254.pdf sin contenido\n",
            "Archivo 1302.1531.pdf sin contenido\n",
            "Archivo 1302.6854.pdf sin contenido\n",
            "Archivo 1301.2301.pdf sin contenido\n",
            "Archivo 1206.5255.pdf sin contenido\n",
            "Archivo 1302.6848.pdf sin contenido\n",
            "Archivo 1206.5260.pdf sin contenido\n",
            "Archivo 1302.1553.pdf sin contenido\n",
            "Archivo 1302.1533.pdf sin contenido\n",
            "Archivo 1705.10694.pdf sin contenido\n",
            "Archivo 1301.2267.pdf sin contenido\n",
            "Archivo 1301.2277.pdf sin contenido\n",
            "Archivo 1206.5251.pdf sin contenido\n",
            "Archivo 1302.1539.pdf sin contenido\n",
            "Archivo 1302.1575.pdf sin contenido\n",
            "Archivo 1302.6843.pdf sin contenido\n",
            "Archivo 1302.6799.pdf sin contenido\n",
            "Archivo 1206.5284.pdf sin contenido\n",
            "Archivo 1301.2275.pdf sin contenido\n",
            "Archivo 1301.2307.pdf sin contenido\n",
            "Datos guardados en CSV: /content/drive/MyDrive/PeerRead-master/data/arxivAi_primer_ciclo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# obtener estado de aceptacion"
      ],
      "metadata": {
        "id": "v9EqF9IqeZUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "lista_part = [\"dev/reviews/\", \"test/reviews/\", \"train/reviews/\"]\n",
        "lista_conf = [\"arxiv.cs.ai_2007-2017\"]\n",
        "csv_path = os.path.join(ruta_carpeta_principal, \"reviews_arxivAi.csv\")\n",
        "\n",
        "# Crear o cargar el DataFrame desde el archivo CSV existente\n",
        "if os.path.exists(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "else:\n",
        "    df = pd.DataFrame(columns=[\"Archivo\", \"Estado\"])\n",
        "\n",
        "dataframes = []  # Lista para almacenar los DataFrames temporales\n",
        "\n",
        "for confe in lista_conf:\n",
        "    for particion in lista_part:\n",
        "        ruta = os.path.join(ruta_carpeta_principal, confe, particion)\n",
        "        archivos = os.listdir(ruta)\n",
        "\n",
        "        for archivo in archivos:\n",
        "            archivo_path = os.path.join(ruta, archivo)\n",
        "\n",
        "            with open(archivo_path, 'r') as f:\n",
        "                contenido = json.load(f)\n",
        "                id = contenido[\"id\"]\n",
        "                id = id+'.pdf'\n",
        "                accepted = contenido[\"accepted\"]\n",
        "\n",
        "                # Verificar si accepted es un booleano y asignar 1 o 0 en consecuencia\n",
        "                if isinstance(accepted, bool):\n",
        "                    accepted_value = \"1\" if accepted else \"0\"\n",
        "                else:\n",
        "                    accepted_value = \"1\" if accepted.lower() == \"verdadero\" else \"0\"\n",
        "\n",
        "                temp_df = pd.DataFrame({\"Archivo\": [id], \"Accepted\": [accepted_value]})\n",
        "                dataframes.append(temp_df)\n",
        "\n",
        "# Concatenar todos los DataFrames temporales en uno solo\n",
        "if dataframes:\n",
        "    df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Guardar el DataFrame actualizado en el archivo CSV\n",
        "df.to_csv(csv_path, sep=',', index=False, encoding='utf-8')\n",
        "print(\"Datos guardados en CSV:\", csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SOZMxDK6QPI",
        "outputId": "8991d182-e2a2-4fb5-ac44-7a9fe4d871ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en CSV: /content/drive/MyDrive/PeerRead-master/data/reviews_arxivAi.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#combinar csvs"
      ],
      "metadata": {
        "id": "NT1MTWJ5l8QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Especifica la ruta de tus archivos CSV en tu Google Drive\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "\n",
        "archivo1 = os.path.join(ruta_carpeta_principal, \"reviews_arxivAi.csv\")\n",
        "archivo2 = os.path.join(ruta_carpeta_principal, \"arxivAi_primer_ciclo.csv\")\n",
        "\n",
        "# Cargar archivos CSV en DataFrames\n",
        "df1 = pd.read_csv(archivo1)\n",
        "df2 = pd.read_csv(archivo2)\n",
        "\n",
        "df1 = df1.drop_duplicates(subset='Title', keep='first')\n",
        "df2 = df2.drop_duplicates(subset='Title', keep='first')\n",
        "\n",
        "# Union con base a la columna 'Archivo'\n",
        "result = pd.merge(df1, df2, on='Archivo', how='outer')\n",
        "#result = result.dropna(subset=['Conferencia'])\n",
        "print(result)\n",
        "\n",
        "result.to_csv('/content/drive/MyDrive/arxivAi_datos_completos.csv', index=False)"
      ],
      "metadata": {
        "id": "SRkdJ3sG5Sc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "0f22cf2a-c87b-44e7-a7fa-aae219c15df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0047b1963180>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Cargar archivos CSV en DataFrames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchivo1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchivo2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Union con base a la columna 'Archivo'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Especifica la ruta de tus archivos CSV en tu Google Drive\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "\n",
        "archivo1 = os.path.join(ruta_carpeta_principal, \"arxivAi_primer_ciclo.csv\")\n",
        "archivo2 = os.path.join(ruta_carpeta_principal, \"reviews_arxivAi.csv\")\n",
        "\n",
        "# Carga los archivos CSV en DataFrames\n",
        "df1 = pd.read_csv(archivo1)\n",
        "df2 = pd.read_csv(archivo2)\n",
        "\n",
        "# Guarda el número inicial de filas en cada DataFrame\n",
        "num_filas_inicial_df1 = len(df1)\n",
        "print(\"Filas del doc 1:\",num_filas_inicial_df1)\n",
        "num_filas_inicial_df2 = len(df2)\n",
        "print(\"Filas del doc 2:\",num_filas_inicial_df2)\n",
        "\n",
        "# Union de csv con base en la columna 'Title'\n",
        "result = pd.merge(df1, df2, on='Archivo', how='outer')\n",
        "print(\"Número total de elementos al unir:\", len(result))\n",
        "\n",
        "# Dropear filas con valores nulos en columnas\n",
        "result = result.dropna(subset=['Accepted'])\n",
        "#result = result.dropna(subset=['Contenido'])\n",
        "print(\"Número total de elementos despues de dropear:\", len(result))\n",
        "# Calcular la diferencia en el numero de filas despues de la limpieza\n",
        "\n",
        "\n",
        "#print(result)\n",
        "# Guarda el resultado en un CSV\n",
        "result.to_csv('/content/drive/MyDrive/arxivAi_datos_completos.csv', index=False)\n",
        "print(result)\n",
        "\n",
        "# Imprime el numero total de filas eliminadas\n",
        "#print(\"Número total de filas eliminadas:\", num_total_filas_eliminadas)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0FlQ7rBWFFT",
        "outputId": "0e1695ec-d284-4875-b7b9-7fbb1c2d793d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filas del doc 1: 4092\n",
            "Filas del doc 2: 4092\n",
            "Número total de elementos al unir: 4092\n",
            "Número total de elementos despues de dropear: 4092\n",
            "                Conferencia Particion         Archivo  \\\n",
            "0     arxiv.cs.ai_2007-2017       dev   1302.4942.pdf   \n",
            "1     arxiv.cs.ai_2007-2017       dev  1703.04912.pdf   \n",
            "2     arxiv.cs.ai_2007-2017       dev   1201.2430.pdf   \n",
            "3     arxiv.cs.ai_2007-2017       dev   1305.3321.pdf   \n",
            "4     arxiv.cs.ai_2007-2017       dev  1511.03532.pdf   \n",
            "...                     ...       ...             ...   \n",
            "4087  arxiv.cs.ai_2007-2017     train   1306.0386.pdf   \n",
            "4088  arxiv.cs.ai_2007-2017     train  1605.04056.pdf   \n",
            "4089  arxiv.cs.ai_2007-2017     train  1603.03518.pdf   \n",
            "4090  arxiv.cs.ai_2007-2017     train   1406.3124.pdf   \n",
            "4091  arxiv.cs.ai_2007-2017     train  1612.07837.pdf   \n",
            "\n",
            "                                                  Title  \\\n",
            "0     Implementation of Continuous Bayesian Networks...   \n",
            "1                                                   NaN   \n",
            "2          A Well-typed Lightweight Situation Calculus∗   \n",
            "3     A Mining-Based Compression Approach for Constr...   \n",
            "4     IBMMS DECISION SUPPORT TOOL FOR MANAGEMENT OF ...   \n",
            "...                                                 ...   \n",
            "4087  Improved and Generalized Upper Bounds on the C...   \n",
            "4088         Causal Discovery for Manufacturing Domains   \n",
            "4089  A Novel Divide and Conquer Approach for Large-...   \n",
            "4090  Guarantees and Limits of Preprocessing in Cons...   \n",
            "4091  SAMPLERNN: AN UNCONDITIONAL END-TO-END NEURAL ...   \n",
            "\n",
            "                                                 Emails  \\\n",
            "0                                                    []   \n",
            "1                                                    []   \n",
            "2                                ['ltan003@cs.ucr.edu']   \n",
            "3                                                    []   \n",
            "4                                                    []   \n",
            "...                                                 ...   \n",
            "4087                        ['bruno.scherrer@inria.fr']   \n",
            "4088  ['kmarazo@cs.umass.edu', 'rumi.ghosh@us.bosch....   \n",
            "4089  ['trevor@mail.ustc.edu.cn;', 'ketang@ustc.edu....   \n",
            "4090                                                 []   \n",
            "4091                                                 []   \n",
            "\n",
            "                                                Autores  \\\n",
            "0                     ['Eric Driver', 'Darryl Morrell']   \n",
            "1     ['ZHIQIANG ZHUANG', 'KEWEN WANG', 'Sebastian B...   \n",
            "2                                            ['Li Tan']   \n",
            "3                      ['Said Jabbour', 'Yakoub Salhi']   \n",
            "4                         ['Ali KELES', 'Ayturk KELES']   \n",
            "...                                                 ...   \n",
            "4087                                 ['Bruno Scherrer']   \n",
            "4088  ['Katerina Marazopoulou', 'Rumi Ghosh', 'Prasa...   \n",
            "4089                ['Peng Yang', 'Ke Tang', 'Xin Yao']   \n",
            "4090                ['Serge Gaspers', 'Stefan Szeider']   \n",
            "4091  ['Soroush Mehri', 'Kundan Kumar', 'Ishaan Gulr...   \n",
            "\n",
            "                                         Texto Completo  \\\n",
            "0     1 INTRODUCTION Bayesian networks provide a met...   \n",
            "1     ar X iv :1 70 3. 04 91 2v 2 [ cs .A I] 1 7 M a...   \n",
            "2     Submitted to: The 21st Workshop on Logic-based...   \n",
            "3     ar X iv :1 30 5. 33 21 v1 [ cs .A I] 1 4 M ay ...   \n",
            "4     DOI : 10.5121/ijdms.2015.7501 1 Although direc...   \n",
            "...                                                 ...   \n",
            "4087  ar X iv :1 30 6. 03 86 v4 [ m at h. O C ] 1 0 ...   \n",
            "4088  Categories and Subject Descriptors H.2.8 [Data...   \n",
            "4089  1 Introduction Developing Artificial Intellige...   \n",
            "4090  ar X iv :1 40 6. 31 24 v1 [ cs .A I] 1 2 Ju n ...   \n",
            "4091  1 INTRODUCTION Audio generation is a challengi...   \n",
            "\n",
            "                                               Abstract  \\\n",
            "0     Bayesian networks provide a method of rep­ res...   \n",
            "1     Recent methods have adapted the well-establish...   \n",
            "2     Situation calculus has been widely applied in ...   \n",
            "3     In this paper, we propose an extension of our ...   \n",
            "4     Although direct marketing is a good method for...   \n",
            "...                                                 ...   \n",
            "4087  Given a Markov Decision Process (MDP) with n s...   \n",
            "4088  Increasing yield and improving quality are of ...   \n",
            "4089  Divide and Conquer (DC) is conceptually well s...   \n",
            "4090  We present a first theoretical analysis of the...   \n",
            "4091  In this paper we propose a novel model for unc...   \n",
            "\n",
            "                                           Introducción  \\\n",
            "0                                                   NaN   \n",
            "1     A key ingredient for any machine to be conside...   \n",
            "2     Introduced by John McCarthy in 1963 [10], situ...   \n",
            "3     The table constraint is considered for a long ...   \n",
            "4     The worldwide banking industry has experienced...   \n",
            "...                                                 ...   \n",
            "4087  We consider a discrete-time dynamic system who...   \n",
            "4088  With increasing interest in the Internet of Th...   \n",
            "4089  Developing Artificial Intelligence (AI) applic...   \n",
            "4090  Many important computational problems that ari...   \n",
            "4091  Audio generation is a challenging task at the ...   \n",
            "\n",
            "                                              Contenido  \\\n",
            "0                                                   NaN   \n",
            "1     **2. PRELIMINARIES**\\nWe first briefly recall ...   \n",
            "2     **2 Related Work**\\nDue to its powerful action...   \n",
            "3     **2 Technical background and preliminary defin...   \n",
            "4     **2. BANK MARKETING DATA**\\nThe UCI Machine Le...   \n",
            "...                                                 ...   \n",
            "4087  **2 Bounds with respect to a fixed discount fa...   \n",
            "4088  **2. THE MANUFACTURING DOMAIN**\\nTypical assem...   \n",
            "4089  **2 Major challenge of dealing with interdepen...   \n",
            "4090  **2 Formal Background**\\nA parameterized probl...   \n",
            "4091  **2 SAMPLERNN MODEL**\\nIn this paper we propos...   \n",
            "\n",
            "                                             Conclusion  \\\n",
            "0                                                   NaN   \n",
            "1     In this work, we presented two new constructio...   \n",
            "2     Type systems have been proposed to guarantee t...   \n",
            "3     In this paper, we propose a data-mining approa...   \n",
            "4     Data mining studies attempting to solve many p...   \n",
            "...                                                 ...   \n",
            "4087                                                NaN   \n",
            "4088  In this work, we apply causal structure learni...   \n",
            "4089  This work investigated the Divide and Conquer ...   \n",
            "4090  We have provided the first theoretical evaluat...   \n",
            "4091  We propose a novel model that can address unco...   \n",
            "\n",
            "                                            Referencias  Accepted  \n",
            "0     [{'title': 'Probabilistic Reasoning In Intelli...         0  \n",
            "1     [{'title': 'On the logic of theory change: Par...         0  \n",
            "2     [{'title': 'An introduction to first-order log...         0  \n",
            "3     [{'title': 'Mining association rules between s...         0  \n",
            "4     [{'title': 'Data Mining: a competitive tool in...         0  \n",
            "...                                                 ...       ...  \n",
            "4087  [{'title': 'Policy iteration for perfect infor...         1  \n",
            "4088  [{'title': 'Sada: A general framework to suppo...         0  \n",
            "4089  [{'title': 'LIBSVM: A library for support vect...         0  \n",
            "4090  [{'title': 'A kernelization algorithm for d-hi...         0  \n",
            "4091  [{'title': 'Modeling high-dimensional discrete...         1  \n",
            "\n",
            "[4092 rows x 13 columns]\n"
          ]
        }
      ]
    }
  ]
}