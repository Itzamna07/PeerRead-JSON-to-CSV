{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqX3eEhi97wa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b538170-0737-4cc3-c92b-1f1dec295633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Cargar algo de drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creacion de dataset y obtencion de datos"
      ],
      "metadata": {
        "id": "KOmlf3GTxrMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.indexing import item_from_zerodim\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "import json,re\n",
        "\n",
        "# Ruta de la carpeta principal\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "lista_part=[\"dev/parsed_pdfs/\",\"test/parsed_pdfs/\",\"train/parsed_pdfs/\"]\n",
        "lista_conf=[\"acl_2017\"]\n",
        "rutas_archivos_json = []\n",
        "secc = {}\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = re.sub(r'\\n\\d+\\n', '\\n', texto)\n",
        "    texto = re.sub(r'[×/]', '', texto)\n",
        "    texto = re.sub(r'\\b\\d+\\b', '', texto)\n",
        "    texto = re.sub(r'\\n+', '\\n', texto).strip()\n",
        "    return texto\n",
        "\n",
        "intro_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(introduction)', re.IGNORECASE)\n",
        "conclusion_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(conclusion)', re.IGNORECASE)\n",
        "\n",
        "for confe in lista_conf:\n",
        "  #print(confe)\n",
        "  for particion in lista_part:\n",
        "    #print(particion)\n",
        "    ruta=(ruta_carpeta_principal+\"/\"+confe+\"/\"+particion)\n",
        "    #print(\"Explorando ruta:\", ruta)\n",
        "    archivos=glob.glob(ruta+\"/*.json\")\n",
        "    #print(archivos)\n",
        "    particion_sin_ruta = particion.strip('/').split('/')[0]\n",
        "    #print(particion_sin_ruta)\n",
        "\n",
        "    for archivo in archivos:\n",
        "      with open(archivo, 'r') as f:\n",
        "        contenido = json.load(f)\n",
        "        #print(contenido.keys())\n",
        "        nombre=contenido[\"name\"]\n",
        "        diccontent=contenido[\"metadata\"]\n",
        "        #print(diccontent)\n",
        "        t = diccontent[\"title\"]\n",
        "        #print(t)\n",
        "        e = diccontent[\"emails\"]\n",
        "        #print(e)\n",
        "        autores=diccontent[\"authors\"]\n",
        "        #print(autores)\n",
        "        dicsecciones=diccontent[\"sections\"]\n",
        "\n",
        "        referencias_archivo = diccontent[\"references\"]\n",
        "\n",
        "        abstract= diccontent[\"abstractText\"]\n",
        "        #print(dicsecciones)\n",
        "        cadena=\"\"\n",
        "        introduccion=\"\"\n",
        "        conclusion=\"\"\n",
        "        contenido=\"\"\n",
        "\n",
        "        #print(dicsecciones)\n",
        "        if dicsecciones!=None:\n",
        "          for item in dicsecciones:\n",
        "            if item['heading'] == None:\n",
        "                cadena = \"\"\n",
        "                text=\"\"\n",
        "            else:\n",
        "                cadena = cadena +\" \"+ item['heading'] + \"\\n\"\n",
        "                cadena += \" \" #limpiar_texto(cadena)+\" \"\n",
        "\n",
        "                # Verificar si el heading coincide con el patron de introduccion y concllu\n",
        "                if re.search(intro_pattern, item['heading']):\n",
        "                  introduccion = item['text'] #limpiar_texto(item['text'])\n",
        "                elif re.search(conclusion_pattern, item['heading']):\n",
        "                  conclusion = item['text'] #limpiar_texto(item['text'])\n",
        "                else:\n",
        "                  text= item['text']# limpiar_texto(item['text'])\n",
        "                  contenido += f\"**{item['heading']}**\\n\"+ text+ \"\\n\"\n",
        "\n",
        "            texto = item['text'] #re.sub(r'(\\n\\d+)+\\n', '',item['text'])\n",
        "            #texto = limpiar_texto(texto)\n",
        "            cadena = cadena+\" \"+texto+\"\\n\"\n",
        "          rutas_archivos_json.append({\n",
        "                      \"Conferencia\":confe,\n",
        "                      \"Particion\":particion_sin_ruta,\n",
        "                      \"Archivo\": nombre,\n",
        "                      #\"Title\": t,\n",
        "                      \"Emails\": e,\n",
        "                      \"Autores\": autores,\n",
        "                      \"Texto Completo\": f\"{t} {cadena} Abstract {abstract}\",\n",
        "                      \"Abstract\": abstract,\n",
        "                      \"Introducción\": introduccion,\n",
        "                      \"Contenido\": contenido,\n",
        "                      \"Conclusion\": conclusion,\n",
        "                      \"Referencias\": referencias_archivo\n",
        "                  })\n",
        "        else:\n",
        "          print(\"Error en el archivo: \"+nombre)\n",
        "\n",
        "          rutas_archivos_json.append({\n",
        "                      \"Conferencia\":confe,\n",
        "                      \"Particion\":particion_sin_ruta,\n",
        "                      \"Archivo\": nombre,\n",
        "                      \"Title\": t,\n",
        "                      \"Emails\": e,\n",
        "                      \"Autores\": autores,\n",
        "                      \"Texto Completo\": \"\",\n",
        "                      \"Abstract\": abstract,\n",
        "                      \"Introducción\": \"\",\n",
        "                      \"Contenido\": \"\",\n",
        "                      \"Conclusion\": \"\",\n",
        "                      \"Referencias\": referencias_archivo\n",
        "                  })\n",
        "\n",
        "\n",
        "df = pd.DataFrame(rutas_archivos_json)\n",
        "df.to_csv(ruta_carpeta_principal+\"/\"+\"acl_primer_ciclo.csv\", sep=',', index=False, encoding='utf-8')\n",
        "print(\"Datos guardados en CSV:\", ruta_carpeta_principal+\"/\"+\"acl_primer_ciclo.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWdNJfnQ-L_y",
        "outputId": "a9f32cc8-e45c-4300-ef2b-024426ff50fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en CSV: /content/drive/MyDrive/PeerRead-master/data/acl_primer_ciclo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.indexing import item_from_zerodim\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import json, re\n",
        "\n",
        "# Ruta de la carpeta principal\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "lista_part = [\"dev/parsed_pdfs/\", \"test/parsed_pdfs/\", \"train/parsed_pdfs/\"]\n",
        "lista_conf = [\"acl_2017\"]\n",
        "rutas_archivos_json = []\n",
        "secc = {}\n",
        "\n",
        "# Contadores para las particiones\n",
        "contador_dev = 0\n",
        "contador_test = 0\n",
        "contador_train = 0\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = re.sub(r'\\n\\d+\\n', '\\n', texto)\n",
        "    texto = re.sub(r'[×/]', '', texto)\n",
        "    texto = re.sub(r'\\b\\d+\\b', '', texto)\n",
        "    texto = re.sub(r'\\n+', '\\n', texto).strip()\n",
        "    return texto\n",
        "\n",
        "intro_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(introduction)', re.IGNORECASE)\n",
        "conclusion_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(conclusion)', re.IGNORECASE)\n",
        "\n",
        "for confe in lista_conf:\n",
        "    for particion in lista_part:\n",
        "        ruta = (ruta_carpeta_principal + \"/\" + confe + \"/\" + particion)\n",
        "        archivos = glob.glob(ruta + \"/*.json\")\n",
        "        particion_sin_ruta = particion.strip('/').split('/')[0]\n",
        "\n",
        "        for archivo in archivos:\n",
        "            with open(archivo, 'r') as f:\n",
        "                contenido = json.load(f)\n",
        "                nombre = contenido[\"name\"]\n",
        "                #print(nombre)\n",
        "                diccontent = contenido[\"metadata\"]\n",
        "                t = diccontent[\"title\"]\n",
        "\n",
        "                e = diccontent[\"emails\"]\n",
        "                autores = diccontent[\"authors\"]\n",
        "                dicsecciones = diccontent[\"sections\"]\n",
        "                referencias_archivo = diccontent[\"references\"]\n",
        "                abstract = diccontent[\"abstractText\"]\n",
        "                cadena = \"\"\n",
        "                introduccion = \"\"\n",
        "                conclusion = \"\"\n",
        "                contenido = \"\"\n",
        "\n",
        "                if dicsecciones is not None:\n",
        "                    for item in dicsecciones:\n",
        "                        if item['heading'] is None:\n",
        "                            cadena = \"\"\n",
        "                            text = \"\"\n",
        "                        else:\n",
        "                            cadena = cadena + \" \" + item['heading'] + \"\\n\"\n",
        "                            cadena += \" \"\n",
        "                            if re.search(intro_pattern, item['heading']):\n",
        "                                introduccion = item['text']\n",
        "                            elif re.search(conclusion_pattern, item['heading']):\n",
        "                                conclusion = item['text']\n",
        "                            else:\n",
        "                                text = item['text']\n",
        "                                contenido += f\"**{item['heading']}**\\n\" + text + \"\\n\"\n",
        "\n",
        "                        texto = item['text']\n",
        "                        cadena = cadena + \" \" + texto + \"\\n\"\n",
        "                    rutas_archivos_json.append({\n",
        "                        \"Conferencia\": confe,\n",
        "                        \"Particion\": particion_sin_ruta,\n",
        "                        \"Archivo\": nombre,\n",
        "                        \"Title\": t,\n",
        "                        \"Emails\": e,\n",
        "                        \"Autores\": autores,\n",
        "                        \"Texto Completo\": f\"{t} {cadena} Abstract {abstract}\",\n",
        "                        \"Abstract\": abstract,\n",
        "                        \"Introducción\": introduccion,\n",
        "                        \"Contenido\": contenido,\n",
        "                        \"Conclusion\": conclusion,\n",
        "                        \"Referencias\": referencias_archivo\n",
        "                    })\n",
        "\n",
        "                    # Actualizar los contadores según la partición actual\n",
        "                    if particion_sin_ruta == \"dev\":\n",
        "                        contador_dev += 1\n",
        "                    elif particion_sin_ruta == \"test\":\n",
        "                        contador_test += 1\n",
        "                    elif particion_sin_ruta == \"train\":\n",
        "                        contador_train += 1\n",
        "                else:\n",
        "                    print(\"Error en el archivo: \" + nombre)\n",
        "                    rutas_archivos_json.append({\n",
        "                        \"Conferencia\": confe,\n",
        "                        \"Particion\": particion_sin_ruta,\n",
        "                        \"Archivo\": nombre,\n",
        "                        \"Title\": t,\n",
        "                        \"Emails\": e,\n",
        "                        \"Autores\": autores,\n",
        "                        \"Texto Completo\": \"\",\n",
        "                        \"Abstract\": abstract,\n",
        "                        \"Introducción\": \"\",\n",
        "                        \"Contenido\": \"\",\n",
        "                        \"Conclusion\": \"\",\n",
        "                        \"Referencias\": referencias_archivo\n",
        "                    })\n",
        "\n",
        "# Imprimir el número de archivos en cada partición\n",
        "#print(\"Archivos en la partición 'dev':\", contador_dev)\n",
        "#print(\"Archivos en la partición 'test':\", contador_test)\n",
        "#print(\"Archivos en la partición 'train':\", contador_train)\n",
        "\n",
        "df = pd.DataFrame(rutas_archivos_json)\n",
        "print(df)\n",
        "df.to_csv(ruta_carpeta_principal + \"/\" + \"acl_primer_ciclo.csv\", sep=',', index=False, encoding='utf-8')\n",
        "print(\"Datos guardados en CSV:\", ruta_carpeta_principal + \"/\" + \"acl_primer_ciclo.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGyYl1uTsqU8",
        "outputId": "892da637-c0db-4318-bf48-8a34c1fe149a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Conferencia Particion  Archivo  \\\n",
            "0      acl_2017       dev  660.pdf   \n",
            "1      acl_2017       dev  371.pdf   \n",
            "2      acl_2017       dev  352.pdf   \n",
            "3      acl_2017       dev  489.pdf   \n",
            "4      acl_2017       dev   37.pdf   \n",
            "..          ...       ...      ...   \n",
            "132    acl_2017     train  419.pdf   \n",
            "133    acl_2017     train  182.pdf   \n",
            "134    acl_2017     train  699.pdf   \n",
            "135    acl_2017     train  676.pdf   \n",
            "136    acl_2017     train  691.pdf   \n",
            "\n",
            "                                                                                                                              Title  \\\n",
            "0                                                                      Automatically Generating Rhythmic Verse with Neural Networks   \n",
            "1                                                                                                                              None   \n",
            "2                                                                                                                              None   \n",
            "3    Combining distributional and referential information for naming objects through cross-modal mapping and direct word prediction   \n",
            "4                     Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots   \n",
            "..                                                                                                                              ...   \n",
            "132                                                                  One-Shot Neural Cross-Lingual Transfer for Paradigm Completion   \n",
            "133                                            Modeling Contextual Relationships Among Utterances for Multimodal Sentiment Analysis   \n",
            "134                                                                                                                            None   \n",
            "135                                                                           Neural Machine Translation via Binary Code Prediction   \n",
            "136                                            Using Ontology-Grounded Token Embeddings To Predict Prepositional Phrase Attachments   \n",
            "\n",
            "    Emails Autores  \\\n",
            "0       []      []   \n",
            "1       []      []   \n",
            "2       []      []   \n",
            "3       []      []   \n",
            "4       []      []   \n",
            "..     ...     ...   \n",
            "132     []      []   \n",
            "133     []      []   \n",
            "134     []      []   \n",
            "135     []      []   \n",
            "136     []      []   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Texto Completo  \\\n",
            "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Automatically Generating Rhythmic Verse with Neural Networks  1 000\\n011\\n012\\n013\\n014\\n015\\n016\\n017\\n018\\n019\\n020\\n021\\n022\\n023\\n024\\n025\\n026\\n027\\n028\\n029\\n030\\n031\\n032\\n033\\n034\\n035\\n036\\n037\\n038\\n039\\n040\\n041\\n042\\n043\\n044\\n045\\n046\\n047\\n048\\n049\\n061\\n062\\n063\\n064\\n065\\n066\\n067\\n068\\n069\\n070\\n071\\n072\\n073\\n074\\n075\\n076\\n077\\n078\\n079\\n080\\n081\\n082\\n083\\n084\\n085\\n086\\n087\\n088\\n089\\n090\\n091\\n092\\n093\\n094\\n095\\n096\\n097\\n098\\n099\\nWe propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machinegenerated poem to be the best amongst all evaluated.\\n 1 Introduction\\n  Poetry is an advanced form of linguistic communication, in which a message is conveyed that satisfies both aesthetic and semantic constraints. As poetry is one of the most expressive forms of language, the automatic creation of texts recognisable as poetry is difficult. In addition to requiring an understanding of many aspects of language including phonetic patterns such as rhyme, rhythm and alliteration, poetry composition also requires a deep understanding of the meaning of language.\\nPoetry generation can be divided into two subtasks, namely the problem of content, which is concerned with a poem’s semantics, and the problem of form, which is concerned with the aesthetic rules that a poem follows. These rules may describe aspects of the literary devices used, and are usually highly prescriptive. Examples of different forms of poetry are limericks, ballads and sonnets. Limericks, for example, are characterised by their strict rhyme scheme (AABBA), their rhythm (two unstressed syllables followed by one stressed syllable) and their shorter third and fourth lines. Creating such poetry requires not only an understanding of the language itself, but also of how it sounds when spoken aloud. Statistical text generation usually requires the construction of a generative language model that explicitly learns the probability of any given word given previous context. Neural language models (Schwenk and Gauvain, 2005; Bengio et al., 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014; Kim et al., 2015). Poetry generation is an interesting application, since performing this task automatically requires the creation of models that not only focus on what is being written (content), but also on how it is being written (form). We experiment with two novel methodologies for solving this task. The first involves training a model to learn an implicit representation of content and form through the use of a phonological encoding. The second involves training a generative language model to represent content, which is then constrained by a discriminative pronunciation model, representing form. This second model is of particular interest because poetry with arbitrary rhyme, rhythm, repetition and themes can be generated by tuning the pronunciation model.\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\n 2 Related Work\\n  Automatic poetry generation is an important task due to the significant challenges involved. Most systems that have been proposed can loosely be categorised as rule-based expert systems, or statistical approaches. Rule-based poetry generation attempts include case-based reasoning (Gervás, 2000), templatebased generation (Colton et al., 2012), constraint satisfaction (Toivanen et al., 2013) and text mining (Netzer et al., 2009). These approaches are often inspired by how humans might generate poetry. Statistical approaches, conversely, make no assumptions about the creative process. Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used generate new poetic variants (Yi et al., 2016; Greene et al., 2010). The work of Zhang and Lapata (2014) is similar to ours, in that they make use of neural language models. For the task of automatic generation of classical Chinese poetry, they were able to outperform all other Chinese poetry generation systems with both manual and automatic evaluation.\\n 3 Phonetic-level Model\\n  Our first model is a pure neural language model, trained on a phonetic encoding of poetry in order to represent both form and content. Phonetic encodings of language represent information as sequences of around 40 basic acoustic symbols. Training on phonetic symbols allows the model to learn effective representations of pronunciation, including rhyme and rhythm. However, just training on a large corpus of poetry data is not enough. Specifically, two problems need to be overcome. 1) Phonetic encoding results in information loss: words that have the same pronunciation (homophones) cannot be perfectly reconstructed from the corresponding phonemes. This means that we require an additional probabilistic model in order to determine the most likely word given a sequence of phonemes. 2) The variety of poetry and poetic devices one can use— e.g., rhyme, rhythm, repetition—means that poems sampled from a model trained on all poetry would be unlikely to maintain internal consistency of meter and rhyme. It is therefore important to train the model on poetry which has its own internal consistency.\\nThus, the model comprises three steps: transliterating an ortographic sequence to its phonetic representation, training a neural language model on the phonetic encoding, and decoding the generated sequence back from phonemes to orthographic symbols.\\nPhonetic encoding To solve the first step, we apply a combination of word lookups from the CMU pronunciation dictionary (Weide, 2005) with letter-to-sound rules for handling out-ofvocabulary words. These rules are based on the CART techniques described by Black et al. (1998), and are represented with a simple Finite State Transducer1. The number of letters and number of phones in a word are rarely a one-to-one match: letters may match with up to three phones. In addition, virtually all letters can, in some contexts, map to zero phones, which is known as ‘wild’ or epsilon. Expectation Maximisation is used to compute the probability of a single letter matching a single phone, which is maximised through the application of Dynamic Time Warping (Myers et al., 1980) to determine the most likely position of epsilon characters. Although this approach offers full coverage over the training corpus—even for abbreviated words like ask’d and archaic words like renewest—it has several limitations. Irregularities in the English language result in difficulty determining general letter-to-sound rules that can manage words with unusual pronunciations such as “colonel” and “receipt” 2.\\nIn addition to transliterating words into phoneme sequences, we also represent word break characters as a specific symbol. This makes decipherment, when converting back into an orthographic representation, much easier. Phonetic transliteration allows us to construct a phonetic poetry corpus comprising 1,046,536 phonemes.\\nNeural language model We train a Long-Short Term Memory Network (Hochreiter and Schmidhuber, 1997) on the phonetic representation of our poetry corpus. The model is trained using stochastic gradient descent to predict the next phoneme given a sequence of phonemes. Specifically, we\\n1Implemented using FreeTTS (Walker et al., 2010) 2An evaluation of models in American English, British English, German and French was undertaken by Black et al. (1998), who reported an externally validated per token accuracy on British English as low as 67%. Although no experiments were carried out on corpora of early-modern English, it is likely that this accuracy would be significantly lower.\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\nmaximize a multinomial logistic regression objective over the final softmax prediction. Each phoneme is represented as a 256-dimensional embedding, and the model consists of two hidden layers of size 256. We apply backpropagationthrough-time (Werbos, 1990) for 150 timesteps, which roughly equates to four lines of poetry in sonnet form. This allows the network to learn features like rhyme even when spread over multiple lines. Training is preemptively stopped at 25 epochs to prevent overfitting.\\nOrthographic decoding When decoding from phonemes back to orthographic symbols, the goal is to compute the most likely word corresponding to a sequence of phonemes. That is, we compute the most probable hypothesis word W given a phoneme sequence ρ:\\nargmaxi P (Wi | ρ ) (1)\\nWe can consider the phonetic encoding of plaintext to be a homophonic cipher; that is, a cipher in which each symbol can correspond to one or more possible decodings. The problem of homophonic decipherment has received significant research attention in the past; with approaches utilising Expectation Maximisation (Knight et al., 2006), Integer Programming (Ravi and Knight, 2009) and A* search (Corlett and Penn, 2010). Transliteration from phonetic to an orthographic representation is done by constructing a Hidden Markov Model using the CMU pronunciation dictionary (Weide, 2005) and an n-gram language model. We calculate the transition probabilities (using the n-gram model) and the emission matrix (using the CMU pronunciation dictionary) to determine pronunciations that correspond to a single word. All pronunciations are naively considered equiprobable. We perform Viterbi decoding to find the most likely sequence of words. This means finding the most likely word wt+1 given a previous word sequence (wt−n, ..., wt).\\nargmaxwt+1 P ( wt+1 | w1, ... , wt ) (2)\\nIf a phonetic sequence does not map to any word, we apply the heuristic of artificially breaking the sequence up into two subsequences at index n, such that nmaximises the n-gram frequency of the subsequences.\\nAnd humble and their fit flees are wits size but that one made and made thy step me lies\\n————————————— Cool light the golden dark in any way the birds a shade a laughter turn away\\n————————————— Then adding wastes retreating white as thine\\nShe watched what eyes are breathing awe what shine —————————————\\nBut sometimes shines so covered how the beak Alone in pleasant skies no more to seek\\nFigure 1: Example output of the phonetic-level model trained on Iambic Pentameter poetry (grammatical errors are emphasised).\\nOutput A popular form of poetry with strict internal structure is the sonnet. Popularised in English by Shakespeare, the sonnet is characterised by a strict rhyme scheme and exactly fourteen lines of Iambic Pentameter (Greene et al., 2010). Since the 17,134 word tokens in Shakespeare’s 153 sonnets are insufficient to train an effective model, we augment this corpus with poetry taken from the website sonnets.org, yielding a training set of 288,326 words and 1,563,457 characters. An example of the output when training on this sonnets corpus is provided in Figure 1. Not only is it mostly in strict Iambic Pentameter, but the grammar of the output is mostly correct and the poetry contains rhyme.\\n 4 Constrained Character-level Model\\n  As the example shows, phonetic-level language models are effective at learning poetic form, despite small training sets and relatively few parameters. However, the fact that they require training data with internal poetic consistency implies that they do not generalise to other forms of poetry. That is, in order to generate poetry in Dactylic Hexameter (for example), a phonetic model must be trained on a corpus of Dactylic poetry. Not only is this impractical, but in many cases no corpus of adequate size even exists. Even when such poetic corpora are available, a new model must be trained for each type of poetry. This precludes tweaking the form of the output, which is important when generating poetry automatically. We now explore an alternative approach. Instead of attempting to represent both form and content in a single model, we construct a pipeline containing a generative language model represent-\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\ning content, and a discriminative model representing form. This allows us to represent the problem of creating poetry as a constraint satisfaction problem, where we can modify constraints to restrict the types of poetry we generate.\\nCharacter Language Model Rather than train a model on data representing features of both content and form, we now use a simple character-level model (Sutskever et al., 2011) focused solely on content. This approach offers several benefits over the word-level models that are prevalent in the literature. Namely, their more compact vocabulary allows for more efficient training; they can learn common prefixes and suffixes to allow us to sample words that are not present in the training corpus and can learn effective language representations from relatively small corpora; and they can handle archaic and incorrect spellings of words. As we no longer need the model to explicitly represent the form of generated poetry, we can loosen our constraints when choosing a training corpus. Instead of relying on poetry only in sonnet form, we can instead construct a generic corpus of poetry taken from online sources. This corpus is composed of 7.56 million words and 34.34 million characters, taken largely from 20th Century poetry books found online. The increase in corpus size facilitates a corresponding increase in the number of permissible model parameters. This allows us to train a 3-layer LSTM model with 2048- dimensional hidden layers. The model was trained to predict the next character given a sequence of characters, using stochastic gradient descent. We attenuate the learning rate over time, and by 20 epochs the model converges.\\nRhythm Modeling Although a character-level language model trained on a corpus of generic poetry allows us to generate interesting text, internal irregularities and noise in the training data prevent the model from learning important features such as rhythm. Hence, we require an additional classifier to constrain our model by either accepting or rejecting sampled lines based on the presence or absence of these features. As the presence of meter (rhythm) is the most characteristic feature of poetry, it therefore must be our primary focus. Pronunciation dictionaries have often been used to determine the syllabic stresses of words (Colton et al., 2012; Manurung et al., 2000; Misztal and Indurkhya, 2014), but suffer from some limitations\\nfor constructing a classifier. All word pronunciations are considered equiprobable, including archaic and uncommon pronunciations, and pronunciations are provided context free, despite the importance of context for pronunciation3. Furthermore, they are constructed from American English, meaning that British English may be misclassified. These issues are circumvented by applying lightly supervised learning to determine the contextual stress pattern of any word. That is, we exploit the latent structure in our corpus of sonnet poetry, namely, the fact that sonnets are composed of lines in rigid Iambic Pentameter, and are therefore exactly ten syllables long with alternating syllabic stress. This allows us to derive a syllablestress distribution. Although we use the sonnets corpus for this, it is important to note that any corpus with such a latent structure could be used. By representing a line as a cascade of Weighted Finite State Transducers (WFST), we can perform Expectation Maximisation over the poetry corpus to obtain a probabilistic classifier which enables us to determine the most likely stress patterns for each word. Every word is represented by a single transducer. Since weights can be assigned to state transitions, we can model the probability that a given input string maps to a particular output. In each cascade, a sequence of input words is mapped onto a sequence of stress patterns ⟨×, /⟩ where each pattern is between 1 and 5 syllables in length4. We initially set all transition probabilities equally, as we make no assumptions about the stress distributions in our training set. We then iterate over each line of the sonnet corpus, using Expectation Maximisation to train the cascades. In practice, there are several de facto variations of Iambic meter which are permissible, as shown in Figure 2. We train the rhythm classifier by converging the cascades to whatever output is the most likely given the line.\\nConstraining the model To generate poetry using this model, we sample sequences of characters from the character-level language model. To impose rhythm constrains on the language model, we first represent these sampled characters at the word level and pool sampled characters into word\\n3For example, the independent probability of stressing the single syllable word at is 40%, but this increases to 91%when the following word is the (Greene et al., 2010)\\n4Words of more than 5 syllables comprise less than 0.1% of the lexicon (Aoyama and Constable, 1998).\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\n× / × / × / × / × / / × × / × / × / × / × / × / × / × / × / × / × × / × / × / × / ×\\nFigure 2: Permissible variations of Iambic Pentameter in Shakespeare’s sonnets.\\ntokens in an intermediary buffer. We then apply the separately trained word-level WFSTs to construct a cascade of this buffer and perform Viterbi decoding over the cascade. This defines the distribution of stress-patterns over our word tokens. We can represent this cascade as a probabilistic classifier, and accept or reject the buffered output based on how closely it conforms to the desired meter. While sampling sequences of words from this model, the entire generated sequence is passed to the classifier each time a new word is sampled. The pronunciation model then returns the probability that the entire line is within the specified meter. If a new word is rejected by the classifier, the state of the network is rolled back to the state of the last formulaically acceptable line, removing the rejected word from memory. The constraint on rhythm can be controlled by adjusting the acceptability threshold of the classifier. By increasing the threshold, output focuses on form over content. Conversely, decreasing the criterion puts greater emphasis on content.\\nGeneric poetry\\nSonnet poetry\\nLSTM\\nWFST\\nRhythmic Output\\nTrained\\nTrained\\nBuffer\\n 4.1 Themes and Poetic devices\\n  It is important for any generative poetry model to include themes and poetic devices. One way to achieve this would be by constructing a corpus that exhibits the desired themes and devices. To create a themed corpus about ‘love’, for instance,\\nThemed Training Set\\nPoetry LSTM\\nThemed Output\\nTraining Set\\nPoetry LSTM\\nThemed Output\\nThematic Boosting\\nImplicit Explicit\\nFigure 3: Two approaches for generating themed poetry.\\nwe would aggregate love poetry to train the model, which would thus learn an implicit representation of love. However, this forces us to generate poetry according to discrete themes and styles from pretrained models, requiring a new training corpus for each model. In other words, we would suffer from similar limitations as with the phonetic-level model, in that we require a dedicated corpus. Alternatively, we can manipulate the language model by boosting character probabilities at sample time to increase the probability of sampling thematic words like ‘love’. This approach is more robust, and provides us with more control over the final output, including the capacity to vary the inclusion of poetic devices in the output.\\nThemes In order to introduce thematic content, we heuristically boost the probability of sampling words that are semantically related to a theme word from the language model. First, we compile a list of similar words to a key theme word by retrieving its semantic neighbours from a distributional semantic model (Mikolov et al., 2013). For example, the theme winter might include thematic words frozen, cold, snow and frosty. We represent these semantic neighbours at the character level, and heuristically boost their probability by multiplying the sampling probability of such character strings by a function of their cosine similarity to the key word. Thus, the likelihood of sampling a thematically related word is artificially increased, while still constraining the model rhythmically.\\nPoetic devices A similar method may be used for poetic devices such as assonance, consonance and alliteration. Since these devices can be orthographically described by the repetition of identical sequences of characters, we can apply the\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nErrors per line 1 2 3 4 Total\\nPhonetic Model 11 2 3 1 28 Character Model + WFST 6 5 1 1 23 Character Model 3 8 7 7 68\\nTable 1: Number of lines with n errors from a set of 50 lines generated by each of the three models.\\nWord Line Coverage\\nWikipedia 64.84% 83.35% 97.53% Sonnets 85.95% 80.32% 99.36%\\nTable 2: Error when transliterating text into phonemes and reconstructing back into text.\\nsame heuristic to boost the probability of sampling character strings that have previously been sampled. That is, to sample a line with many instances of alliteration (multiple words with the same initial sound) we record the historical frequencies of characters sampled at the beginning of each previous word. After a word break character, we boost the probability that those characters will be sampled again in the softmax. We only keep track of frequencies for a fixed number of time steps. By increasing or decreasing the size of this window, we can manipulate the prevalence of alliteration. Variations of this approach are applied to invoke consonance (by boosting intra-word consonants) and assonance (by boosting intra-word vowels). An example of two sampled lines with high degrees of alliteration, assonance and consonance is given in Figure 4c.\\n 5 Evaluation\\n  In order to examine how effective our methodologies for generating poetry are, we evaluate the proposed models in two ways. First, we perform an intrinsic evaluation where we examine the quality of the models and the generated poetry. Second, we perform an extrinsic evaluation where we evaluate the generated output using human annotators, and compare it to human-generated poetry.\\n 5.1 Intrinsic evaluation\\n  To evaluate the ability of both models to generate formulaic poetry that adheres to rhythmic rules, we compared sets of fifty sampled lines from each model. The first set was sampled from the phonetic-level model trained on Iambic poetry.\\nThe second set was sampled from the characterlevel model, constrained to Iambic form. For comparison, and to act as a baseline, we also sampled from the unconstrained character model. We created gold-standard syllabic classifications by recording each line spoken-aloud, and marking each syllable as either stressed or unstressed. We then compared these observations to loose Iambic Pentameter (containing all four variants), to determine how many syllabic misclassifications existed on each line. This was done by speaking each line aloud, and noting where the speaker put stresses. As Table 1 shows, the constrained character level model generated the most formulaic poetry. Results from this model show that 70% of lines had zero mistakes, with frequency obeying an inverse power-law relationship with the number of errors. We can see that the phonetic model performed similarly, but produced more subtle mistakes than the constrained character model: many of the errors were single mistakes in an otherwise correct line of poetry. In order to investigate this further, we examined to what extent these errors are due to transliteration (i.e., the phonetic encoding and orthographic decoding steps). Table 2 shows the reconstruction accuracy per word and per line when transliterating either Wikipedia or Sonnets to phonemes using the CMU pronunciation dictionary and subsequently reconstructing English text using the ngram model5. Word accuracy reflects the frequency of perfect reconstruction, whereas per line tri-gram similarity (Kondrak, 2005) reflects the overall reconstruction. Coverage captures the percentage of in-vocabulary items. The relatively low per-word accuracy achieved on the Wikipedia corpus is likely due to the high frequency of out-ofvocabulary words. The results show that a significant number of errors in the phonetic-level model are likely to be caused by transliteration mistakes.\\n 5.2 Extrinsic evaluation\\n  We conducted an indistinguishability study with a selection of automatically generated poetry and human poetry. As extrinsic evaluations are expensive and the phonetic model was unlikely to do well (as illustrated in Figure 4e: the model generates good Iambic form, but not very good English),\\n5Obviously, calculating this value for the character-level model makes no sense, since no transliteration occurs in that case.\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\n(a) The crow crooked on more beautiful and free, He journeyed off into the quarter sea. his radiant ribs girdled empty and very - least beautiful as dignified to see.\\n(c) Man with the broken blood blue glass and gold. Cheap chatter chants to be a lover do.\\n(e) The son still streams and strength and spirit. The ridden souls of which the fills of.\\n(b) Is that people like things (are the way we to figure it out) and I thought of you reading and then is your show or you know we will finish along will you play.\\n(d) How dreary to be somebody, How public like a frog To tell one’s name the livelong day To an admiring bog.\\nFigure 5: The experimental environment for asking participants to distinguish between automatically generated and human poetry.\\nwe only evaluate on the constrained characterlevel model.\\nThe aim of the study was to determine whether participants could distinguish between human and generated poetry, and if so to what extent. A set of 70 participants (of whom 61 were English native speakers) were each shown a selection of randomly chosen poetry segments, and were invited to classify them as either human or generated. Participants were recruited from friends and people within poetry communities, with an age range of 17 to 80, and a mean age of 29. Our participants were not financially incentivised, perceiving the evaluation as an intellectual challenge.\\nIn addition to the classification task, each partic-\\nipant was also invited to rate each poem on a 1-5 scale with respect to three criteria, namely readability, form and evocation (how much emotion did a poem elicit). We naively consider the overall quality of a poem to be the mean of these three measures. We used a custom web-based environment, built specifically for this evaluation6, which is illustrated in Figure 5. Based on human judgments, we can determine whether the models presented in this work can produce poetry of a similar quality to humans. To select appropriate human poetry that could be meaningfully compared with the machinegenerated poetry, we performed a comprehension test on all poems used in the evaluation, using the Dale-Chall readability formula (Dale and Chall, 1948). This formula represents readability as a function of the complexity of the input words. We selected nine machine-generated poems with a high readability score. The generated poems produced an average score of 7.11, indicating that readers over 15 years of age should easily be able to comprehend them. For our human poems, we focused explicitly on poetry where greater consideration is placed on prosodic elements like rhythm and rhyme than semantic content (known as “nonsense verse”). We randomly selected 30 poems belonging to that category from the website poetrysoup.com, of which\\n6[URL-ANONYMIZED]\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nPoet Title Human Readability Emotion Form\\nGenerated Best 0.66 0.60 -0.77 0.90 G. M. Hopkins Carrion Comfort 0.62 -1.09 1.39 -1.55\\nJ. Thornton Delivery of Death 0.60 0.26 -1.38 -0.65\\nGenerated All 0.54 -0.28 -0.30 0.23 M. Yvonne Intricate Weave 0.53 2.38 0.94 -1.67\\nE. Dickinson I’m Nobody 0.52 -0.46 0.92 0.44\\nG. M. Hopkins The Silver Jubilee 0.52 0.71 -0.33 0.65\\nR. Dryden Mac Flecknoe 0.51 -0.01 0.35 -0.78\\nA. Tennyson Beautiful City 0.48 -1.05 0.97 -1.26\\nW. Shakespeare A Fairy Song 0.45 0.65 1.30 1.18\\nTable 3: Proportion of people classifying each poem as ’human’, as well as the relative qualitative scores of each poem as deviations from the mean.\\neight were selected for the final comparison based on their comparable readability score. The selected poems were segmented into passages of between four and six lines, to match the length of the generated poetry segments. An example of such a segment is shown in Figure 4d. The human poems had an average score of 7.52, requiring a similar level of English aptitude to the generated texts. The performance of each human poem, alongside the aggregated scores of the generated poems, is illustrated in Table 3. For the human poems, our group of participants guessed correctly that they were human 51.4% of the time. For the generated poems, our participants guessed correctly 46.2% of the time that they were machine generated. To determine whether our results were statistically significant, we performed a Chi2 test. This resulted in a p-value of 0.718. This indicates that our participants were unable to tell the difference between human and generated poetry in any significant way. Although our participants generally considered the human poems to be of marginally higher quality than our generated poetry, they were unable to effectively distinguish between them. Interestingly, our results seem to suggest that our participants consider the generated poems to be more ‘human-like’ than those actually written by humans. Furthermore, the poem with the highest overall quality rating is a machine generated one. This shows that our approach was effective at generating high-quality rhythmic verse. It should be noted that the poems that were most ‘human-like’, most aesthetic and most emotive re-\\nspectively (though not the most readable) were all generated by the neural character model. Generally the set of poetry produced by the neural character model was slightly less readable and emotive than the human poetry, but had above average form. All generated poems included in this evaluation can be found in the supplementary material.\\n 6 Conclusions\\n  Our contributions are twofold. First, we developed a neural language model trained on a phonetic transliteration of poetic form and content. Although example output looked promising, this model was limited by its inability to generalise to novel forms of verse. We then proposed a more robust model trained on unformed poetic text, whose output form is constrained at sample time. This approach offers greater control over the style of the generated poetry than the earlier method, and facilitates themes and poetic devices. An indistinguishability test, where participants were asked to classify a randomly selected set of human “nonsense verse” and machine-generated poetry, showed generated poetry to be indistinguishable from that written by humans. In addition, the poems that were deemed most ‘humanlike’, most aesthetic and most emotive, respectively, were all machine-generated. In future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (Luong et al., 2013), which are common in poetry.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899\\n Abstract We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machinegenerated poem to be the best amongst all evaluated.   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  1 000\\n011\\n012\\n013\\n014\\n015\\n016\\n017\\n018\\n019\\n020\\n021\\n022\\n023\\n024\\n025\\n026\\n027\\n028\\n029\\n030\\n031\\n032\\n033\\n034\\n035\\n036\\n037\\n038\\n039\\n040\\n041\\n042\\n043\\n044\\n045\\n046\\n047\\n048\\n049\\n061\\n062\\n063\\n064\\n065\\n066\\n067\\n068\\n069\\n070\\n071\\n072\\n073\\n074\\n075\\n076\\n077\\n078\\n079\\n080\\n081\\n082\\n083\\n084\\n085\\n086\\n087\\n088\\n089\\n090\\n091\\n092\\n093\\n094\\n095\\n096\\n097\\n098\\n099\\n 1 Introduction\\n  There are latent nest structures beyond sequential surface words in natural language (Chomsky, 1957). In the last two decades, researchers incorporated more and more rich structural information into conventional language model (Jelinek and Lafferty, 1991; Chelba, 1997; Chelba and Jelinek, 2000; Emami and Jelinek, 2005) and\\nmore recently on neural network-based language modeling (Dyer et al., 2016). Among the effort, one direction is to explore sub-word structures(Costa-Jussà and Fonollosa, 2016; Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015), such as characters, mostly to handle out-of-vocabulary word problem. A somewhat opposite direction is to explore hyper-word structure. For example, (Eriguchi et al., 2016) adopts parsing tree in encoder phase in machine translation, (Stahlberg et al., 2016) proposes to use hierarchical phrase-based (HPB) model to guide the search in decoding. Both models, however, rely heavily on human labeled data on the language structures, which is extremely expensive and limited in scale.\\nIn this paper, we propose phrasal recurrent neural networks (pRNNs; §2), a general framework of RNNs (Elman, 1990) that explicitly models task-specific nested phrases from plain text. Here we use “phrase” as its definition in phrase-based statistical machine translation (PBSMT(Zens et al., 2002; Koehn et al., 2003)), which indicates any continues sequences of words. What different here are pRNNs permit phrases with arbitrary lengths instead of limiting them for the computational issue. The phrases in pRNNs are composed and selected in a way that is jointly learned in the language modeling, therefore requiring no human-labeled data or external model such as word alignment. In previous RNN-based language modeling, the hidden state of RNN before the word to predict summarizes the history of all previous words. Similarly, in pRNNs, we use the all state of all parallel RNNs (with the same parameters) to capture the history of all subsequence of words that precede the word to predict, with the starting word shifting from the first word the one right before the word to predict.\\nThis set of RNNs applied parallelly to different choices of word sequences are called RNN pyra-\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nmid. While most of those RNNs’ status deal with incorrect word sequences: they could either start in the middle of a chunk, or in a place too early or too late for the prediction tasks, we left it to an attention mechanism to select and combine, therefore eliminate the need for external knowledge on chunking and composition. This mechanism will be trained jointly with the composition models in pRNNs in optimizing a designed objective function, e.g, perplexity or likelihood. With proper composition function in pRNNs, the RNN pyramid provides a “phrase forest”, which could potentially contain a fairly deep nested structure in some of its members.\\nOur pRNN models have two merits:\\n• They represent all phrases in the same vector space in an explicit and unsupervised way. Which shows the potential to discover and utilize hidden structures of surface word sequences.\\n• They explore the possibility of network construction in another dimension: Parallel. Instead of stacking deeper and deeper layers of RNNs.\\nExperiments show that pRNNs are effective for language modeling (§4). Our model obtains significant better perplexities than state-of-the-art sequential Long-Short Term (LSTM) model on language modeling task, both on PTB and FBIS English data set. We also apply pRNNs to encode the source sentence of machine translation besides a conventional bi-direction encoder, which improves over the Moses (phrase-based statistical model) and a strong sequence-to-sequence baseline in the Chinese-English machine translation task.\\n 2 Phrasal RNNs\\n  We assume that, in the task of language model and machine translation, selecting the appropriate hidden structures for one sentence is highly related to the performance of the task.\\nFormally, a typical pRNN consist of three subnetworks: phrasal part P , attention part A and sequential part S. Each sub-network plays its own role and collaborates with others. P (§2.1) constructs the neural structures (realvalued vectors) which corresponding to natural linguistic structures (phrases). It takes embed-\\ndings (x?) of all words in a candidate phrase, as input, then output one fix-length real-valued vector p as the distributed representation (Hinton et al., 1986) of the phrase;\\np = P (xji ) (1)\\nWhere xji = [xi, · · · , xj ] (2)\\nA (§2.2) compares the candidate phrase regards to the current situation, give a probabilistic distribution over them, then provides the weighted sum of their representations. It takes previous calculated candidate set {p} as input, then output a weighted sum of {p} as p̂ with the help of current hidden state (ht) at time step t given by (S).\\np̂ = A(h, pk, · · · , pl) (3)\\nS (§2.2) combine the weighted sum of candidate phrases into original RNN, forces RNN taking the structural history information into consideration. It is similar to original RNN except for one point: when predict next hidden state ht+1, besides ht and xt, it takes p̂ as input too.\\nht = S(ht−1, xt, p̂t) (4)\\n 2.1 Represent Phrases\\n  There are many types of neural networks which can transfer phrase into distributed representation. However, when we need to handle arbitrary length phrases, another way of saying, the entire history\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\nof words, the choices are very limited to a few RNNs’ variants.\\nEven when we choose RNNs to construct structure vectors, we are still facing a big problem. Because the hidden state ht are considered to encode the entire history information from the beginning of the sentence xt1, they can be utilized as representations of phrases begin at the sentence head {xj1|1 ≤ j ≤ t}. But they do not provide the representations of phrases which do not begin with the first word of the sentence {xji |1 < i < j ≤ t}.\\nTo represent all candidate phrases in a sentence with n words, we build a RNN pyramid (RNNP (Fig. 3)), with n horizontal parallel RNNs {RNNn}Nn=1. RNNn indicates that it begins at the n-th word of the sentence. With all N(N + 1)/2 hidden status generated by RNN pyramid, we obtain distributed representation of all candidate phrases/structures of a sentence.\\nTo keep consistent among these parallel RNNs in the pyramid and to limit the number of parameters for keeping the model simple, we let all parallel RNNs share the same network parameters (W,U, b).\\nhnt = σ(Wxt + Uh n t−1 + b) (5)\\nWhere hnt of the RNNn indicates the hidden state of the t-th word in the sentence.\\nThis method is kind of similar to the sharing parameters between filters of convolutional neural network (Cun et al., 1990), except for it working on the time axis, which recognizes the local invariant along each time steps.\\nWith RNN pyramid built on a sentence, we can map all potential phrases with varying lengths into real-valued fix-length vectors. These vectors are representations of candidate structures we plan to compare at next stage.\\n 2.2 Utilize Phrases\\n  With the candidate structures represented by a fixlength vector (Fig. 2), we can easily apply attention mechanism on these vectors, and soft combine them to output a weighted sum as the best structure selected:\\nŝt = ∑ t,n αt,nh n t−1 (6)\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nFigure 3: On a 4-word sentence, RNN pyramid (dashed line triangle) generated by 4 horizontal parallel RNNs, each begins at 1 of the 4 words in the sentence. Initial status are indicated by circles. Because hidden state is considered containing all history information. The set of all hidden status in the pyramid can be mapped one-to-one to the representation of all candidate phrases of the sentence.\\nWhere the weight of hidden state (structure) αt,n can be represented by the following form:\\nαt,n = exp(et,n)∑\\nt,n exp(et,n)\\n(7)\\nIn which we define et,n as:\\net,n = a(hk, ht,n) (8)\\nHere we combine a(hk, ht,n) with one layer of feedforward neural network, where hk is the k-th word of sequential part S.\\nWe adopt the attention mechanism from Bahdanau et. al. (2014). As we showed in Fig 4. We put ŝ into R part of the network, let the network to combine it with h and x to predict next hidden state. We also apply our model on machine translation task within successful EncoderDecoder framework as in Fig 5.\\nFigure 4: Combine best structure ŝ given by attention partA in each predicting step of sequential part S. To reduce calculation, we limit the candidate phrase set at each step to the newly generated ones (the blue rectangles with solid boundaries), which means to ignore structures generated at previous steps (grey ones with dashed boundaries), therefore reduce the scale of candidate phrases set from O(N2) to O(N). The Pyramid and Seq part share the same embedding in experiments, we draw them separately in the diagram just for clearance.\\n 3 LM Experiment\\n  \\n 3.1 Data\\n  To make experiment comparable with other methods, we apply our models on language model task, evaluate it in perplexity on the widely used English Penn Treebank (PTB) (Marcus et al., 1993), which pre-processing and splitting by Mikolov (2010). The data is utilized as following: sections (0-20) with 929k tokens are used for training, sections (21-22) with 73k tokens are held out as validation, and sections (23-24) with 82k tokens are used for testing. There are only top 10,000 highfrequency words are kept in the corpus. All rest low-frequency words are replaced with UNK tag. This version of data is widely used among the language modeling community. It is publicly avail-\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nFigure 5: We use our pRNNs as the pyramid encoder to represent more structural information (all candidate phrases) of source sentences, just beside the original bi-direction encoder which only represents surface word sequence explictly. Then we join two context vectors from two encoders into a larger one. Then we allow the decoder to choose which portion of the larger context (which source words or candidate phrases) is more relevant to the next generated word of the target sentence. We adopt two settings of pyramid encoder, one takes only last status (the blue rectangles with solid boundaries) of each RNN as the input of attention part (src-pyr-last in Fig 4), the other takes all status (all rectangles including the grey ones with dashed boundaries) of all RNNs as the input of attention part (src-pyr-all in Fig 4).\\nable.1\\nBecause the scale of PTB corpus is relatively small, we also train our model on larger FBIS English(LDC2003E14). Accordingly, we only keep top 40,000 high-frequency words in the corpus, replace rest low-frequency word with UNK tag. We use NIST MT06 as the validation set, NIST MT08 as the test set.\\ntrain valid test FBIS MT06 MT08\\nSequences 219,280 6,560 5,424 Tokens 7,877,650 190,065 166,937 Types 49,210 8,476 9,576\\nTable 1: FBIS and NIST MT Corpus statistics.\\n 3.2 Model Configuration\\n  Baseline In this paper, we utilize state-of-theart LSTM framework on language model proposed by Zaremba (2014) as the baseline model. Firstly, we stack 2 layers of LSTMs, to explore more abstracted patterns which are not supposed to be discovered by a single layer. Secondly, to increase the model’s capability of noise tolerance and reduce the overfitting to training data, we introduce dropout (Hinton et al., 2012) before and after each recurrent layer. We choose dropout rate as 0.5 without tuning. For training, we used AdaDelta (Zeiler, 2012). To handle nan and inf which occasionally occur in gradient, we normalize the gradient of each batch to 1.0 and drop the parameter updates on such batches. To determine when to stop training, we set patience to 100. We set the dimension of both word embedding and hidden dimensions to 200. We initialized all parameters according to recommendations given in (Zaremba et al., 2014) and blocks(van Merriënboer et al., 2015). For all models, we used word embedding, hidden dimensions of 200 and 2-layer LSTMs. For both models, we choose dropout rate as 0.5. For training, we used AdaDelta and normalize the gradient to 1.0, set patience to 100. We initialized all parameters according to recommendations given in Zaremba et al. (2014).\\nPhrasal RNN We configure our model exactly as the baseline model, except adding an extra RNN\\n1http://www.fit.vutbr.cz/˜imikolov/ rnnlm/\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\npyramid layer above baseline’s 2-layer LSTM. We also add dropout between 2nd LSTM layer and RNN pyramid layer. We set dimension of hidden state in RNN pyramid layer as 200 either. We utilize Gated Recurrent Unit (GRU) (Chung et al., 2014) to construct RNN pyramid layer. We also tried a simplified version of GRU to build the pyramid (pRNNv in table 2), which achieves the best result.\\n 3.3 Results\\n  Model Perplexity 5-gram, KN5 141.2 FFNN-LM 140.2 RNN 124.7 LSTM 126 genCNN 116.4 LSTM (baseline) 106.9 pRNN 97.6 pRNNv 94.5\\nTable 2: Perplexity on PENN TREEBANK, where the top 5 rows of numbers are results reported in previous work, our baseline and new pRNN model are in last three rows.\\nModel Perplexity 5-gram, KN5 278.6 FFNN-LM(5-gram) 248.3 FFNN-LM(20-gram) 228.2 RNN 223.4 LSTM 206.9 genCNN 181.2 LSTM (baseline) 171.8 pRNN 161.5\\nTable 3: Perplexity on FBIS data set, where the top 5 rows of numbers are results reported in previous work, our baseline and new pRNN model are in last two rows.\\nWe report our perplexities result of language model in table (2 and 3). We calculate perplexity over a sequence [w1, . . . , wn] with\\nPPL = exp ( − log(Prob(w n 1 ))\\nn\\n) (9)\\n(including the end of sentence (EOS) symbol). pRNN and its variant outperform over 10 points of\\nppl over a strong baseline on both PTB and FBIS English data set.\\n 4 MT Experiment\\n  \\n 4.1 Data\\n  We evaluate all three models, PBSMT, RNNsearch, pRNN on the same data set. We utilize 1.25M sentence pairs, which are extracted from LDC corpora as training data. There are 34.5 English words and 27.9M Chinese words in the training data. We select NIST 2002 (MT02) data set as our development set, the NIST 2003 (MT03), NIST 2004 (MT04), NIST 2005 (MT05), NIST 2006 (MT06) and NIST 2008 (MT08) as test sets. When training neural networks, we limit the size of vocabularies of both source and target side to the most frequent 16K words. All rest low-frequency words are replaced with UNK tag. Chinese vocabulary covers approximately 95.8% of the corpora. English vocabulary covers approximately 98.3% of the corpora.\\n 4.2 Model Configuration\\n  Baseline In this paper, we use an open-source implementation (Meng et al., 2015) of RNNsearch (Bahdanau et al., 2014) as baseline model. To increase the model’s capability of noise tolerance and reduce the overfitting to training data, we introduce dropout (Hinton et al., 2012) before softmax layer, and set the dropout rate equal to 0.5. We choose dropout rate as 0.5 without tuning. For training, we used AdaDelta (Zeiler, 2012) and normalize the gradient to 1.0, . To handle nan and inf which occasionally occur in gradient, we normalize the gradient of each batch to 1.0 and drop the parameter updates on such batches. We set the dimension of both source and target word embedding as 620, and hidden dimensions to 1000. We initialized all parameters according to recommendations given in (Bahdanau et al., 2014). We also introduce the phrase-based model of Moses (Koehn et al., 2007) as a secondary baseline too.\\nPhrase-based NMT We configure our model exactly as the baseline model, except adding an extra RNN pyramid as a secondary source encoder. As the limitation on the memory of GPU, we keep only phrases ended at eos into consideration. Thus we name it src-pyr-last in table 4. We set the dimension of hidden dimensions inside pyramid as 1000 too. We utilize Gated Recur-\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nModels MT02 MT03 MT04 MT05 MT06 MT08 Test Avg. Diff moses 33.41 31.61 33.48 30.75 31.07 23.37 30.056 +0.754 RNNsearch (groundhog) 32.32 29.02 31.25 28.32 27.99 20.29 27.374 -1.928 RNNsearch (baseline) 34.28 30.61 33.24 30.66 29.83 22.17 29.302 +0.000 pRNN (src-pyr-last) 35.48 31.61 34.40 31.96 31.36 22.82 30.430 +1.128 pRNN (src-pyr-all) 35.49 32.08 34.51 31.81 30.91 22.86 30.434 +1.132\\nTable 4: BLEU score on 1.25M training corpus with 16k dictionary on both source and target side. The above two lines of the table are results of open-source machine translation systems. Bold numbers indicate the best results on the data set (column). pRNNs are better than original RNNsearch model baseline (in-house reimplemented). We find it is interesting that results of src-pyr-all are only slightly better than src-pyr-last, we guess this is due to the limited discriminative power of simple attention mechanism when meeting large number of complex candidates.\\nrent Unit (GRU) (Chung et al., 2014) to construct RNN pyramid layer.\\nFor a fair comparison, we run baseline system (RNNsearch) many times (not epoch) and report only the best one. We only run PBNMT once.\\n 4.3 Results\\n  We report our BLEU result of three models in table (4). We use the case-insensitive 4-gram NIST BLEU (Papineni et al., 2002) score given by mteval v11.pl pRNNs outperforms both PBSMT and Encoder-Decoder model.\\n 5 Related Work\\n  \\n 5.1 Relation to Previous Attempts on\\n  Structural Information\\nIn the last two decades, to achieve better performance, researchers incorporated more and more rich structural information into conventional model (Jelinek and Lafferty, 1991; Chelba, 1997; Chelba and Jelinek, 2000; Emami and Jelinek, 2005). This trend is more clear in statistical machine translation (SMT) community, from wordbased SMT (Brown et al., 1993) to phrase-based SMT (Koehn et al., 2003), hierarchical phrasebased SMT (Chiang, 2005) and syntax-based SMT (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Xie et al., 2011) (forest-based (Mi and Huang, 2008; Mi et al., 2008)). among them, phrase-based SMT (Zens et al., 2002; Koehn et al., 2003) was the most widely adopted translation model. The Same trend can be observed in neural network strand. There were many works which successfully modeled structure in neural network on parsing (Dyer et al., 2016; Emami and Jelinek, 2005; Henderson, 2004; Titov and Hender-\\nson, 2007; Buys and Blunsom, 2015) or language modeling tasks (Dyer et al., 2016; Chelba and Jelinek, 2000; Emami and Jelinek, 2005; Chelba, 1997).\\nIn neural machine translation (NMT) area, the situation is more complex. In recent years, the most popular and success NMT model is EncoderDecoder model (Bahdanau et al., 2014; Sutskever et al., 2014). Which has achieved competitive or better results in many translation tasks(Luong et al., 2015a,b). However, beyond sequential surface words, there are latent nest structures in natural language (Chomsky, 1957). One direction to explore is to introduce sub-word structures(CostaJussà and Fonollosa, 2016; Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015), such as characters. The most important reason to dig into sub-word is to handle out-of-vocabulary word problem. This problem is rooted in the limited size of vocabulary, which utilized by NMT mapping symbols to real-valued dense vector.\\nAnother direction is to explore hyper-word structure. Authors of (Eriguchi et al., 2016) adopted parsing tree in encoder phase. However, this method depends heavily on human-labeled data, which is always expensive and limited in scale. Authors of (Stahlberg et al., 2016) introduce hierarchical phrase-based (HPB) model as the guider of search space. However, the HPB model and NMT model are trained separately and combined only when decoding. Compare this to the previous method, (Eriguchi et al., 2016) can be categorized into introducing external data, (Stahlberg et al., 2016) can be categorized into introducing external model. In an ideal situation, all external model can be replaced by a neural net-\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nwork with equal ability.\\n 5.2 Similarity to Deep Memory Network\\n  Deep Memory Network is an effective implementation of the neural turing machine. When they use state machine such as GRU or LSTM to read from one memory and write to another, the memory IO addresses are either content-based (via attention mechanism) or location-based (actually sequentially cell-by-cell) (Meng et al., 2015). However, there are much more other methods in the location-based category.\\nCell-by-cell vs. Incremental If we consider the hidden of one time step inside pyramid RNN as memory, we can name the operations as incremental read and write. The intuition behind incremental addressing is, when we read little, we know little, we only have the ability to write little. But when we read more, we know more, we are gone to have the ability to write more.\\n 6 Discussion\\n  Our experiments clearly show that the proposed pRNN model is quite effective in language modeling and machine translation. This is the because of:\\n• when model predicting, it is provided with all candidate phrases as structure information rather than just surface sequential words.\\n• utilizing attention mechanism to compare and combine to get the weighted sum which best fit for predicting next hidden state.\\nThe most significant question that remains is how well the quality of forest generated as a by-product of pRNN, will it get a better result than other supervised parsing model trained on human label data.\\n 7 Conclusion\\n  We introduced phrasal recurrent neural network, an RNN model with all potential candidate phrases considered. Our model does not require any human labeled data to construct the structures. It outperforms the state-of-the-art LSTM language models. Our model does not require any external resources such as human labeled data or word align model to construct the phrases. It outperforms both state-of-the-art PBSMT and RNNsearch model.\\nWe make two main contributions:\\n• Instead of packing all information in distributed representation and internal hidden status, which are computing-friendly, we try to represent natural structure in an explicit way, which are human-friendly.\\n• Instead of stacking deeper and deeper layers of RNNs, we explore the possibility of network construction in another dimension: making RNN sequences parallel.\\n Abstract We propose a new, simple, yet effective framework, phrasal recurrent neural networks (pRNN), for language modeling and machine translation. Different from previous RNN-based language models, pRNNs store the sentential history as a set of candidate phrases with different lengths that precede the word to predict. To represent phrases as fix-length realvalued vectors, we build the RNN pyramid, which is composed of shifted parallel RNN sequences. When predicting the next word, pRNNs employ a soft attention mechanism to selective and combine the suggestions of candidate phrases. We test our model on language model and machine translation tasks. Our model leads to an improvement of over 10 points in perplexity both on standard Penn Treebank and FBIS English data set over a state-of-the-art LSTM language modeling baseline. We also apply pRNNs to encode the source sentence of machine translation besides a conventional bi-direction encoder, which improves over the Moses (phrase-based statistical model) and a strong sequence-to-sequence baseline in the Chinese-English machine translation task.   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         None  1 000\\n011\\n012\\n013\\n014\\n015\\n016\\n017\\n018\\n019\\n020\\n021\\n022\\n023\\n024\\n025\\n026\\n027\\n028\\n029\\n030\\n031\\n032\\n033\\n034\\n035\\n036\\n037\\n038\\n039\\n040\\n041\\n042\\n043\\n044\\n045\\n046\\n047\\n048\\n049\\n068\\n069\\n070\\n071\\n072\\n073\\n074\\n075\\n076\\n077\\n078\\n079\\n080\\n081\\n082\\n083\\n084\\n085\\n086\\n087\\n088\\n089\\n090\\n091\\n092\\n093\\n094\\n095\\n096\\n097\\n098\\n099\\n 1 Introduction\\n  Multi-task learning is an effective approach to improve the performance of a single task with the help of other related tasks. Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information frommultiple tasks.\\nHowever, most existing work on multi-task learning attempts to divide the features of different tasks into private and shared spaces, merely based on whether parameters of some components should be shared. As shown in Figure 1- (a), the general shared-private model introduces two feature spaces for any task: one is used to\\nstore task-dependent features, the other is used to capture shared features. The major limitation of this framework is that the shared feature space could contain some unnecessary task-specific features, while some sharable features could also be mixed in private space, suffering from feature redundancy. Taking the following two sentences as examples, which are extracted from two different sentiment classification tasks: Movie reviews and Baby products reviews.\\nThe infantile cart is simple and easy to use. This kind of humour is infantile and boring.\\nThe word “infantile” indicates negative sentiment in Movie task while it is neutral in Baby task. However, the general shared-private model could place the task-specific word “infantile” in a shared space, leaving potential hazards for other tasks. Additionally, the capacity of shared space could also be wasted by some unnecessary features. To address this problem, in this paper we propose an adversarial multi-task framework, in which the shared and private feature spaces are inherently disjoint by introducing orthogonality constraints. Specifically, we design a generic sharedprivate learning framework to model the text se-\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nquence. To prevent the shared and private latent feature spaces from interfering with each other, we introduce two strategies: adversarial training and orthogonality constraints. The adversarial training is used to ensure that the shared feature space simply contains common and task-invariant information, while the orthogonality constraint is used to eliminate redundant features from the private and shared spaces.\\nThe contributions of this paper can be summarized as follows.\\n1. Proposed model divides the task-specific and shared space in a more precise way, rather than roughly sharing parameters. 2. We extend the original binary adversarial training to multi-class, which not only enables multiple tasks to be jointly trained, but allows us to utilize unlabeled data. 3. We can condense the shared knowledge among multiple tasks into an off-the-shelf neural layer, which can be easily transferred to new tasks.\\n 2 Recurrent Models for Text Classification\\n  There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks.\\nLong Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the issue of learning long-term dependencies. While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al., 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.\\nWe define the LSTM units at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. d is the number of the LSTM units. The elements of the gating vectors it, ft and ot are in [0, 1].\\nThe LSTM is precisely specified as follows.\\n c̃t ot it ft  =  tanh σ σ σ (Wp [ xtht−1 ] + bp ) , (1)\\nct = c̃t ⊙ it + ct−1 ⊙ ft, (2) ht = ot ⊙ tanh (ct) , (3)\\nwhere xt ∈ Re is the input at the current time step; Wp ∈ R4d×(d+e) and bp ∈ R4d are parameters of affine transformation; σ denotes the logistic sigmoid function and ⊙ denotes elementwise multiplication. The update of each LSTM unit can be written precisely as follows:\\nht = LSTM(ht−1,xt, θp). (4)\\nHere, the function LSTM(·, ·, ·, ·) is a shorthand for Eq. (1-3), and θp represents all the parameters of LSTM.\\nText Classification with LSTM Given a text sequence x = {x1, x2, · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi. The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes.\\nŷ = softmax(WhT + b) (5)\\nwhere ŷ is prediction probabilities, W is the weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi, yi), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions.\\nL(ŷ, y) = − N∑ i=1 C∑ j=1 yji log(ŷ j i ), (6)\\nwhere yji is the ground-truth label; ŷ j i is prediction probabilities, and C is the class number.\\n 3 Multi-task Learning for Text Classification\\n  The goal of multi-task learning is to utilizes the correlation among these related tasks to improve\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\nsoftmax Lmtask\\nLSTM\\nsoftmax Lntask\\nxm xn\\n(a) Fully Shared Model (FS-MTL)\\nxm\\nxn\\nLSTM\\nLSTM\\nLSTM\\nsoftmax\\nsoftmax\\nLmtask\\nLntask\\n(b) Shared-Private Model (SP-MTL)\\nFigure 2: Two architectures for learning multiple tasks. Yellow and gray boxes represent shared and private LSTM layers respectively.\\nclassification by learning tasks in parallel. To facilitate this, we give some explanation for notations used in this paper. Formally, we refer to Dk as a dataset with Nk samples for task k. Specifically,\\nDk = {(xki , yki )} Nk i=1 (7)\\nwhere xki and y k i denote a sentence and corresponding label for task k.\\n 3.1 Two Sharing Schemes for Sentence Modeling\\n  The key factor of multi-task learning is the sharing scheme in latent feature space. In neural network based model, the latent features can be regarded as the states of hidden neurons. Specific to text classification, the latent features are the hidden states of LSTM at the end of a sentence. Therefore, the sharing schemes are different in how to group the shared features. Here, we first introduce two sharing schemes with multi-task learning: fully-shared scheme and shared-private scheme.\\nFully-SharedModel (FS-MTL) In fully-shared model, we use a single shared LSTM layer to extract features for all the tasks. For example, given two tasks m and n, it takes the view that the features of taskm can be totally shared by task n and vice versa. This model ignores the fact that some features are task-dependent. Figure 2a illustrates the fully-shared model.\\nShared-Private Model (SP-MTL) As shown in Figure 2b, the shared-private model introduces two feature spaces for each task: one is used to store task-dependent features, the other is used\\nto capture task-invariant features. Accordingly, we can see each task is assigned a private LSTM layer and shared LSTM layer. Formally, for any sentence in task k, we can compute its shared representation skt and task-specific representation h k t as follows:\\nskt = LSTM(xt, s k t−1, θs), (8)\\nhkt = LSTM(xt,h m t−1, θk) (9)\\nwhere LSTM(., θ) is defined as Eq. (4). The final features are concatenation of the features from private space and shared space.\\n 3.2 Task-Specific Output Layer\\n  For a sentence in task k, its feature h(k), emitted by the deep muti-task architectures, is ultimately fed into the corresponding task-specific softmax layer for classification or other tasks. The parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions on all the tasks. The loss Ltask can be computed as:\\nLTask = K∑ k=1 αkL(ŷ (k), y(k)) (10)\\nwhere αk is the weights for each task k respectively. L(ŷ, y) is defined as Eq. 6.\\n 4 Incorporating Adversarial Training\\n  Although the shared-private model separates the feature space into the shared and private spaces, there is no guarantee that sharable features can not exist in private feature space, or vice versa. Thus, some useful sharable features could be ignored in shared-private model, and the shared feature space is also vulnerable to contamination by some taskspecific information. Therefore, a simple principle can be applied into multi-task learning that a good shared feature space should contain more common information and no task-specific information. To address this problem, we introduce adversarial training into multi-task framework as shown in Figure 3 (ASPMTL).\\n 4.1 Adversarial Network\\n  Adversarial networks have recently surfaced and are first used for generative model (Goodfellow\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nxm\\nxn\\nLSTM\\nLSTM\\nLSTM\\nLDiff LAdvLDiff\\nsoftmax\\nsoftmax\\nLmtask\\nLntask\\nFigure 3: Adversarial shared-private model. Yellow and gray boxes represent shared and private LSTM layers respectively.\\net al., 2014). The goal is to learn a generative distribution pG(x) that matches the real data distribution Pdata(x) Specifically, GAN learns a generative network G and discriminative model D, in which G generates samples from the generator distribution pG(x). and D learns to determine whether a sample is from pG(x) or Pdata(x). This min-max game can be optimized by the following risk:\\nϕ = min G max D\\n( Ex∼Pdata [logD(x)]\\n+ Ez∼p(z)[log(1−D(G(z)))] )\\n(11)\\nWhile originally proposed for generating random samples, adversarial network can be used as a general tool to measure equivalence between distributions (Taigman et al., 2016). Formally, (Ajakan et al., 2014) linked the adversarial loss to the H-divergence between two distributions and successfully achieve unsupervised domain adaptation with adversarial network. Motivated by theory on domain adaptation (Ben-David et al., 2010, 2007; Bousmalis et al., 2016) that a transferable feature is one for which an algorithm cannot learn to identify the domain of origin of the input observation.\\n 4.2 Task Adversarial Loss for MTL\\n  Inspired by adversarial networks (Goodfellow et al., 2014), we proposed an adversarial sharedprivate model for multi-task learning, in which a shared recurrent neural layer is working adversarially towards a learnable multi-layer perceptron, preventing it from making an accurate prediction about the types of tasks. This adversarial training encourages shared space to be more pure and ensure the shared representation not be contaminated by task-specific features.\\nTask Discriminator Discriminator is used to map the shared representation of sentences into a\\nprobability distribution, estimating what kinds of tasks the encoded sentence comes from.\\nD(skT , θD) = softmax(b+Us k T ) (12)\\nwhereU ∈ Rd×d is a learnable parameter and b ∈ Rd is a bias.\\nAdversarial Loss Different with most existing multi-task learning algorithm, we add an extra task adversarial loss LAdv to prevent task-specific feature from creeping in to shared space. The task adversarial loss is used to train a model to produce shared features such that a classifier cannot reliably predict the task based on these features. The original loss of adversarial network is limited since it can only be used in binary situation. To overcome this, we extend it to multi-class form, which allow our model can be trained together with multiple tasks:\\nLAdv = min θs\\n( λmax\\nθD ( K∑ k=1 Nk∑ i=1 dki log[D(E(x k))])\\n) (13)\\nwhere dki denotes the ground-truth label indicating the type of the current task. Here, there is a minmax optimization and the basic idea is that, given a sentence, the shared LSTM generates a representation to mislead the task discriminator. At the same time, the discriminator tries its best to make a correct classification on the type of task. After the training phase, the shared feature extractor and task discriminator reach a point at which both cannot improve and the discriminator is unable to differentiate among all the tasks.\\nSemi-supervised Learning Multi-task Learning We notice that the LAdv requires only the input sentence x and does not require the corresponding label y, which makes it possible to combine our model with semi-supervised learning. Finally, in this semi-supervised multi-task learning framework, our model can not only utilize the data from related tasks, but can employ abundant unlabeled corpora.\\n 4.3 Orthogonality Constraints\\n  We notice that there is a potential drawback of the above model. That is, the task-invariant features can appear both in shared space and private space. Motivated by recently work(Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nDataset Train Dev. Test Unlab. Avg. L Vocab.\\nBooks 1400 200 400 2000 159 62K Elec. 1398 200 400 2000 101 30K DVD 1400 200 400 2000 173 69K Kitchen 1400 200 400 2000 89 28K Apparel 1400 200 400 2000 57 21K Camera 1397 200 400 2000 130 26K Health 1400 200 400 2000 81 26K Music 1400 200 400 2000 136 60K Toys 1400 200 400 2000 90 28K Video 1400 200 400 2000 156 57K Baby 1300 200 400 2000 104 26K Mag. 1370 200 400 2000 117 30K Soft. 1315 200 400 2000 129 26K Sports 1400 200 400 2000 94 30K IMDB 1400 200 400 2000 269 44K MR 1400 200 400 2000 21 12K\\nTable 1: Statistics of the 16 datasets. The columns 2-5 denote the number of samples in training, development, test and unlabeled sets. The last two columns represent the average length and vocabulary size of corresponding dataset.\\non shared-private latent space analysis, we introduce orthogonality constraints, which penalize redundant latent representations and encourages the shared and private extractors to encode different aspects of the inputs.\\nAfter exploring many optional methods, we find below loss is optimal, which is used by Bousmalis et al. (2016) and achieve a better performance:\\nLdiff = K∑ k=1 ∥∥∥Sk⊤Hk∥∥∥2 F , (14)\\nwhere ∥ · ∥2F is the squared Frobenius norm. S k and Hk are two matrics, whose rows are the output of shared extractor Es(, ; θs) and task-specific extrator Ek(, ; θk) of a input sentence.\\n 4.4 Put It All Together\\n  The final loss function of our model can be written as:\\nL = LTask + λLAdv + γLDiff (15)\\nwhere λ and γ are hyper-parameter. The networks are trained with backpropagation and this minimax optimization becomes possible via the use of a gradient reversal layer (Ganin and Lempitsky, 2015).\\n 5 Experiment\\n  \\n 5.1 Dataset\\n  To make an extensive evaluation, we collect 16 different datasets from several popular review corpora. The first 14 datasets are product reviews, which contain Amazon product reviews from different domains, such as Books, DVDs, Electronics, ect. The goal is to classify a product review as either positive or negative. These datasets are collected based on the raw data 1 provided by (Blitzer et al., 2007). Specifically, we extract the sentences and corresponding labels from the unprocessed original data 2. The only preprocessing operation of these sentences is tokenized using the Stanford tokenizer 3. The remaining two datasets are about movie reviews. The IMDB dataset4 consists of movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. The MR dataset also consists of movie reviews from rotten tomato website with two classes 5(Pang and Lee, 2005). All the datasets in each task are partitioned randomly into training set, development set and testing set with the proportion of 70%, 20% and 10% respectively. The detailed statistics about all the datasets are listed in Table 1.\\n 5.2 Competitor Methods for Multi-task Learning\\n  The multi-task frameworks proposed by previous works are various while not all can be applied to the tasks we focused. Nevertheless, we chose two most related neural models for multi-task learning and implement them as competitor methods.\\n• MT-CNN: This model is proposed by Collobert and Weston (2008) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific.\\n1https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/\\n2Blitzer et al. (2007) also provides two extra processed datasets with the format of Bag-of-Words, which are not proper for neural-based models.\\n3http://nlp.stanford.edu/software/ tokenizer.shtml\\n4https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/unprocessed.tar.gz\\n5https://www.cs.cornell.edu/people/ pabo/movie-review-data/.\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nTask Single Task Multiple Tasks\\nLSTM BiLSTM sLSTM Avg. MT-DNN MT-CNN FS-MTL SP-MTL ASP-MTL\\nBooks 20.5 19.0 18.0 19.2 17.7(−1.5) 15.6(−3.6) 17.5(−1.7) 18.7(−0.5) 13.0(−6.2) Electronics 19.5 21.5 23.3 21.4 18.2(−3.2) 16.9(−4.5) 14.3(−7.1) 12.3(−9.1) 11.0(−10.4) DVD 18.3 19.5 22.0 19.9 15.8(−4.1) 16.1(−3.8) 16.5(−3.4) 16.1(−3.8) 12.6(−7.3) Kitchen 22.0 18.8 19.5 20.1 19.2(−0.9) 16.8(−3.3) 14.0(−6.1) 14.8(−5.3) 12.8(−7.3) Apparel 16.8 14.0 16.3 15.7 14.9(−0.8) 16.1(+0.4) 15.5(−0.2) 13.4(−2.3) 11.3(−4.4) Camera 14.8 14.0 15.0 14.6 13.7(−0.9) 14.0(−0.6) 13.5(−1.1) 12.1(−2.5) 8.7(−5.9) Health 15.5 21.3 16.5 17.8 14.3(−3.5) 12.9(−4.9) 12.0(−5.8) 12.8(−5.0) 10.9(−6.9) Music 23.3 22.8 23.0 23.0 15.3(−7.7) 16.3(−6.7) 18.8(−4.2) 17.0(−6.0) 17.4(−5.6) Toys 16.8 15.3 16.8 16.3 12.1(−4.2) 10.9(−5.4) 15.5(−0.8) 14.9(−1.4) 11.2(−5.1) Video 18.5 16.3 16.3 17.0 15.0(−2.0) 18.7(+1.7) 16.3(−0.7) 16.8(−0.2) 14.5(−2.5) Baby 15.3 16.5 15.8 15.9 12.1(−3.8) 12.4(−3.5) 12.0(−3.9) 13.2(−2.7) 10.2(−5.7) Magazines 10.8 8.5 12.3 10.5 10.6(+0.1) 12.3(+1.8) 7.5(−3.0) 8.1(−2.4) 7.6(−2.9) Software 15.3 14.3 14.5 14.7 14.4(−0.3) 13.4(−1.3) 13.8(−0.9) 13.1(−1.6) 12.7(−2.0) Sports 18.3 16.0 17.5 17.3 16.8(−0.5) 16.1(−1.2) 14.5(−2.8) 12.7(−4.6) 13.3(−4.0) IMDB 18.3 15.0 18.5 17.3 16.7(−0.6) 13.7(−3.6) 17.5(+0.2) 15.2(−2.1) 14.2(−3.1) MR 27.3 25.3 28.0 26.9 24.5(−2.4) 25.5(−1.4) 25.3(−1.6) 24.1(−2.8) 22.7(−4.2)\\nAVG 18.2 17.4 18.3 18.0 15.7(−2.3) 15.5(−2.5) 15.3(−2.7) 14.7(−3.3) 12.8(−5.2)\\nTable 2: Error rates of our models on 16 datasets against typical baselines. The numbers in brackets represent the improvements relative to the average performance (Avg.) of three single task baselines.\\n• MT-DNN: The model is proposed by Liu et al. (2015) with bag-of-words input and multi-layer perceptrons, in which a hidden layer is shared.\\n 5.3 Hyperparameters\\n  The word embeddings for all of the models are initialized with the 200d GloVe vectors (840B token version, (Pennington et al., 2014)). The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. The minibatch size is set to 16.\\nFor each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.1, 0.01], λ ∈ [0.01, 0.1], and γ ∈ [0.01, 0.1]. Finally, we chose the learning rate as 0.01, λ as 0.05 and γ as 0.01.\\n 5.4 Performance Evaluation\\n  Table 2 shows the error rates on 16 text classification tasks. The column of “Single Task” shows the results of vanilla LSTM, bidirectional LSTM (BiLSTM), stacked LSTM (sLSTM) and the average error rates of previous three models. The column of “Multiple Tasks” shows the results achieved by corresponding multi-task models. From this table, we can see that the performance of most tasks can be improved with a large margin with the help of multi-task learning, in which our model achieves the lowest error rates.\\nMore concretely, compared with SP-MTL, ASPMTL achieves 5.2% average improvement surpassing SP-MTL with 1.9%, which indicates the importance of adversarial learning. It is noteworthy that for FS-MTL, the performances of some tasks are degraded, since this model puts all private and shared information into a unified space.\\n 5.5 Shared Knowledge Transfer\\n  With the help of adversarial learning, the shared feature extractor Es can generate more pure taskinvariant representations, which can be considered as off-the-shelf knowledge and then be used for unseen new tasks. To test the transferability of our learned shared extractor, we also design an experiment, in which we take turns choosing 15 tasks to train our model MS with multi-task learning, then the learned shared layer are transferred to a second network MT that is used for the remaining one task. The parameters of transferred layer are kept frozen, and the rest of parameters of the network MT are randomly initialized. More formally, we investigate two mechanisms towards the transferred shared extractor. As shown in Figure 4. The first one Single Channel (SC) model consists of one shared feature extractor Es from MS , then the extracted representation will be sent to an output layer. By contrast, the BiChannel (BC) model introduces an extra LSTM layer to encode more task-specific information. To evaluate the effectiveness of our introduced adver-\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nSource Tasks Single Task Transfer Models\\nLSTM BiLSTM sLSTM Avg. SP-MTL-SC SP-MTL-BC ASP-MTL-SC ASP-MTL-BC\\nϕ (Books) 20.5 19.0 18.0 19.2 17.8(−3.6) 16.2(−3.0) 16.7(−2.5) 13.3(−5.9) ϕ (Electronics) 19.5 21.5 23.3 21.4 15.1(−4.5) 14.6(−6.8) 15.1(−6.3) 14.9(−6.5) ϕ (DVD) 18.3 19.5 22.0 19.9 14.7(−3.8) 15.5(−4.4) 12.1(−7.8) 12.4(−7.5) ϕ (Kitchen) 22.0 18.8 19.5 20.1 15.0(−3.3) 16.6(−3.5) 14.6(−5.5) 14.1(−6.0) ϕ (Apparel) 16.8 14.0 16.3 15.7 14.9(+0.4) 12.3(−3.4) 11.6(−4.1) 13.6(−2.1) ϕ (Camera) 14.8 14.0 15.0 14.6 13.1(−0.6) 12.1(−2.5) 11.6(−3.0) 10.3(−4.3) ϕ (Health) 15.5 21.3 16.5 17.8 14.1(−4.9) 14.2(−3.6) 12.2(−5.6) 10.5(−7.3) ϕ (Music) 23.3 22.8 23.0 23.0 19.9(−6.7) 17.9(−5.1) 16.4(−6.6) 18.2(−4.8) ϕ (Toys) 16.8 15.3 16.8 16.3 13.8(−5.4) 12.2(−4.1) 13.0(−4.7) 11.2(−5.1) ϕ (Video) 18.5 16.3 16.3 17.0 14.2(+1.7) 15.1(−1.9) 14.8(−2.2) 14.8(−2.2) ϕ (Baby) 15.3 16.5 15.8 15.9 16.6(−3.5) 16.9(+1.0) 11.5(−4.4) 10.0(−5.9) ϕ (Magazines) 10.8 8.5 12.3 10.5 10.6(+1.8) 10.2(−0.3) 8.6(−1.9) 9.7(−0.8) ϕ (Software) 15.3 14.3 14.5 14.7 13.0(−1.3) 12.7(−2.0) 14.3(−0.4) 11.1(−3.6) ϕ (Sports) 18.3 16.0 17.5 17.3 16.3(−1.2) 16.2(−1.1) 13.4(−3.9) 13.6(−3.7) ϕ (IMDB) 18.3 15.0 18.5 17.3 12.4(−3.6) 12.8(−4.5) 12.5(−4.8) 13.3(−4.0) ϕ (MR) 27.3 25.3 28.0 26.9 26.0(−1.4) 26.5(−0.4) 22.7(−4.2) 23.5(−3.4)\\nAVG 18.2 17.4 18.3 18.0 15.5(−2.5) 15.1(−2.9) 13.6(−4.2) 13.4(−4.6)\\nTable 3: Error rates of our models on 16 datasets against vanilla multi-task learning. ϕ (Books) means that we transfer the knowledge of the other 15 tasks to the target task Books.\\nxt LSTM softmax\\nEs\\n(a) Single Channel\\nxt LSTM\\nLSTM\\nsoftmax\\nEs\\n(b) Bi-Channel\\nFigure 4: Two transfer strategies using a pretrained shared LSTM layer. Yellow box denotes shared feature extractor Es trained by 15 tasks.\\nsarial training framework, we also make a comparison with vanilla multi-task learning method.\\nResults and Analysis As shown in Table 3, we can see the shared layer from ASP-MTL achieves a better performance compared with SP-MTL. Besides, for the two kinds of transfer strategies, the Bi-Channel model performs better. The reason is that the task-specific layer introduced in the BiChannel model can store some private features. Overall, the results indicate that we can save the existing knowledge into a shared recurrent layer using adversarial multi-task learning, which is quite useful for a new task.\\n 5.6 Visualization\\n  To get an intuitive understanding of how the introduced orthogonality constraints worked compared with vanilla shared-private model, we design an experiment to examine the behaviors of neurons from private layer and shared layer. More concretely, we refer to htj as the activation of the j-\\nneuron at time step t, where t ∈ {1, . . . , n} and j ∈ {1, . . . , d}. By visualizing the hidden state hj and analyzing the maximum activation, we can find what kinds of patterns the current neuron focuses on. Figure 5 illustrates this phenomenon. Here, we randomly sample a sentence from the validation set of Baby task and analyze the changes of the predicted sentiment score at different time steps, which are obtained by SP-MTL and our proposed model. Additionally, to get more insights into how neurons in shared layer behave diversely towards different input word, we visualize the activation of two typical neurons. For the positive sentence “Five stars, my baby can fall asleep soon in the stroller”, both models capture the informative pattern “Five stars” 6. However, SP-MTL makes a wrong prediction due to misunderstanding of the word “asleep”. By contrast, our model makes a correct prediction and the reason can be inferred from the activation of Figure 5-(b), where the shared layer of SP-MTL is so sensitive that many features related to other tasks are included, such as ”asleep”, which misleads the final prediction. This indicates the importance of introducing adversarial learning to prevent the shared layer from being contaminated by task-specific features.\\n6For this case, the vanilla LSTM also give a wrong answer due to ignoring the feature “Five stars”.\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nFive stars , my baby can fall asleep soon in the stroller 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 SP-MTL Ours\\n(a) Predicted Sentiment Score by Two Models (b) Behaviours of Neuron hs18 and h s 21\\nFigure 5: (a) The change of the predicted sentiment score at different time steps. Y-axis represents the sentiment score, while X-axis represents the input words in chronological order. The darker grey horizontal line gives a border between the positive and negative sentiments. (b) The blue heat map describes the behaviour of neuron hs18 from shared layer of SP-MTL, while the purple one is used to show the behaviour of neuron hs21, which belongs to the shared layer of our model.\\nModel Shared Layer Task-Movie Task-Baby\\nSP-MTL\\ngood, great bad, love, simple, cut, slow, cheap, infantile good, great, well-directed, pointless, cut, cheap, infantile love, bad, cute, safety, mild, broken simple\\nASP-MTL good, great, love, bad poor well-directed, pointless, cut, cheap, infantile cute, safety, mild, broken simple\\nTable 4: Typical patterns captured by shared layer and task-specific layer of SP-MTL and ASP-MTL models on Movie and Baby tasks.\\nWe also list some typical patterns captured by neurons from shared layer and task-specific layer in Table 4, and we have observed that: 1) for SP-MTL, if some patterns are captured by taskspecific layer, they are likely to be placed into shared space. Clearly, suppose we have many tasks to be trained jointly, the shared layer bear much pressure and must sacrifice substantial amount of capacity to capture the patterns they actually do not need. Furthermore, some typical taskinvariant features also go into task-specific layer. 2) for ASP-MTL, we find the features captured by shared and task-specific layer have a small amount of intersection, which allows these two kinds of layers can work effectively.\\n 6 Related Work\\n  There are two threads of related work. One thread is multi-task learning with neural network. Neural networks based multi-task learning has been proven effective in many NLP problems (Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2015, 2016). In most of these models, the\\nlower layers are shared across all tasks, while top layers are task-specific. These work has potential limitation of just learning a shared space solely on sharing parameters, while our model introduce two strategies to learn the clear and non-redundant shared-private space. Another thread of work is adversarial network. Adversarial networks have recently surfaced as a general tool measure equivalence between distributions and it has proven to be effective in a variety of tasks. Ajakan et al. (2014); Bousmalis et al. (2016) applied adverarial training to domain adaptation, aiming at transferring the knowledge of one source domain to target domain. Park and Im (2016) proposed a novel approach for multimodal representation learning which uses adversarial back-propagation concept. Different from these models, our model aims to find task-invariant sharable information for multiple related tasks using adversarial training strategy. Moreover, we extend binary adversarial training to multi-class, which enable multiple tasks to be jointly trained.\\n 7 Conclusion\\n  In this paper, we have proposed an adversarial multi-task learning framework, in which the taskspecific and task-invariant features are learned non-redundantly, therefore capturing the sharedprivate separation of different tasks. We have demonstrated the effectiveness of our approach by applying our model to 16 different text classification tasks. We also perform extensive qualitative analysis, deriving insights and indirectly explaining the quantitative improvements in the overall performance.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899\\n Abstract Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks.   \n",
            "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Combining distributional and referential information for naming objects through cross-modal mapping and direct word prediction  1 000\\n011\\n012\\n013\\n014\\n015\\n016\\n017\\n018\\n019\\n020\\n021\\n022\\n023\\n024\\n025\\n026\\n027\\n028\\n029\\n030\\n031\\n032\\n033\\n034\\n035\\n036\\n037\\n038\\n039\\n040\\n041\\n042\\n043\\n044\\n045\\n046\\n047\\n048\\n049\\n074\\n075\\n076\\n077\\n078\\n079\\n080\\n081\\n082\\n083\\n084\\n085\\n086\\n087\\n088\\n089\\n090\\n091\\n092\\n093\\n094\\n095\\n096\\n097\\n098\\n099\\n 1 Introduction\\n  Expressions referring to objects in visual scenes typically include a word naming the type of the object: E.g., “house” in Figure 1 (a), or, as a very general type, “thingy” in Figure 1 (d). Determining such a name is is a crucial step for referring expression generation (REG) systems, as many other decisions, concerning e.g. the selection of attributes, follow from it (Dale and Reiter,\\n1995; Krahmer and Van Deemter, 2012). For a long time, however, research on REG mostly assumed the availability of symbolic representations of referent and scene, and sidestepped questions about how speakers actually choose these names, due to the lack of models capable of capturing what a word like house refers to in the real world.\\nRecent advances in image processing promise to fill this gap, with state-of-the-art computer vision systems being able to classify images into thousands of different categories (eg. Szegedy et al. (2015)). However, classification is not naming (Ordonez et al., 2016). Classification schemes are typically designed to be “flat”, with labels being on the same ontological level and, ideally, having disjunct extensions. In contrast, humans seem to be more flexible as to the chosen level of generality. Depending on the prototypicality of the object to name, and possibly other visual properties, a general name might be more or less appropriate. For instance, a robin can be named “bird”, but a penguin is better referred to as “penguin” (Rosch, 1978); along the same lines, the rather unusual building in Figure 1 that is not easy to otherwise\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\ncategorise was named “structure”. Other work at the intersection of image and language processing has investigated models that learn to directly associate visual objects with a representation of word meaning, for example through cross-modal transfer into distributional vector spaces. Under the assumption that such semantic spaces represent, in some form at least, taxonomic knowledge, this makes labels on different levels of specificity available for a given object. Moreover, if the mapping is sufficiently general, it should be able to map objects to an appropriate label, even if during training of the mapping this label has not been seen (zero-shot learning). While indeed performing with some promise on this task (Lazaridou et al., 2014), this approach does not generally outperform standard object classification with known categories (Frome et al., 2013; Norouzi et al., 2013).\\nThis paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Anonymous, in press) that treats words as individual predictors capturing referential appropriateness. We explore different ways of linking these predictors to distributional knowledge, during application and during training. We find that these improve over direct cross-modal mapping and direct visual classification in a standard and a zero-shot setup of an object naming task, as they allow for a more flexible combination of lexical and visual information when modeling referential meaning.\\n 2 Related Work\\n  Grounding and Reference An early example for work in REG that goes beyond Dale and Reiter (1995)’s dominant symbolic paradigm is Deb Roy’s work from the early 2000s (Roy et al., 2002; Roy, 2002, 2005). More recently, research on REG, which has traditionally been done on small toy data sets, is being scaled up to realworld images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015). In this paper, we focus on a particular problem posed by REG on real-world images,\\nnamely generating the appropriate head noun for a given object. Similarly, Ordonez et al. (2016) have studied the problem of deriving appropriate object names, or so-called entry-level categories, from the output of an object recognizer. Their approach focusses links abstract object categories in ImageNet to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction.\\nMulti-modal and cross-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016). Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. More related to our work are cross-modal mapping models (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014), that learn to transfer a representation of an object or image in the visual space to a vector in a distributional space. When tested on standard object recognition tasks, transfer, however, comes at a price. Frome et al. (2013) and Norouzi et al. (2013) both find that it slightly degrades performance as compared to a plain object classification using standard accuracy metrics (called flat “hit @k metric” in their paper). Interestingly though, Frome et al. (2013) report better performance using “hierarchical precision”, which essentially means that transfer predicts words that are ontologically closer to the gold label and makes “semantically more reasonable errors”. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors observed during training (Lazaridou et al., 2015a). In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning.\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\n 3 Task and Data\\n  We define object naming as follows: Given an object x in an image, the task is to predict a word w that could be used as the head noun of a realistic referring expression. (Cf. discussion above: “bird” when naming a robin, but “penguin” when naming a penguin.) To get at this, we develop our approach using a corpus of referring expressions produced by human users under natural, interactive conditions (Kazemzadeh et al., 2014), and train and test on the corresponding head nouns in these REs. This is similar to picture naming setups used in psycholinguistic research (cf. Levelt et al. (1991)) and based on the simplifying assumption that the name used for referring to an object can be determined successfully without looking at other objects in the image.\\nWe now summarise the details of our setup:\\nCorpus We train and test on the REFERIT corpus (Kazemzadeh et al., 2014), which is based on the SAIAPR image collection (Grubinger et al., 2006) (99.5k image regions;120K REs). We follow (Schlangen et al., 2016) and select words with a minimum frequency of 40 in these two data sets, which gives us a vocabulary of 793 words.\\nNames For most of our experiments, we only use a subset of this vocabulary, namely the set of object names. As the REs contain nouns that cannot be considered to be names (background, bottom, etc.), we extract from the semantically annotated portion of the REFERIT corpus a list of names which correspond to ‘entry-level’ nouns in terms of (Kazemzadeh et al., 2014). This gives us a list of 159 names. Thus, our experiments are on a smaller scale as compared to (Ordonez et al., 2016). Nevertheless, the data is challenging, as the corpus contains references to objects that fall outside of the object labeling scheme that available object recognition systems are typically optimized for, cf. Hu et al. (2015)’s discussion on “stuff” entities such “sky” or “grass” in the REFERIT data. For testing, we remove relational REs (containing a relational preposition such as ‘left of X’), because here we cannot be sure that the head noun of the target is fully informative; we also remove REs with more than one head noun from our list (i.e. these are mostly relational expressions as well such as ‘girl laughing at boy’). We pair each image region from the test set with its corresponding names from the remaining REs.\\nImage and Word Embeddings Following Schlangen et al. (2016), we derive representations of our visual inputs with a convolutional neural network, ‘GoogleNet’ (Szegedy et al., 2015), which was trained on the ImageNet corpus (Deng et al., 2009), and extract the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region. We add 7 features that encode information about the region relative to the image, thus representing each object as a vector of 1031 features. As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions).\\n 4 Three Models of Interfacing Visual and\\n  Distributional Information\\n 4.1 Direct Cross-Modal Mapping\\n  Following e.g. Lazaridou et al. (2014), referential meaning can be represented as a translation function that projects visual representations of objects to linguistic representations of words in a distributional vector space. Thus, in contrast to standard object recognition systems or the other models we will use here, cross-modal mapping does not treat words as individual labels or classifiers, but learns to directly predict continuous representations of words in a vector space, such as the space defined by the word2vec embeddings that we use in this work. This model will be called TRANSFER below.\\nDuring training, we pair each object with the distributional embedding of its name, and use standard Ridge regression for learning the transformation. Lazaridou et al. (2014) and Lazaridou et al. (2015a) test a range of technical tweaks and different algorithms for cross-modal mapping. For ease of comparison with other models, we stick with simple Ridge Regression in this work.\\nFor decoding, we map an object into the distributional space, and retrieve the nearest neighbors of the predicted vector using cosine similarity. In theory, the model should generalize easily to words that it has not observed in a pair with an object during training as it can map an object anywhere in the distributional space.\\n 4.2 Lexical Mapping Through Individual Word Classifiers\\n  Another approach is to keep visual and distributional information separate, by training a separate visual classifier for each word w in the vocabu-\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nlary. Predictions can then be mapped into distributional space during application time via the vectors of the predicted words. Here, we use Schlangen et al. (2016)’s WAC model, building the training set for each word w as follows: all visual objects in a corpus that have been referred to as w are used as positive instances, the remaining objects as negative instances. Thus, the classifiers learn to predict referential appropriateness for individual words based on the visual features of the objects they refer to, in isolation of other words.\\nDuring decoding, we apply all word classifiers from the model’s vocabulary to the given object, and take the argmax over the individual word probabilities. The model can be used to predict names directly, without links into a distributional space.\\nIn order to extend the model’s vocabulary for zero-shot learning, we follow Norouzi et al. (2013) and associate the top n words with their corresponding distributional vector and compute the convex combination of these vectors. Then, in parallel to cross-modal mapping, we retrieve the nearest neighbors of the combined embedding from the distributional space. Thus, with this model, we use two different modes of decoding: one that projects into distributional space, one that only applies the available word classifiers.\\n 4.3 Word Prediction via Cross-Modal Similarity Mapping\\n  Finally, we implement an approach that combines ideas from cross-modal mapping with the WAC model: we train individual predictors for each word in the vocabulary, but, during training, we exploit lexical similarity relations encoded in a distributional space. Instead of treating a word as a binary classifier, we annotate its training instances with a fine-grained similarity signal according to their object names. When building the training set for such a word predictor w, instead of simply dividing objects into w and ¬w instances, we label each object with a real-valued similarity obtained from cosine similarity between w and v in a distributional vector space, where v is the word that was used to refer to the object. Thus, we task the model with jointly learning similarities and referential appropriateness, by training it with Ridge regression on a continuous output space. Object instances where v = w (i.e., the positive instances in the binary setup) have maximal similarity; the\\nremaining instances have a lower value which is more or less close to maximal similarity. This is the SIM-WAP model, recently proposed in (Anonymous).\\nImportantly, and going beyond (Anonymous), this model allows for an innovative treatment of words that only exist in a distributional space (without being paired with visual referents in the image corpus): as the predictors are trained on a continuous output space, no genuine positive instances of a word’s referent are needed. When training a predictor for such a word w, we use all available objects from our corpus and annotate them with the expected lexical similarity between w and the actual object names v, which for all objects will be below the maximal value that marks genuine positive instances. During decoding, this model does not need to project its predictions into a distributional space, but it simply applies all available predictors to the object, and takes the argmax over the predicted referential appropriateness scores.\\n 5 Experiment 1: Naming Objects\\n  This Section reports on experiments in a standard setup of the object naming task where all object names are paired with visual instances of their referents during training. In a comparable task, i.e. object recognition with known object categories, cross-modal projection or transfer approaches have been reported to perform worse than standard object classification methods (Frome et al., 2013; Norouzi et al., 2013). This seems to suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the real world and that referential meaning should potentially be learned from visual object representation only.\\n 5.1 Model comparison\\n  Setup We use the train/test split of REFERIT data as in (Schlangen et al., 2016). We consider image regions with non-relational referring expressions that contain at least one of the 159 head nouns from the list of entry-level nouns (see section 3). This amounts to 6208 image regions for testing and 73K instances for training.\\nResults Table 1 shows accuracies in the object naming task for the TRANSFER, WAC and SIMWAP models according to their accuracies in the top n, including two variants of WAC where its top\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\n5 and top 10 predictions are project into the distributional space. Overall, the differences in accuracy between the models are small, but the various models that link their predictions to word representations in the distributional space all perform slightly worse than the plain WAC model, i.e. individual word classifiers trained on visual features only. This suggests that referential meanings for a word are learned less accurately when mapping from visual to distributional space, which replicates results reported in the literature on standard object recognition benchmarks.\\nhit @k(%) @1 @2 @5\\ntransfer 48.34 60.49 74.89 wac 49.34 61.86 75.35 wac, project top5 48.73 61.10 74.07 wac, project top10 48.68 61.23 74.31 sim-wap 48.13 60.60 75.40\\nTable 1: Accuracies in object naming\\n 5.2 Model combination\\n  In order to get more insight into why the TRANSFER and SIM-WAP models produce slightly worse results than individual visual word classifiers, we now test to what extent the different models are complementary and combine them by aggregating over their naming predictions. If the models are complementary, their combination should lead to more confident and accurate naming decisions.\\nSetup We combine TRANSFER, SIM-WAP and WAC by aggregating the scores they predict for different object names for a given object. During testing, we apply all models to an image region and consider words ranked among the top 10. We first normalize the referential appropriateness scores in each top-10 list and then compute their sum. This aggregation scheme will give more weight to words that appear in the top 10 list of different models, and less weight to words that only get top-ranked by a single model. We test on the same data as in Section 5.1.\\nhit @k(%) 1 5 10\\nsim-wap + transfer 49.10 61.78 75.81 sim-wap + wac 51.10 63.45 77.92 transfer + wac 51.13 63.76 77.84 wac + transfer + sim-wap 52.19 64.71 78.40\\nTable 2: Object naming acc., combined models\\nResults Table 2 shows that all model combinations improve over the results of their isolated models in Table 1, suggesting that WAC, TRANSFER and SIM-WAP indeed do capture complementary aspects of referential word meaning. On their own, the distributionally informed models are less tuned to specific word occurrences than the visual word classifiers in the WAC model, but they can add useful information which leads to a clear overall improvement. We take this as a promising finding, supporting our initial hypothesis that knowledge on lexical distributional meaning should and can be exploited when learning how to use words for reference.\\nAv. cosine distance among top k gold - top k\\n5 10 5 10\\ntransfer 0.68 0.73 0.72 0.75 wac 0.82 0.80 0.82 0.84 sim-wap 0.68 0.74 0.72 0.75\\nTable 3: Cosine distances between word2vec embeddings of nouns generated in the top k\\n 5.3 Analysis\\n  Figure 2 illustrates objects from our test set where the combination of TRANSFER, SIM-WAP and WAC predicts an accurate name, whereas the models in isolation do not. These examples give some interesting insight into why the models capture different aspects of referential word use and meaning.\\nWord Similarities Many of the examples in Figure 2 suggest that the object names ranked among the top 3 by the TRANSFER and SIMWAP model are semantically similar to each other, whereas WAC generates object names on top that describe very different underlying object categories, such as seal / rock in Figure 2(a), animal / lamp in Figure 2(g) or chair / shirt in Figure 2(c). To quantify this general impression, Table 3 shows cosine distances among words in the top n generated by our models, using their word2vec embeddings. The average cosine distance between words in our vocabulary is 0.83. The transfer and sim-wap model rank words on top that are clearly more similar to each other than word pairs on average, whereas words ranked top by the wac model are more dissimilar. This parallels findings by Frome et al. (2013), discussed in Section 2. Additional evaluation metrics, such as success rates\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nin a human evaluation (cf. Zarrieß and Schlangen (2016)), would be an interesting direction for more detailed investigation here.\\nWord Use But even though the WAC classifiers lack knowledge on lexical similarities, they seem to able to detect relatively specific instances of word use such as hut in Figure 2(b), shirt in 2(c) or lamp in 2(h). Here, the combination with TRANSFER and SIM-WAP is helpful to give more weight to the object name that is taxonomically correct (sometimes pushing up words below the top-3 and hence not shown in Figure 2). In Figure 1(e), SIMWAP and TRANSFER give more weight to typical names for persons, whereas WAC top-ranks more unusual names, reflecting that the person is difficult to identify visually. Another observation is that the mapping models have difficulties dealing with object names in singular and plural. As these words have very similar representations in the distributional space, they are often predicted as likely variants among the top 10 by SIM-WAP and TRANSFER, whereas the WAC model seems to predict inappropriate plural words less often among the top 3. Such specific phenomena at the intersection of visual and semantic similarity have found very little attention in the literature. We will investigate them further in our Experiments on zeroshot naming in the following Section.\\n 6 Zero-Shot Naming\\n  Zero-shot learning is an attractive prospect for REG from images, as it promises to overcome dependence on pairings of visual instances and natural names being available for all names, if visual/referential data can be generalised from other types of information. Previous work has looked at the feasibility of zero-shot learning as a function of semantic similarity or ontological closeness between unknown and known categories, and confirmed the intuition that the task is harder the less close unknown categories are to known ones (Frome et al., 2013; Norouzi et al., 2013).\\nOur experiments on object naming in Section 5 suggest that lexical similarities encoded in a distributional space might not always fully carry over to referential meaning. This could constitute an additional challenge for zero-shot learning, as distributional similarities might be misleading when the model has to fully rely on them for learning referential word meanings. Therefore, the following experiments investigate the performance of\\nour models in zero-shot naming as a function of the lexical relation between unknown and known object names, i.e. namely hypernyms and singular/plurals. Both relations are typically captured by distributional models of word meaning in terms of closeness in the vector space, but their visual and referential relation is clearly different.\\n 6.1 Vocabulary Splits and Testsets\\n  Random As in previous work on zero-shot learning, we consider zero-shot naming for words of varying degrees of similarity in our vocabulary. We randomly split our 159 names from Experiment 1 into 10 subsets. We train the models on 90% of the nouns (and all their visual instances in the image corpus) and test on the set of image regions that are named with words which the model did not observe during training. Results reported in Table 4 on the random test set correspond to averaged scores from cross-validation over the 10 splits.\\nHypernyms We manually split the model’s vocabulary into set of hypernyms (see Appendix A) and the remaining nouns. We train the models on those 84K image regions that where not named with a hypernym, and test on 8895 image regions that were named with a hypernym in the corpus. We checked that for each of these hypernyms, the vocabulary contains at least one or two names that can be considered as hyponyms, i.e. the model sees objects during training that are instances of vehicle for example, but never encounters actual uses of that name. This test set is particularly interesting from an REG perspective, as objects named with very general terms by human speakers are often difficult to describe with more common, but more specific terms, as is illustrated by the uses of structure and thingy in Figure 1.\\nSingulars/Plurals We pick 68 words from our vocabulary that can be grouped into 34 singularplural noun pairs (see Appendix A). From each pair, we randomly include the singular or plural noun in the set of zero-shot nouns. Thus, we make sure that the model encounters singular and plural names during training, but it never encounters both variants of a name. This results in a more even training/test split, i.e. we train on 23K image regions and evaluate on 13825 instances.\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n(a) wac: seal, rock, water sim-wap: side, rock,rocks transfer: rocks, rock, water combination: rock\\n(c) wac: chair, shirt, guy sim-wap: woman, man, girl transfer: door, woman, window combination: shirt\\n(e) wac: chick, person, guy sim-wap: man, person, woman transfer: man, guy, girl combination: person\\n(g) wac: animal, lamp, table sim-wap: man, girl, person transfer: man, clouds, cloud combination: person\\n(b) wac: cactus, hut, mountain sim-wap: side, rock, mountain transfer: mountain, rocks, rock combination: hut\\n(d) wac: roof, house, building sim-wap: building, house, trees transfer: building, house, trees combination: house\\n(f) wac: bush, bushes, tree sim-wap: trees, tree, grass transfer: trees, tree, bushes combination: bushes\\n(h) wac: post, light, lamp sim-wap: tree, sky, pole transfer: tree, sky, trees combination: lamp\\nFigure 2: Examples from object naming experiment where model combination is accurate\\nZero-shot Model full vocab disjoint vocab names @1 @2 @5 @10 @1 @2\\nRandom\\ntransfer 0.05 2.38 16.57 35.71 41.49 62.34 wac, project top10 0.00 4.42 21.16 39.17 38.03 58.07 wac, project top5 0.00 4.39 21.63 40.01 37.46 57.36 sim-wap 3.71 13.13 36.49 54.44 42.28 64.26\\nHypernyms\\ntransfer 0.07 1.25 7.75 29.93 59.88 73.88 wac, project top10 0.00 3.01 15.55 36.99 50.51 66.33 wac, project top5 0.00 2.78 16.75 38.13 47.73 64.38 sim-wap 3.16 10.33 31.14 49.62 57.55 70.15\\nSingulars/Plurals transfer 0.01 22.84 44.30 72.85 34.56 51.79 wac, project top10 0.00 22.21 43.43 68.95 31.46 48.76 wac, project top5 0.00 22.18 43.93 69.33 31.46 48.88 sim-wap 15.39 34.73 56.62 77.32 37.24 54.02\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\n 6.2 Evaluation\\n  Some previous work on zero-shot image labeling assumes additional components that first identify whether an image should be labelled by a known or unknown word (Frome et al., 2013). We follow Lazaridou et al. (2014) and let the model decide whether to refer to an object by a known or unknown name. Related to that, distinct evaluation procedures have been used in the literature on zero-shot learning:\\nTesting on full vocabulary A realistic way to test zero-shot learning performance is to consider all words from a given vocabulary during testing, though the testset only contains instances of objects that have been named with a ‘zero-shot word’ (for which no visual instances were seen during training). Accuracies in this setup reflect how well the model is able to generalize, i.e. how often it decides to deviate from the words it was trained on, and (implicitly) predicts that the given object requires a “new” name. In case of the (i) hypernym and (ii) singular/plural test set, this accuracy also reflects to what extent the model is able to detect cases where (i) a more general or vague term is needed, where (ii) an unknown singular/plural counterpart of a known object type occurs.\\nTesting on disjoint vocabulary Alternatively, the model’s vocabulary can be restricted during testing to zero-shot words only, such that names encountered during training and testing are disjoint, see e.g. (Lampert et al., 2009, 2013). This setup factors out the generalization problem, and assesses to what extent a model is able to capture the referential meaning of a word that does not have instances in the training data.\\n 6.3 Results\\n  As compared to Experiment 1 where models achieved similar performance, differences are more pronounced in the zero-shot setup, as shown in Table 4. In particular, we find that the SIMWAP model which induces individual predictors for words that have not been observed in the training data is clearly more successful than TRANSFER or WAC that project predictions into the distributional space. When tested on the full vocabulary, we find that TRANSFER and WAC very rarely generate names whose referents were excluded from training, which is in line with observations made by Lazaridou et al. (2015a). The SIM-WAP\\npredictors generalize much better, in particular on the singular/plural testset.\\nAn interesting exception is the good performance of the TRANSFER model on the hypernym test set, when evaluated with a disjoint vocabulary. This corroborates evidence from Experiment 1, namely that the transfer model captures taxonomic aspects of object names better than the other models. Projection via individual word classifiers, on the other hand, seems to generalize better than TRANSFER, at least when looking at accuracies @2 ... @10. Thus, combining several vectors predicted by a model of referential word meaning can provide additional information, as compared to mapping an object to a single vector in distributional space. More work is needed to establish how these approaches can be integrated more effectively.\\n 7 Discussion and Conclusion\\n  In this paper, we have investigated models of referential word meaning, using different ways of combining visual information about a word’s referent and distributional knowledge about its lexical similarities. Previous cross-modal mapping models essentially force semantically similar objects to be mapped into the same area in the semantic space regardless of their actual visual similarity. We found that cross-modal mapping produces semantically appropriate and mutually highly similar object names in its top-n list, but does not preserve differences in referential word use (e.g. appropriatness of person vs. woman) especially within the same semantic field. We have shown that it is beneficial for performance in standard and zeroshot object naming to treat words as individual predictors that capture referential appropriateness and are only indirectly linked to a distributional space, either through lexical mapping during application or through cross-modal similarity mapping during training. As we have tested these approaches on a rather small vocabulary, which may limit generality of conclusions, future work will be devoted to scaling up these findings to larger test sets, as e.g. recently collected through conversational agents (Das et al., 2016) that circumvent the need for human-human interaction data. Also from an REG perspective, various extensions of this approach are possible, such as the inclusion of contextual information during object naming and its combination with attribute selection.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899\\n Abstract We compare three recent models of referential word meaning that link visual object representations to lexical representations in a distributional vector space, either directly through cross-modal mapping or indirectly through visual predictors for individual words. We use these models to predict object names as they could be used in naturalistic referring expressions. We find that cross-modal mapping generally produces semantically appropriate and mutually highly similar object names in its topn list, but sometimes fails to make desired distinctions. Visual word predictors, on the other hand, can react to more subtle visual distinctions and select specific terms, but sometimes stray taxonomically very far from the correct one. Combination of the approaches improves over the individual predictions in a standard naming task. All approaches can be extended to the zero-shot naming case, where the correct name is one for which no instances were seen during training; again they show complementary strengths and weaknesses, depending on the setup and the lexical relation of the unattested object name to known ones.   \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots  1 000\\n011\\n012\\n013\\n014\\n015\\n016\\n017\\n018\\n019\\n020\\n021\\n022\\n023\\n024\\n025\\n026\\n027\\n028\\n029\\n030\\n031\\n032\\n033\\n034\\n035\\n036\\n037\\n038\\n039\\n040\\n041\\n042\\n043\\n044\\n045\\n046\\n047\\n048\\n049\\n071\\n072\\n073\\n074\\n075\\n076\\n077\\n078\\n079\\n080\\n081\\n082\\n083\\n084\\n085\\n086\\n087\\n088\\n089\\n090\\n091\\n092\\n093\\n094\\n095\\n096\\n097\\n098\\n099\\n 1 Introduction\\n  Conversational agents include task-oriented dialog systems and non-task-oriented chatbots. Dialog systems focus on helping people complete specific tasks in vertical domains (Young et al., 2010), while chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter et al., 2011). Existing work on building chatbots includes generation based methods and retrieval based methods. Retrieval based chatbots enjoy the advantage of informative and fluent responses, because they select a proper response for the current conversation from a repository with re-\\nsponse selection algorithms. While most existing work on retrieval based chatbots studies response selection for single-turn conversation (Wang et al., 2013) which only considers the last input message, we consider the problem in a multi-turn scenario. In a chatbot, multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context.\\nThe key to response selection lies in inputresponse matching. Different from single-turn conversation, multi-turn conversation requires matching between a response and a conversation context in which one needs to consider not only the matching between the response and the input message but also the matching between the response and the utterances in previous turns. The challenges of the task include (1) how to identify important information (words, phrases, and sentences) in the context that is crucial to selecting a proper response and how to leverage the information in matching; and (2) how to model relationships among the utterances in the context. Table 1 illustrates the challenges with an example. First, “hold a drum class” and “drum” in the context are very important. Without them, one may find responses relevant to the message (i.e., the last turn of the context) but nonsense in the context (e.g., “what lessons do you want?”). Second,\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nthe message highly depends on the second turn in the context, and the order of the utterances matters in response selection: exchanging the third turn and the last turn may lead to different responses. Existing work, however, either ignores relationships among utterances when concatenating them together (Lowe et al., 2015), or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.g., by a hierarchical RNN (Zhou et al., 2016)).\\nWe propose a sequential matching network (SMN), a new context based matching model that can tackle both challenges in an end-to-end way. The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector. Thus responses in these models cannot meet the context until the final step in matching. To avoid information loss, SMN matches a response with each utterance in the context at the beginning and encodes important information in each pair into a matching vector. The matching vectors are then accumulated in the temporal order of the utterances to model their relationships. The final matching degree is computed with the accumulation of the matching vectors. Specifically, for each utteranceresponse pair, the model constructs a word-word similarity matrix and a sequence-sequence similarity matrix by the embedding of words and the hidden states of a recurrent neural network with gated recurrent unites (GRU) (Chung et al., 2014) respectively. The two matrices capture important matching information in the pair on a word level and a segment level respectively, and the information is distilled and fused as a matching vector through an alternation of convolution and pooling operations on the matrices. By this means, important information from multiple levels of granularity in the context is recognized under sufficient supervision from the response and carried into matching with minimal loss. The matching vectors are then uploaded to another GRU to form a matching score for the context and the response. The GRU accumulates the pair matching in its hidden states in the chronological order of the utterances in the context. It models relationships and dependencies among the utterances in a matching fashion and has the utterance order supervise the accumulation of pair matching. The match-\\ning degree of the context and the response is computed by a logit model with the hidden states of the GRU. SMN extends the powerful “2D” matching paradigm in text pair matching for single-turn conversation to context based matching for multi-turn conversation, and enjoys the advantage that both important information in utterance-response pairs and relationships among utterances are sufficiently preserved and leveraged in matching.\\nWe test our model on the Ubuntu dialogue corpus (Lowe et al., 2015) which is a large scale public English data set for research in multi-turn conversation. The results show that our model can significantly outperform state-of-the-art methods, and improvement to the best baseline model on R10@1 is over 6%. In addition to the Ubuntu corpus, we create a human labeled Chinese data set, namely Douban Conversation Corpus, and test our model on it. Different from the Ubuntu corpus in which data is collected from a specific domain and negative candidates are randomly sampled, conversations in this data come from open domain, and response candidates in this data set are collected from a retrieval engine and labeled by three human judges. On this data, our model improves the best baseline model over 3% on R10@1 and 4% on P@1. As far as we know, Douban Conversation Corpus is the first human labeled data set for multi-turn response selection and could be a good complement to the Ubuntu corpus. We release Douban Conversation Corups and our source code at an anonymous url for blind review. We have uploaded code and data with this paper.\\nOur contributions in this paper are three-folds: (1) proposal of a new context based matching model for multi-turn response selection in retrieval based chatbots; (2) publication of a large human labeled data set to research communities. (3) empirical verification of the effectiveness of the model on public data sets;\\n 2 Related Work\\n  Recently, building a chatbot with data driven approaches (Ritter et al., 2011; Higashinaka et al., 2014) has drawn a lot of attention. Existing work along this line includes retrieval based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation based methods (Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2015, 2016; Xing\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\n.... .... ....\\nScore\\n1 2,M M Convolution Pooling\\n( )L\\n.... .... ....\\n1u\\n1nu \\nnu\\nr\\nWord Embedding GRU1\\nGRU2\\n....\\n1v\\n1nv \\nnv\\n1'nh \\nUtterance-Response Matching Matching Accumulation\\nSegment PairsWord Pairs\\nMatching Prediction\\n1'h\\n'nh\\nFigure 1: Architecture of SMN\\net al., 2016; Serban et al., 2016a). Our work belongs to retrieval based methods, and we study context based response selection.\\nEarly studies of retrieval based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b). Recently, researchers begin to pay attention to multi-turn conversation. For example, Lowe et al. (2015) match a response with the literal concatenation of context utterances. Yan et al. (2016) concatenate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture. Zhou et al. (2016) improve multi-turn response selection with a multi-view model including an utterance view and a word view. Our model is different in that it matches a response with each utterance at first and accumulates matching information instead of sentences by a GRU, thus useful information for matching can be sufficiently retained.\\n 3 Sequential Matching Network\\n  \\n 3.1 Problem Formalization\\n  Suppose that we have a data set D = {(yi, si, ri)}Ni=1, where si = {ui,1, . . . , ui,ni} represents a conversation context with {ui,k}nik=1 as utterances. ri is a response candidate and yi ∈ {0, 1} denotes a label. yi = 1 means ri is a proper response for si, otherwise yi = 0. Our goal is to learn a matching model g(·, ·) with D. For any context-response pair (s, r), g(s, r) measures the matching degree between s and r.\\n 3.2 Model Overview\\n  We propose a sequential matching network (SMN) to model g(·, ·). Figure 1 gives the architecture.\\nSMN first decomposes context-response matching into several utterance-response pair matching and then all pair matching is accumulated as a context based matching through a recurrent neural network. SMN consists of three layers. The first layer matches a response candidate with each utterance in the context on a word level and a segment level, and important matching information from the two levels is distilled by convolution and pooling and encoded in a matching vector. The matching vectors are then fed into the second layer where they are accumulated in the hidden states of a recurrent neural network with GRU following the chronological order of the utterances in the context. The third layer calculates the final matching score with the hidden states of the second layer.\\nSMN enjoys several advantages over the existing models. First, a response candidate can meet each utterance in the context at the very beginning, thus matching information in every utteranceresponse pair can be sufficiently extracted and carried to the final matching score with minimal loss. Second, information extraction from each utterance is conducted on different levels of granularity and under sufficient supervision from the response, thus semantic structures that are useful to response selection in each utterance can be well identified and extracted. Third, matching and utterance relationships are coupled rather than separately modeled, thus utterance relationships (e.g., order), as a kind of knowledge, can supervise the formation of the matching score.\\nBy taking utterance relationships into account, SMN extends the “2D” matching that has proven effective in text pair matching for single-turn response selection to sequential “2D” matching for context based matching in response selection for multi-turn conversation. In the following sections,\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nwe will describe details of the three layers.\\n 3.3 Utterance-Response Matching\\n  Given an utterance u in a context s and a response candidate r, the model looks up an embedding table and represents u and r as U = [eu,1, . . . , eu,nu ] and R = [er,1, . . . , er,nr ] respectively, where eu,i, er,i ∈ Rd are the embeddings of the i-th word of u and r respectively. U ∈ Rd×nu and R ∈ Rd×nr are then used to construct a word-word similarity matrix M1 ∈ Rnu×nr and a sequence-sequence similarity matrix M2 ∈ Rnu×nr which are two input channels of a convolutional neural network (CNN). The CNN distills important matching information from the matrices and encodes the information into a matching vector v.\\nSpecifically, ∀i, j, the (i, j)-th element of M1 is defined by\\ne1,i,j = e > u,i · er,j . (1)\\nM1 models the matching between u and r on a word level.\\nTo construct M2, we first employ a GRU to transform U and R to hidden vectors. Suppose that Hu = [hu,1, . . . , hu,nu ] are the hidden vectors of U, then ∀i, hu,i ∈ Rm is defined by\\nzi = σ(Wzeu,i +Uzhu,i−1) ri = σ(Wreu,i +Urhu,i−1)\\nh̃u,i = tanh(Wheu,i +Uh(ri hu,i−1)) hu,i = zi h̃u,i + (1− zi) hu,i−1, (2)\\nwhere hu,0 = 0, zi and ri are an update gate and a reset gate respectively, σ(·) is a sigmoid function, and Wz, Wh, Wr, Uz, Ur,Uh are parameters. Similarly, we have Hr = [hr,1, . . . , hr,nr ] as the hidden vectors of R. Then, ∀i, j, the (i, j)-th element of M2 is defined by\\ne2,i,j = h > u,iAhr,j , (3)\\nwhere A ∈ Rm×m is a linear transformation. ∀i, GRU models the sequential relationship and the dependency among words up to position i and encodes the text segment until the i-th word to a hidden vector. Therefore, M2 models the matching between u and r on a segment level. M1 and M2 are then processed by a CNN to form v. ∀f = 1, 2, CNN regards Mf as an input channel, and alternates convolution and max-pooling operations. Suppose that z(l,f) =\\n[ z (l,f) i,j ] I(l,f)×J(l,f) denotes the output of feature maps of type-f on layer-l, where z(0,f) = Mf , ∀f = 1, 2. On the convolution layer, we employ a 2D convolution operation with a window size r (l,f) w × r(l,f)h , and define z (l,f) i,j as\\nz (l,f) i,j = σ( Fl−1∑ f ′=0 r (l,f) w∑ s=0 r (l,f) h∑ t=0 W (l,f) s,t · z (l−1,f ′) i+s,j+t + b l,k), (4)\\nwhere σ(·) is a ReLU, W(l,f) ∈ Rr (l,f) w ×r (l,f) h and bl,k are parameters, and Fl−1 is the number of feature maps on the (l − 1)-th layer. A max pooling operation follows a convolution operation and can be formulated as\\nz (l,f) i,j = max\\np (l,f) w >s≥0 max p (l,f) h >t≥0 zi+s,j+t, (5)\\nwhere p(l,f)w and p (l,f) h are the width and the height of the 2D pooling respectively. The output of the final feature maps are concatenated and mapped to a low dimensional space with a linear transformation as the matching vector v ∈ Rq.\\nFrom Equation (1), (3), (4), and (5), we can see that by learning word embedding and parameters of GRU from training data, words or segments in an utterance that are useful to recognize the appropriateness of a response may have high similarity with some words or segments in the response and result in high value areas in the similarity matrices. These areas will be transformed and selected by convolution and pooling operations and carry the important information in the utterance to the matching vector. This is how our model identifies important information in context and leverage it in matching under the supervision of the response. We consider multiple channels because we want to capture important matching information on multiple levels of granularity of text.\\n 3.4 Matching Accumulation\\n  Suppose that [v1, . . . , vn] is the output of the first layer (corresponding to n pairs), at the second layer, a GRU takes [v1, . . . , vn] as an input and encodes the matching sequence into its hidden states Hm = [h ′ 1, . . . , h ′ n] ∈ Rq×n with a detailed parameterization similar to Equation (2). This layer has two functions: (1) it models the dependency and the temporal relationship of utterances in the context; (2) it leverages the temporal relationship to supervise the accumulation of the pair matching as a context based matching. Moreover, from\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nEquation (2), we can see that the reset gate (i.e., ri) and the update gate (i.e., zi) control how much information from the previous hidden state and the current input flows to the current hidden state, thus important matching vectors (corresponding to important utterances) can be accumulated while noise in the vectors can be filtered out.\\n 3.5 Matching Prediction and Learning\\n  With [h′1, . . . , h ′ n], we define g(s, r) as\\ng(s, r) = softmax(W2L[h ′ 1, . . . , h ′ n] + b2), (6)\\nwhere W2 and b2 are parameters. We consider three parameterizations for L[h′1, . . . , h ′ n]: (1) only the last hidden state is used. Then L[h′1, . . . , h ′ n] = h ′ n. (2) the hidden states\\nare linearly combined. Then, L[h′1, . . . , h ′ n] =∑n\\ni=1wih ′ i, where wi ∈ R. (3) we follow (Yang et al., 2016) and employ an attention mechanism to combine the hidden states. Then, L[h′1, . . . , h ′ n] is defined as\\nti = tanh(W1,1hui,nu +W1,2h ′ i + b1), αi = exp(t>i ts)∑ i(exp(t > i ts)) ,\\nL[h′1, . . . , h ′ n] = n∑ i=1 αih ′ i, (7)\\nwhere W1,1 ∈ Rq×m,W1,2 ∈ Rq×q and b1 ∈ Rq are parameters. h′i and hui,nu are the i-th matching vector and the final hidden state of the i-th utterance respectively. ts ∈ Rq is a virtual context vector which is randomly initialized and jointly learned in training.\\nBoth (2) and (3) aim to learn weights for {h′1, . . . , h′n} from training data and highlight the effect of important matching vectors in the final matching. The difference is that weights in (2) are static, because the weights are totally determined by the positions of utterances, while weights in (3) are dynamically computed by the matching vectors and utterance vectors. We denote our model with the three parameterizations of L[h′1, . . . , h ′ n] as SMNlast, SMNstatic, and SMNdynamic, and empirically compare them in experiments.\\nWe learn g(·, ·) by minimizing cross entropy withD. Let Θ denote the parameters of SMN, then the objective function L(D,Θ) of learning can be formulated as\\n− N∑ i=1 [yilog(g(si, ri)) + (1− yi)log(1− g(si, ri))] . (8)\\n 4 Response Candidate Retrieval\\n  In practice of a retrieval based chatbot, to apply the matching approach to response selection, one needs to retrieve a bunch of response candidates from an index beforehand. While candidate retrieval is not the focus of the paper, it is an important step in a real system. In this work, we exploit a heuristic method to obtain response candidates from the index. Given a message un with {u1, . . . , un−1} utterances in its previous turns, we extract top 5 keywords from {u1, . . . , un−1} based on their tf-idf scores1 and expand un with the keywords. Then we send the expanded message to the index and retrieve response candidates using the inline retrieval algorithm of the index. Finally, we use g(s, r) to re-rank the candidates and return the top one as a response to the context.\\n 5 Experiments\\n  We tested our model on a public English data set and a Chinese data set we publish with this paper.\\n 5.1 Ubuntu Corpus\\n  The English data set is the Ubuntu Corpus (Lowe et al., 2015) which contains multi-turn dialogues collected from chat logs of Ubuntu Forum. The data set consists of 1 million context-response pairs for training, 0.5 million pairs for validation, and 0.5 million pairs for test. Positive responses are true responses from human, and negative ones are randomly sampled. The ratio of the positive and the negative is 1:1 in training, and 1:9 in validation and test. We used the copy shared by Xu et al. (2016) 2 in which numbers, urls, and paths are replaced by special placeholders. We followed (Lowe et al., 2015) and employed recall at position k in n candidates (Rn@k) as evaluation metrics.\\n 5.2 Douban Conversation Corpus\\n  Ubuntu Corpus is a domain specific data set, and response candidates are obtained from negative sampling without human judgment. To further verify the efficacy of our model, we created a new data set with open domain conversations, namely Douban Conversation Corpus. Response candidates in the test set of Douban Conversation Corpus are collected following the procedure of a re-\\n1Tf is word frequency in the context, while idf is calculated using the entire index.\\n2https://www.dropbox.com/s/ 2fdn26rj6h9bpvl/ubuntudata.zip?dl=0\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\ntrieval based chatbot and are labeled by human judges. Douban Conversation Corpus simulates the real scenario of a retrieval based chatbot, and we publish it to research communities to facilitate the research of multi-turn response selection.\\nSpecifically, we crawled 1.1 million dyadic dialogues (conversation between two persons) longer than 2 turns from Douban group3 which is a popular social networking service in China. From the data, we randomly sampled 0.5 million dialogues for creating a training set, 25 thousand dialouges for creating a validation set, and 1, 000 dialogues for creating a test set, and made sure that there is no overlap among the three sets. For each dialogue in training and validation, we took the last turn as a positive response for the previous turns as a context and randomly sampled another response from the 1.1 million data as a negative response. There are 1 million context-response pairs in the training set and 50 thousand pairs in the validation set.\\nTo create the test set, we first crawled 15 million post-reply pairs from Sina Weibo4 which is the largest microblogging service in China and indexed the pairs with Lucene5. We took the last turn of each Douban dyadic dialogue in the test set as a message, retrieved 10 response candidates from the index following the method in Section 4, and finally formed a test set with 10, 000 context-response pairs. We recruited three labelers to judge if a candidate is a proper response to the context. A proper response means the response can naturally reply to the message given the whole context. Each pair received three labels and the majority of the labels were taken as the final decision. Table 2 gives the statistics of the three sets. Note that the Fleiss’ kappa (Fleiss, 1971) of the labeling is 0.41, which indicates that the three labelers reached a relatively high agreement.\\nBesides Rn@ks, we also followed the convention of information retrieval and employed mean average precision (MAP) (Baeza-Yates et al., 1999), mean reciprocal rank (MRR) (Voorhees et al., 1999), and precision at position 1 (P@1) as evaluation metrics. We did not calculate R2@1 because in Douban corpus one context could have more than one correct responses, and we have to randomly sample one for R2@1, which may bring bias to evaluation. When using the labeled set,\\n3https://www.douban.com/group 4http://weibo.com/ 5https://lucenenet.apache.org/\\nwe removed conversations with all negative responses or all positive responses, as models make no difference on them. There are 6, 670 contextresponse pairs left in the test set.\\n 5.3 Baseline\\n  We considered the following baselines:\\nBasic models: models in (Lowe et al., 2015) and (Kadlec et al., 2015) including TF-IDF, RNN, CNN, LSTM and BiLSTM.\\nMulti-view: the model proposed by Zhou et al. (2016) that utilizes a hierarchical recurrent neural network to model utterance relationships.\\nDeep learning to respond (DL2R): the model proposed by Yan et al. (2016) that reformulates the message with other utterances in the context.\\nAdvanced single-turn matching models: since BiLSTM does not represent the state-ofthe-art matching model, we concatenated the utterances in a context and matched the long text with a response candidate using more powerful models including MV-LSTM (Wan et al., 2016) (2D matching), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al., 2015) (two attention based models), and Multi-Channel which is described in Section 3.3. Multi-Channel is a simple version of our model without considering utterance relationships. We also appended the top 5 tf-idf words in context to the input message, and computed the score between the expanded message and a response with Multi-Channel, denoted as Multi-Channelexp.\\n 5.4 Parameter Tuning\\n  For baseline models, if their results are available in the existing literatures (e.g., those on the Ubuntu corpus), we just copied the numbers, otherwise we implemented the models following the settings in the literatures. All models were implemented using Theano (Theano Development Team, 2016). Word embeddings were initialized by the results of word2vec (Mikolov et al., 2013) which ran on the training data, and the dimensionality of word vectors is 200. For Multi-Channel and layer one of\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nUbuntu Corpus Douban Conversation Corpus R2@1 R10@1 R10@2 R10@5 MAP MRR P@1 R10@1 R10@2 R10@5\\nTF-IDF 0.659 0.410 0.545 0.708 0.331 0.359 0.179 0.095 0.172 0.405 RNN 0.768 0.403 0.547 0.819 0.390 0.422 0.208 0.011 0.223 0.589 CNN 0.848 0.549 0.684 0.896 0.417 0.440 0.226 0.012 0.252 0.647 LSTM 0.901 0.638 0.784 0.949 0.485 0.527 0.320 0.187 0.343 0.720 BiLSTM 0.895 0.630 0.780 0.944 0.479 0.514 0.313 0.184 0.330 0.716 Multi-View 0.908 0.662 0.801 0.951 0.505 0.543 0.342 0.202 0.350 0.729 DL2R 0.899 0.626 0.783 0.944 0.488 0.527 0.330 0.193 0.342 0.705 MV-LSTM 0.906 0.653 0.804 0.946 0.498 0.538 0.348 0.202 0.351 0.710 Match-LSTM 0.904 0.653 0.799 0.944 0.500 0.537 0.345 0.202 0.348 0.720 Attentive-LSTM 0.903 0.633 0.789 0.943 0.495 0.523 0.331 0.192 0.328 0.718 Multi-Channel 0.904 0.656 0.809 0.942 0.506 0.543 0.349 0.203 0.351 0.709 Multi-Channelexp 0.714 0.368 0.497 0.745 0.476 0.515 0.317 0.179 0.335 0.691 SMNlast 0.923 0.723 0.842 0.956 0.526 0.571 0.392 0.236 0.387 0.729 SMNstatic 0.927 0.725 0.838 0.962 0.523 0.572 0.387 0.228 0.387 0.734 SMNdynamic 0.926 0.726 0.847 0.961 0.529 0.569 0.395 0.233 0.396 0.724\\nTable 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statistically significant compared with the best baseline.\\nour model, we set the dimensionality of the hidden states of GRU as 200. We tuned the window size of convolution and pooling in {(2, 2), (3, 3)(4, 4)} and chose (3, 3) finally. The number of feature maps is 8. In layer two, we set the dimensionality of matching vectors and the hidden states of GRU as 50. The parameters were updated by stochastic gradient descent with Adam algorithm (Kingma and Ba, 2014) on a single Tesla K80 GPU. The initial learning rate is 0.001, and the parameters of Adam, β1 and β2 are 0.9 and 0.999 respectively. We employed early-stopping as a regularization strategy. Models were trained in minibatches with a batch size 200, and maximum utterance length is 50. We set the maximum context length (i.e., number of utterances) as 10, because performance of models does not get improved on contexts longer than 10 (details are shown in the supplementary material). We padded zeros if the number of utterances in a context is less than 10, otherwise we kept the last 10 utterances.\\n 5.5 Evaluation Results\\n  Table 3 shows the evaluation results on the two data sets. Our models outperform baselines greatly in terms of all metrics on both data sets, and the improvements are statistically significant (t-test with p-value ≤ 0.01, except R10@5 on Douban Corpus). Even the state-of-the-art singleturn matching models perform much worse than our models. The results demonstrate that one cannot neglects utterance relationships and simply perform multi-turn response selection by concatenating utterances together. Our models achieve significant improvements over Multi-View, which justified our “matching first” strategy. DL2R is\\nworse than our models, indicating that utterance reformulation with heuristic rules is not a good method to utilize context information. Rn@ks are low on Douban corpus as there are multiple correct candidates for a context (e.g., if there are 3 correct responses, then the maximumR10@1 is 0.33). SMNdynamic is only slightly better than SMNstatic and SMNlast. The reason might be that GRU can select useful signals from the matching sequence and accumulate them in the final state with its gate mechanism, thus the efficacy of attention mechanism is not obvious for the task.\\n 5.6 Further Analysis\\n  Visualization: we visualize the similarity matrices and the gates of GRU in layer two using an example from the Ubuntu corpus to further clarify how our model identifies important information in the context and how it selects important matching vectors with the gate mechanism of GRU as described in Section 3.3 and Section 3.4. The example is {u1: how can unzip many rar ( number for example ) files at once; u2: sure you can do that in bash; u3: okay how? u4: are the files all in the same directory? u5: yes they all are; r: then the command glebihan should extract them all from/to that directory}. It is from the test set and our model successfully ranked the correct response to the top position. Due to space limitation, we only visualized M1, M2 and the update gate (i.e. z) in Figure 2. Other pieces of our model are shown in the supplementary material. We can see that in u1 important words including “unzip”, “rar”, “files” are recognized and carried to matching by “command”, “extract”, and “directory” in r, while u3 is almost useless and thus little infor-\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nUbuntu Corpus Douban Conversation Corpus R2@1 R10@1 R10@2 R10@5 MAP MRR P@1 R10@1 R10@2 R10@5\\nReplaceM 0.905 0.661 0.799 0.950 0.503 0.541 0.343 0.201 0.364 0.729 ReplaceA 0.918 0.716 0.832 0.954 0.522 0.565 0.376 0.220 0.385 0.727 Only M1 0.919 0.704 0.832 0.955 0.518 0.562 0.370 0.228 0.371 0.737 Only M2 0.921 0.715 0.836 0.956 0.521 0.565 0.382 0.232 0.380 0.734 SMNlast 0.923 0.723 0.842 0.956 0.526 0.571 0.392 0.236 0.387 0.729\\nTable 4: Evaluation results of model ablation.\\nth en th e\\nco m\\nm an\\nd gl eb ih an sh ou ld ex tra ct th em a ll fro m /toth at di re ct or y\\nhow can\\nunzip many\\nrar ( _number_ for\\nexample )\\nfiles at once\\n0.00\\n0.15\\n0.30\\n0.45\\n0.60\\n0.75\\n0.90\\n1.05\\n1.20\\n1.35\\n1.50\\nv a lu\\ne\\n(a) M1 of u1 and r\\nth en th e\\nco m\\nm an\\nd\\ngl eb\\nih an\\nsh ou\\nld ex tr ac\\nt th em a ll\\nfro m\\n/toth at\\ndi re\\nct or\\ny\\nokay how 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 v a lu e\\n(b) M1 of u3 and r\\n0 10 20 30 40\\nu_1\\nu_2\\nu_3\\nu_4\\nu_5\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nv a lu\\ne\\n(c) Update gate\\nFigure 2: Model visualization. Darker areas mean larger value.\\nmation is extracted from it. u1 is crucial to response selection and nearly all information from u1 and r flows to the hidden state of GRU, while other utterances are less informative and the corresponding gates are almost “closed” to keep the information from u1 and r until the final state.\\nModel ablation: we investigate the effect of different parts of SMN by removing them one by one from SMNlast, shown in Table 4. First, replacing the multi-channel “2D” matching with a neural tensor network (NTN) (Socher et al., 2013) (denoted as ReplaceM ) makes the performance drop dramatically. This is because NTN only matches a pair by an utterance vector and a response vector and loses important information in the pair. Together with the visualization, we can conclude that “2D” matching plays a key role in the “matching first” strategy as it captures the important matching information in each pair with minimal loss. Second, the performance slightly drops when replacing the GRU for matching accumulation with a multi-layer perceptron (denoted as ReplaceA). This indicates that utterance relationships are useful. Finally, we left only one channel in matching and found that M2 is a little more powerful than M1 and we achieve the best results with both of them (except on R10@5 on Douban Corpus).\\nContext length: we study how our model (SMNlast) performs across the length of contexts. Figure 3 shows the comparison on MAP in different length intervals on the Douban corpus. Our model consistently performs better than the baselines, and when contexts become longer, the gap becomes larger. The results demonstrate that our model can well capture the dependencies, espe-\\ncially long dependencies, among utterances in contexts. We give the comparisons on other metrics in our supplementary material.\\n(2,5] (5,10] (10,) context length\\n40\\n45\\n50\\n55 60 M A P\\nLSTM MV-LSTM Multi-View SMN\\nFigure 3: Comparison across context length Retrieval v.s. Generation: we compared SMN with a state-of-the-art response generation model VHERD (Serban et al., 2016b) which was trained using D on the Douban corpus. We conducted a side-by-side human comparison on the top one responses of the two models for each context in the test set. The result is that SMN wins on 238 examples, loses on 207 examples, and is comparable with VHRED on the remaining 555 examples. This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice.\\n 6 Conclusion and Future Work\\n  We present a new context based model for multiturn response selection in retrieval-based chatbots. Experiment results on public data sets show that the model can significantly outperform the stateof-the-art methods. Besides, we publish the first human labeled multi-turn response selection data set to research communities. In the future, we are going to study how to model logical consistency of responses and improve candidate retrieval (see supplementary material).\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899\\n Abstract We study response selection for multi-turn conversation in retrieval based chatbots. Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among the utterances or important information in the context. We propose a sequential matching network (SMN) to address both problems. SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among the utterances. The final matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.   \n",
            "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ...   \n",
            "132  One-Shot Neural Cross-Lingual Transfer for Paradigm Completion  1 000\\n011\\n012\\n013\\n014\\n015\\n016\\n017\\n018\\n019\\n020\\n021\\n022\\n023\\n024\\n025\\n026\\n027\\n028\\n029\\n030\\n031\\n032\\n033\\n034\\n035\\n036\\n037\\n038\\n039\\n040\\n041\\n042\\n043\\n044\\n045\\n046\\n047\\n048\\n049\\n066\\n067\\n068\\n069\\n070\\n071\\n072\\n073\\n074\\n075\\n076\\n077\\n078\\n079\\n080\\n081\\n082\\n083\\n084\\n085\\n086\\n087\\n088\\n089\\n090\\n091\\n092\\n093\\n094\\n095\\n096\\n097\\n098\\n099\\n 1 Introduction\\n  Low-resource natural language processing remains an open problem for many tasks of interest. Furthermore, for most languages in the world, high-cost linguistic annotation and resource creation are unlikely to be undertaken in the near future. In the case of morphology, out of the 7000 currently spoken (Lewis, 2009) languages, only about 200 have computer-readable annotations (Sylak-Glassman et al., 2015) – although morphology is easy to annotate compared to syntax and semantics. Transfer learning is one solution to this problem: it exploits annotations in a high-resource language to train a system for a low-resource language. In this work, we present a method for cross-lingual transfer of inflectional morphology using an encoder-decoder recurrent neural network (RNN). This allows for the development of tools for computational morphology with limited annotated data.\\nIn morphologically rich languages, individual lexical entries may be realized as distinct inflec-\\ntions of a single lemma depending on the syntactic context. For example, the 3SgPresInd of the English verbal lemma to bring is brings. In many languages, a lemma can have hundreds of individual forms. Thus, both generation and analysis of such morphological inflections are active areas of research in NLP and morphological processing has been shown to be a boon to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and Çetinoğlu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7→ brought (with Past being the target tag).\\nRNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Schütze, 2016a; Cotterell et al., 2016a). However, these models require a large amount of data to achieve competitive performance; this makes them unsuitable for out-of-thebox application to paradigm completion in the low-resource scenario. To mitigate this, we consider transfer learning: we train an end-to-end neural system jointly with limited data from a lowresource language and a larger amount of data from a high-resource language. This technique allows the model to apply knowledge distilled from the high-resource training data to the low-resource language as needed.\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nWe conduct experiments on 21 language pairs from four language families, emulating a lowresource setting. Our results demonstrate successful transfer of morphological knowledge. We show improvements in accuracy and edit distance of up to 58% (accuracy) and 4.62 (edit distance) over the same model with only in-domain language data on the paradigm completion task. We further obtain up to 44% (resp. 14%) improvement in accuracy for the one-shot (resp. zero-shot) setting, i.e., one (resp. zero) in-domain language sample per target tag. We also show that the effectiveness of morphological transfer depends on language relatedness, measured by lexical similarity.\\n 2 Inflectional Morphology and Paradigm Completion\\n  Many languages exhibit inflectional morphology, i.e., the form of an individual lexical entry mutates to show properties such as person, number or case. The citation form of a lexical entry is referred to as the lemma and the collection of its possible inflections as its paradigm. Tab. 1 shows an example of a partial paradigm; we display several forms for the Spanish verbal lemma soñar. We may index the entries of a paradigm by a morphological tag, e.g., the 2SgPresInd form sueñas in Tab. 1. In generation, the speaker must select an entry of the paradigm given the form’s context. In general, the presence of rich inflectional morphology is problematic for NLP systems as it greatly increases the token-type ratio and, thus, word form sparsity.\\nAn important task in inflectional morphology is paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Cotterell et al., 2015; Faruqui et al., 2016). Its goal is to map a lemma to all individual inflections, e.g., (soñar, 1SgPresInd) 7→ sueño. There are good solutions for paradigm completion when a large amount of annotated training data is available (Cotterell et al., 2016a).1 In this work, we address the lowresource setting, an up to now unsolved challenge.\\n 2.1 Transferring Inflectional Morphology\\n  In comparison to other NLP annotations, e.g., partof-speech (POS) and named entities, morphological inflection does not lend itself easily to transfer.\\n1The SIGMORPHON 2016 shared task (Cotterell et al., 2016a) on morphological reinflection, a harder generalization of paradigm completion, found that ≥ 98% accuracy can be achieved in many languages with neural sequence-to-sequence models, improving the state of the art by 10%.\\nWe can define a universal set of POS tags (Petrov et al., 2012) or of entity types (e.g., coarse-grained types like person and location or fine-grained types (Yaghoobzadeh and Schütze, 2015)), but inflection is much more language-specific. It is infeasible to transfer morphological knowledge from Chinese to Portuguese as Chinese does not use inflected word forms. Transferring named entity recognition, however, among Chinese and European languages works well (Wang and Manning, 2014a). But even transferring inflectional paradigms from morphologically rich Arabic to Portuguese seems difficult as the inflections often mark dissimilar subcategories. In contrast, transferring morphological knowledge from Spanish to Portuguese, two languages with similar conjugations and 89% lexical similarity, appears promising. Thus, we conjecture that transfer of inflectional morphology is only viable among related languages.\\n 2.2 Formalization of the Task\\n  We now offer a formal treatment of the crosslingual paradigm completion task and develop our notation. Let Σ` be a discrete alphabet for language ` and let T` be a set of morphological tags for `. Given a lemma w` in `, the morphological paradigm (inflectional table) π can be formalized as a set of pairs\\nπ(w`) = {( fk[w`], tk )} k∈T (w`)\\n(1)\\nwhere fk[w`] ∈ Σ+` is an inflected form, tk ∈ T` is its morphological tag and T (w`) is the set of slots in the paradigm; e.g., a Spanish paradigm is:\\nπ(SOÑAR)= {( sueño, 1SgPresInd ) , . . . , ( soñaran, 3PlPastSbj )}\\nParadigm completion consists of predicting the entire paradigm π(w`) given the lemma w`.\\nIn cross-lingual paradigm completion, we consider a high-resource source language `s (lots of training data available) and a low-resource target language `t (little training data available). We denote the source training examples as Ds (with |Ds| = ns) and the target training examples as Dt (with |Dt| = nt). The goal of cross-lingual paradigm completion is to populate paradigms in the low-resource target language with the help of data from the high-resource source language, using only few in-domain examples.\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\n 3 Cross-Lingual Transfer as Multi-Task Learning\\n  We describe our probability model for morphological transfer using terminology from multi-task learning (Caruana, 1997; Collobert et al., 2011). We consider two tasks, training a paradigm completor (i) for a high-resource language and (ii) for a low-resource language. We want to train jointly so we reap the benefits of having related languages. Thus, we define the log-likelihood as\\nL(θ)= ∑\\n(k,w`t )∈Dt\\nlog pθ (fk[w`t ] | w`t , tk) (2)\\n+ ∑\\n(k,w`s )∈Ds\\nlog pθ(fk[w`s ] | w`s , tk)\\nwhere we tie parameters θ for the two languages together to allow the transfer of morphological knowledge between languages. Each probability distribution pθ defines a distribution over all possible realizations of an inflected form, i.e., a distribution over Σ∗. For example, consider the related Romance languages Spanish and French; focusing on one term from each of the summands in Eq. (2) (the past participle of the translation of to visit in each language), we arrive at\\nLvisit(θ) = log pθ(visitado | visitar, PastPart) + log pθ(visité | visiter, PastPart) (3)\\nOur cross-lingual setting forces both transductions to share part of the parameter vector θ, to represent morphological regularities between the two languages in a common embedding space and, thus, to enable morphological transfer. This is no different from monolingual multi-task settings, e.g., jointly training a parser and tagger for transfer of syntax.\\nBased on recent advances in neural transducers, we parameterize each distribution as an encoderdecoder RNN, as in (Kann and Schütze, 2016b). In their setup, the RNN encodes the input and predicts the forms in a single language. In contrast, we force the network to predict two languages.\\n 3.1 Encoder-Decoder RNN\\n  We parameterize the distribution pθ as an encoderdecoder gated RNN with attention (Bahdanau et al., 2015), the state-of-the-art solution for the monolingual case (Kann and Schütze, 2016b). A bidirectional gated RNN encodes the input sequence (Cho et al., 2014) – the concatenation of (i) the language\\n! h1\\n! h2\\n! h3\\n! hN\\nh1\\nh2\\nh3\\nhN\\ns o ñ r\\ns u e s1 s2 s3 sN\\ny1= y2= y3=M\\n…\\nFigure 1: Encoder-decoder RNN for paradigm completion. The lemma soñar is mapped to a target form (e.g., sueña). For brevity, language and target tags are omitted from the input. Thickness of red arrows symbolizes the degree to which the model attends to the corresponding hidden state of the encoder.\\ntag, (ii) the morphological tag of the form to be generated and (iii) the characters of the input word – represented by embeddings. The input to the decoder consists of concatenations of −→ hi and ←− hi , the forward and backward hidden states of the encoder. The decoder, a unidirectional RNN, uses attention: it computes a weight for each hi. Each weight reflects the importance given to that input position. Using the attention weights αij , the probability of the output sequence given the input sequence is:\\np(y | x1, . . . , x|X|) = |Y |∏\\nt=1\\ng(yt−1, st, ct) (4)\\nwhere y = (y1, . . . , y|Y |) is the output sequence (a sequence of |Y | characters), x = (x1, . . . x|X|) is the input sequence (a sequence of |X| characters), g is a non-linear function, st is the hidden state of the decoder and ct is the sum of the encoder states hi, weighted by attention weights αi(st−1) which depend on the decoder state:\\nct =\\n|X|∑\\ni=1\\nαi(st−1)hi (5)\\nFig. 1 shows the encoder-decoder. See Bahdanau et al. (2015) for further details.\\n 3.2 Input Format\\n  Each source form is represented as a sequence of characters; each character is represented as an embedding. In the same way, each source tag is represented as a sequence of subtags, and each subtag is represented as an embedding. More formally,\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nwe define the alphabet Σ = ∪`∈LΣ` as the set of characters in the languages in L, with L being the set of languages in the given experiment. Next, we define S as the set of subtags that occur as part of the set of morphological tags T = ∪`∈LT`; e.g., if 1SgPresInd ∈ T , then 1, Sg, Pres, Ind ∈ S . Note that the set of subtags S is defined as attributes from the UNIMORPH schema (Sylak-Glassman, 2016) and, thus, is universal across languages; the schema is derived from research in linguistic typology.2 The format of the input to our system is S+Σ+. The output format is Σ+. Both input and output are padded with distinguished BOW and EOW symbols.\\nWhat we have described is the representation of Kann and Schütze (2016b). In addition, we preprend a symbol λ ∈ L to the input string (e.g., λ = Es, also represented by an embedding), so the RNN can handle multiple languages simultaneously and generalize over them.\\n 4 Languages and Language Families\\n  To verify the applicability of our method to a wide range of languages, we perform experiments on example languages from several different families.\\nRomance languages, a subfamily of IndoEuropean, are widely spoken, e.g., in Europe and Latin America. Derived from the common ancestor Vulgar Latin (Harris and Vincent, 2003), they share large parts of their lexicon and inflectional morphology; we expect knowledge among them to be easily transferable.\\nWe experiment on Catalan, French, Italian, Portuguese and Spanish. Tab. 2 shows that Spanish – which takes the role of the low-resource language in our experiments – is closely related with the other four, with Portuguese being most similar. We hypothesize that the transferability of morphological knowledge between source and target corresponds to the degree of lexical similarity; thus, we expect Portuguese and Catalan to be more beneficial for Spanish than Italian and French.\\nThe Indo-European Slavic language family has its origin in eastern-central Europe (Corbett and Comrie, 2003). We experiment on Bulgarian, Macedonian, Russian and Ukrainian (Cyrillic script) and on Czech, Polish and Slovene (Latin script). Macedonian and Ukranian are low-resource\\n2Note that while the subtag set is universal, which subtags a language actually uses is language-specific; e.g., Spanish does not mark animacy as Russian does. We contrast this with the universal POS set (Petrov et al., 2012), where it is reasonable to expect that we see all 17 tags in every language.\\nlanguages, so we assign them the low-resource role. For Romance and for Uralic, we experiment with groups containing three or four source languages. To arrive at a comparable experimental setup for Slavic, we run two experiments, each with three source and one target language: (i) from Russian, Bulgarian and Czech to Macedonian; and (ii) from Russian, Polish and Slovene to Ukrainian.\\nWe hope that the paradigm completor learns similar embeddings for, say, the characters “e” in Polish and “ ” in Ukrainian. Thus, the use of two scripts in Slavic allows us to explore transfer across different alphabets.\\nWe further consider a non-Indo-European language family, the Uralic languages. We experiment on the three most commonly spoken languages – Finnish, Estonian and Hungarian (Abondolo, 2015) – as well as Northern Sami, a language used in Northern Scandinavia. While Finnish and Estonian are closely related (both are members of the Finnic subfamily), Hungarian is a more distant cousin. Estonian and Northern Sami are lowresource languages, so we assign them the lowresource role, resulting in two groups of experiments: (i) Finnish, Hungarian and Estonian to Northern Sami; (ii) Finnish, Hungarian and Northern Sami to Estonian.\\nArabic (baseline) is a Semitic language (part of the Afro-Asiatic family (Hetzron, 2013)) that is spoken in North Africa, the Arabian Peninsula and other parts of the Middle East. It is unrelated to all other languages used in this work. Both in terms of form (new words are mainly built using a templatic system) and categories (it has tags such as construct state), Arabic is very different. Thus, we do not expect it to support morphological knowledge transfer and we use it as a baseline for all target languages.\\n 5 Experiments\\n  We run three experiments on 21 distinct pairings of languages to show the feasibility of morphological transfer and analyze our method. We first discuss details common to all experiments.\\nWe keep hyperparameters during all experiments (and for all languages) fixed to the following values. Encoder and decoder RNNs each have 100\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\n50·20 50·21 50·22 50·23 50·24 50·25 50·26 50·27 Number of Samples\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8 1.0 A cc ur ac y\\nLanguages Pt Ca It Fr Ar Es\\nFigure 2: Learning curves showing the accuracy on Spanish test when training on language λ ∈ {PT, CA, IT, FR, AR, ES}. Except for λ=ES, each model is trained on 12,000 samples from λ and “Number of Samples” (x-axis) of Spanish.\\nhidden units and the size of all subtag, character and language embeddings is 300. For training we use ADADELTA (Zeiler, 2012) with minibatch size 20. All models are trained for 300 epochs. Following Le et al. (2015), we initialize all weights in the encoder, decoder and the embeddings except for the GRU weights in the decoder to the identity matrix. Biases are initialized to zero.\\nEvaluation metrics: (i) 1-best accuracy: the percentage of predictions that match the true answer exactly; (ii) average edit distance between prediction and true answer. The two metrics differ in that accuracy gives no partial credit and incorrect answers may be drastically different from the annotated form without incurring additional penalty. In contrast, edit distance gives partial credit for forms that are closer to the true answer.\\n 5.1 Exp. 1: Transfer Learning for Paradigm Completion\\n  In this experiment, we investigate to what extent our model transfers morphological knowledge from a high-resource source language to a low-resource target language. We experimentally answer three questions. (i) Is transfer learning possible for morphology? (ii) How much annotated data do we need in the low-resource target language? (iii) How closely related must the two languages be to achieve good results?\\nData. Based on complete inflection tables from unimorph.org (Kirov et al., 2016), we create datasets as follows. Each training set consists of 12,000 samples in the high-resource source language and nt∈{50, 200} samples in the lowresource target language. We create target lan-\\nguage dev and test sets of sizes 1600 and 10,000, respectively.3 For Romance and Arabic, we create learning curves for nt∈{100, 400, 800, 1600, 3200, 6400, 12000}. Lemmata and inflections are randomly selected from all available paradigms.\\nResults and Discussion. Tab. 3 shows the effectiveness of transfer learning. There are two baselines. (i) “0”: no transfer, i.e., we consider only in-domain data; (ii) “AR”: Arabic, which is unrelated to all target languages.\\nWith the exception of the 200 sample case of ET→SME, cross-lingual transfer is always better than the two baselines; the maximum improvement is 0.58 (0.58 vs. 0.00) in accuracy for the 50 sample case of CA→ES. More closely related source languages improve performance more than distant ones. French, the Romance language least similar to Spanish, performs worst for →ES. For the target language Macedonian, Bulgarian provides most benefit. This can again be explained by similarity: Bulgarian is closer to Macedonian than the other languages in this group. The best result for Ukrainian is RU→UK. Unlike Polish and Slowenian, Russian is the only language in this group that uses the same script as Ukrainian, showing the importance of the alphabet for transfer. Still, the results also demonstrate that transfer works across alphabets (although not as well); this suggests that similar embeddings for similar characters have been learned. Finnish is the language that is closest to Estonian and it again performs best as a source language for Estonian. For Northern Sami, transfer works least well, probably because the distance between sources and target is largest in this case. The distance of the Sami languages from the Finnic (Estonian, Finnish) and Ugric (Hungarian) languages is much larger than the distances within Romance and within Slavic.4 However, even for Northern Sami, adding an additional language is still always beneficial compared to the monolingual baseline.\\nLearning curves for Romance and Arabic further support our finding that language similarity is important. In Fig. 2, knowledge is transferred to Spanish, and a baseline – a model trained only on Spanish data – shows the accuracy obtained without any transfer learning. Here, Catalan and Italian help the most, followed by Portuguese, French and,\\n3For Estonian, we use 7094 (not 12,000) train and 5000 (not 10,000) test samples as more data is unavailable.\\n4We have enlisted the expert for Uralic at our university and are in the process of analyzing SME results in more detail.\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nRomance Slavic I Slavic II Uralic I Uralic II source 0 AR PT CA IT FR 0 AR RU BG CS 0 AR RU PL SL 0 AR FI HU ET 0 AR FI HU SME target →ES →MK →UK →SME →ET\\n5 0 acc 0.00 0.04 0.48 0.58 0.46 0.29 0.00 0.00 0.23 0.47 0.13 0.01 0.01 0.47 0.16 0.07 0.00 0.00 0.03 0.01 0.01 0.02 0.01 0.35 0.21 0.17\\nED 5.42 4.06 0.85 0.80 1.15 1.82 5.71 5.59 1.61 0.87 2.32 5.23 4.80 0.77 2.14 3.12 8.47 8.64 5.41 6.53 7.03 4.50 4.51 1.55 2.19 2.60 2 0 0 acc 0.38 0.54 0.62 0.78 0.74 0.60 0.21 0.40 0.62 0.77 0.57 0.16 0.21 0.64 0.55 0.50 0.05 0.09 0.20 0.18 0.06 0.34 0.53 0.74 0.71 0.66 ED 1.37 0.87 0.57 0.78 0.44 0.82 1.93 1.12 0.68 0.36 0.72 2.09 1.60 0.49 0.73 0.82 5.43 4.93 3.20 3.37 5.01 1.47 0.98 0.41 0.48 0.62\\nTable 3: Accuracy (acc) and edit distance (ED) of cross-lingual transfer learning for paradigm completion. The target language is indicated by “→”, e.g., it is Spanish for “→ES”. Sources are indicated in the row “source”; “0” is the monolingual case. Except for Estonian, we train on ns = 12,000 source samples and nt ∈ {50, 200} target samples (as indicated by the row). There are two baselines in the table. (i) “0”: no transfer, i.e., we consider only in-domain data; (ii) “AR”: the Semitic language Arabic is unrelated to all target languages and functions as a dummy language that is unlikely to provide relevant information. All languages are denoted using the official codes (SME=Northern Sami).\\nfinally, Arabic. This corresponds to the order of lexical similarity with Spanish, except for the performance of Portuguese (cf. Tab. 2). A possible explanation is the potentially confusing overlap of lemmata between the two languages – cf. discussion in the next subsection. That the transfer learning setup improves performance for the unrelated language Arabic as source is at first surprising. But adding new samples to a small training set helps prevent overfitting (e.g., rote memorization) even if the source is a morphologically unrelated language; effectively acting as a regularizer.5 This will also be discussed below.\\nError Analysis for Romance. Even for only 50 Spanish instances, many inflections are correctly produced in transfer. For, e.g., (criar, 3PlFutSbj) 7→ criaren, model outputs are: fr: criaren, ca: criaren, es: crntaron, it: criaren, ar: ecriren, pt: criaren (all correct except for the two baselines). Many errors involve accents, e.g., (contrastar, 2PlFutInd) 7→ contrastaréis; model outputs are: fr: contrastareis, ca: contrastareis, es: conterarı́an, it: contrastareis, ar: contastarı́as, pt: contrastareis. Some inflections all systems get wrong, mainly because of erroneously applying the inflectional rules of the source to the target. Finally, the output of the model trained on Portuguese contains a class of errors that are unlike those of other systems. Example: (contraatacar, 1SgCond) 7→ contraatacarı́a with those solutions: fr: contratacarı́am, ca: contraatacarı́a, es: concarnar, it: contratacé, ar: cuntatarı́a and pt: contra-atacarı́a. The Portuguese model inserts “-” because Portuguese train data contains contraatacar and “-” appears in its inflected form.6\\n5Following (Kann and Schütze, 2016b) we did not use standard regularizers. To verify that the effect of Arabic is a regularization effect, we ran a small monolingual experiment on ES (200 setting) with dropout 0.5 (Srivastava et al., 2014). The resulting accuracy is 0.57, very similar to the comparable Arabic number of 0.54 in the table.\\n6To investigate this in more detail we retrain the Portuguese model with 50 Spanish samples, but exclude all lemmata\\n0 PT CA IT FR AR →ES\\non e sh ot acc 0.00 0.44 0.39 0.23 0.13 0.00 ED 6.26 1.01 1.27 1.83 2.87 7.00\\nze ro sh ot acc 0.00 0.14 0.08 0.01 0.02 0.00 ED 7.18 1.95 1.99 3.12 4.27 7.50\\nTable 4: Results for one-shot and zero-shot transfer learning. Formatting is the same as for Tab. 3. We still use ns = 12000 source samples. In the one-shot (resp. zero-shot) case, we observe exactly one form (resp. zero forms) for each tag in the target language at training time.\\nAn example for the generally improved performance across languages for 200 Spanish training samples is (contrastar, 2PlIndFut) 7→ contrastaréis: all models now produce the correct form.\\n 5.2 Exp. 2: Zero-Shot/One-Shot Transfer\\n  In §5.1, we investigated the relationship between indomain (target) training set size and performance. Here, we look at the extreme case of training set sizes 1 (one-shot) and 0 (zero-shot) for a tag. We train our model on a single sample for half of the tags appearing in the low-resource language, i.e., if T` is the set of morphological tags for the target language, train set size is |T`|/2. As before, we add 12,000 source samples.\\nWe report one-shot accuracy (resp. zero-shot accuracy), i.e., the accuracy for samples with a tag that has been seen once (resp. never) during training. Note that the model has seen the individual subtags each tag is composed of.7\\nData. Our experimental setup is similar to §5.1: we use the same dev, test and high-resource train sets as before. However, the low-resource data is created in the way specified above. To remove a potentially confounding variable, we impose the condition that no two training samples belong to\\nthat appear in Spanish train/dev/test, resulting in only 3695 training samples. Accuracy on test increases by 0.09 despite the reduced size of the training set.\\n7It is very unlikely that due to random selection a subtag will not be in train; this case did not occur in our experiments.\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nthe same lemma. Results and Discussion. Tab. 4 shows that the Spanish and Arabic systems do not learn anything useful for either half of the tags. This is not surprising as there is not enough Spanish data for the system to generalize well and Arabic does not contribute exploitable information. The systems trained on French and Italian, in contrast, get a nonzero accuracy for the zero-shot case as well as 0.13 and 0.23, respectively, in the one-shot case. This shows that a single training example is sometimes sufficient for successful generation although generalization to tags never observed is rarely possible. Catalan and Portuguese show the best performance in both settings; this is intuitive since they are the languages closest to the target (cf. Tab. 2). In fact, adding Portuguese to the training data yields an absolute increase in accuracy of 0.44 (0.44 vs. 0.00) for one-shot and 0.14 (0.14 vs. 0.00) for zero-shot with corresponding improvements in edit distance.\\nOverall, this experiment shows that with transfer learning from a closely related language the performance of zero-shot morphological generation improves over the monolingual approach, and, in the one-shot setting, it is possible to generate the right form nearly half the time.\\n 5.3 Exp. 3: True Transfer vs. Other Effects\\n  We would like to separate the effects of regularization that we saw for Arabic from true transfer.\\nTo this end, we generate a random cipher (i.e., a function γ : Σ ∪ S 7→ Σ ∪ S) and apply it to all word forms and morphological tags of the high-resource train set; target language data are not changed. Ciphering makes it harder to learn true “linguistic” transfer of morphology. Consider the simplest case of transfer: an identical mapping in two languages, e.g., (visitar, 1SgPresInd) 7→ visito in both Portuguese and Spanish. If we transform Portuguese using the cipher γ(iostv...) = kltqa..., then visito becomes aktkql in Portuguese and its tag becomes similarly unrecognizable as being identical to the Spanish tag 1SgPresInd. Our intuition is that ciphering will disrupt transfer of morphology.8 On the other hand, the regularization effect we observed with Arabic should still be effective.\\nData. We use the Portuguese-Spanish and 8Note that ciphered input is much harder than transfer between two alphabets (Latin/Cyrillic) because it creates ambiguous input. In the example, Spanish “i” is totally different from Portuguese “i” (which is really “k”), but the model must use the same representation.\\nArabic-Spanish data from Experiment 1. We generate a random cipher and apply it to morphological tags and word forms for Portuguese and Arabic. The language tags are kept unchanged. Spanish is also not changed. For comparability with Tab. 3, we use the same dev and test sets as before.\\nResults and Discussion. Tab. 5 shows that performance of PT→ES drops a lot: from 0.48 to 0.09 for 50 samples and from 0.62 to 0.54 for 200 samples. This is because there are no overt similarities between the two languages left after applying the cipher, e.g., the two previously identical forms visito are now different.\\nThe impact of ciphering on AR→ES varies: slightly improved in one case (0.54 vs. 0.56), slightly worse in three cases. We also apply the cipher to the tags and Arabic and Spanish share subtags, e.g., Sg. Just the knowledge that something is a subtag is helpful because subtags must not be generated as part of the output. We can explain the tendency of ciphering to decrease performance on AR→ES by the “masking” of common subtags.\\nFor 200 samples and ciphering, there is no clear difference in performance between Portuguese and Arabic. However, for 50 samples and ciphering, Portuguese (0.09) seems to perform better than Arabic (0.02) in accuracy. Portuguese uses suffixation for inflection whereas Arabic is templatic and inflectional changes are not limited to the end of the word. This difference is not affected by ciphering. Perhaps even ciphered Portugese lets the model learn better that the beginnings of words just need to be copied. For 200 samples, the Spanish dataset may be large enough, so that ciphered Portuguese no longer helps in this regard.\\nComparing no transfer with transfer from a ciphered language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place.\\n 6 Related Work\\n  Cross-lingual transfer learning has been used for many tasks: automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012), entity recog-\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\n0→ES PT→ES AR→ES orig ciph orig ciph\\n5 0 acc 0.00 0.48 0.09 0.04 0.02\\nED 5.42 0.85 3.25 4.06 4.62 2 0 0 acc 0.38 0.62 0.54 0.54 0.56\\nED 1.37 0.57 0.95 0.87 0.93\\nTable 5: Results for ciphering. “0→ES” and “orig” are original results, copied from Tab. 3; “ciph” is the result after the cipher has been applied.\\nnition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Padó and Lapata, 2005). The drawback is that machine translation errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010).\\nIn the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages.\\nWork on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-Markov conditional random field (Sarawagi and Cohen, 2004). Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2015; Kann and Schütze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art.\\nEncoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves\\nand Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation (MT): (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k ∈ {1, 2} and m = k. Other work has addressed cases with k > 2 or m > 2; this would be an interesting avenue of future research for paradigm completion. (iii) Whereas training RNNs in MT is hard, we only experienced one difficult issue in our experiments (due to the low-resource setting): regularization. (iv) Some work is word- or subword-based, our work is character-based. The same way that similar word embeddings are learned for the inputs cow and vache (French for “cow”) in MT, we expect similar embeddings to be learned for similar Cyrillic/Latin characters. (v) Similar to work in MT, we show that zero-shot (and, by extension, one-shot) learning is possible.\\n(Ha et al., 2016) (which was developed in parallel to our transfer model although we did not prepublish our paper on arxiv) is most similar to our work. Whereas Ha et al. (2016) address MT, we focus on the task of paradigm completion in low-resource settings and establish the state of the art for this problem.\\n 7 Conclusion\\n  We presented a cross-lingual transfer learning method for paradigm completion, based on an RNN encoder-decoder model. Our experiments showed that information from a high-resource language can be leveraged for paradigm completion in a related low-resource language. Our analysis showed that the degree to which the source language data helps for a certain target language depends on their relatedness. Our method led to significant improvements in settings with limited training data – up to 58% absolute improvement in accuracy – and, thus, enables the use of state-of-the-art models for paradigm completion in low-resource languages.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899\\n Abstract We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a lowresource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.   \n",
            "133                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Modeling Contextual Relationships Among Utterances for Multimodal Sentiment Analysis  1 000\\n011\\n012\\n013\\n014\\n015\\n016\\n017\\n018\\n019\\n020\\n021\\n022\\n023\\n024\\n025\\n026\\n027\\n028\\n029\\n030\\n031\\n032\\n033\\n034\\n035\\n036\\n037\\n038\\n039\\n040\\n041\\n042\\n043\\n044\\n045\\n046\\n047\\n048\\n049\\n061\\n062\\n063\\n064\\n065\\n066\\n067\\n068\\n069\\n070\\n071\\n072\\n073\\n074\\n075\\n076\\n077\\n078\\n079\\n080\\n081\\n082\\n083\\n084\\n085\\n086\\n087\\n088\\n089\\n090\\n091\\n092\\n093\\n094\\n095\\n096\\n097\\n098\\n099\\n 1 Introduction\\n  Emotion recognition and sentiment analysis have become a new trend in social media, helping users to automatically extract the opinions expressed in user-generated content, especially videos. Thanks to the high availability of computers and smartphones, and the rapid rise of social media, consumers tend to record their reviews and opinions about products or films and upload them on social media platforms, such as YouTube or Facebook. Such videos often contain comparisons, which can aid prospective buyers make an informed decision.\\nThe primary advantage of analyzing videos over text is the surplus of behavioral cues present in vocal and visual modalities. The vocal modulations and facial expressions in the visual data, along with textual data, provide important cues to better identify affective states of the opinion holder. Thus, a combination of text and video data helps to create a better emotion and sentiment analysis model (Poria et al., 2017).\\nRecently, a number of approaches to multimodal sentiment analysis, producing interesting results, have been proposed (Pérez-Rosas et al., 2013; Wollmer et al., 2013; Poria et al., 2015). However, there are major issues that remain unaddressed, such as the role of speaker-dependent versus speaker-independent models, the impact of each modality across the dataset, and generalization ability of a multimodal sentiment classifier. Leaving these issues unaddressed has presented difficulties in effective comparison of different multimodal sentiment analysis methods.\\nAn utterance is a unit of speech bound by breathes or pauses. Utterance-level sentiment analysis focuses on tagging every utterance of a video with a sentiment label (instead of assigning a unique label to the whole video). In particular, utterance-level sentiment analysis is useful to understand the sentiment dynamics of different aspects of the topics covered by the speaker throughout his/her speech. The true meaning of an utterance is relative to its surrounding utterances.\\nIn this paper, we consider such surrounding utterances to be the context, as the consideration of temporal relation and dependency among utterances is key in human-human communication. For example, the MOSI dataset (Zadeh et al., 2016) contains a video, in which a girl reviews the movie ‘Green Hornet’. At one point, she says “The Green Hornet did something similar”. Normally, doing something similar, i.e., monotonous or repetitive might be perceived as negative. However, the nearby utterances “It engages the audience more”, “they took a new spin on it”, “and I just loved it” indicate a positive context.\\nIn this paper, we discard the oversimplifying hypothesis on the independence of utterances and develop a framework based on long short-term memory (LSTM) to extract utterance features that also consider surrounding utterances.\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nOur model enables consecutive utterances to share information, thus providing contextual information in the classification process. Experimental results show that the proposed framework has outperformed the state of the art on benchmark datasets by 5−10%. The paper is organized as follows: Section 2 provides a brief literature review on multimodal sentiment analysis; Section 3 describes the proposed method in detail; experimental results and discussion are shown in Section 4; finally, Section 5 concludes the paper.\\n 2 Related Work\\n  Text-based sentiment analysis systems can be broadly categorized into knowledge-based and statistics-based systems (Cambria, 2016). While the use of knowledge bases was initially more popular for the identification of emotions and polarity in text, sentiment analysis researchers have recently been using statistics-based approaches, with a special focus on supervised statistical methods (Pang et al., 2002; Socher et al., 2013).\\nIn 1970, Ekman (Ekman, 1974) carried out extensive studies on facial expressions which showed that universal facial expressions are able to provide sufficient clues to detect emotions. Recent studies on speech-based emotion analysis (Datcu and Rothkrantz, 2008) have focused on identifying relevant acoustic features, such as fundamental frequency (pitch), intensity of utterance, bandwidth, and duration.\\nAs for fusing audio and visual modalities for emotion recognition, two of the early works were done by De Silva et al. (De Silva et al., 1997) and Chen et al. (Chen et al., 1998). Both works showed that a bimodal system yielded a higher accuracy than any unimodal system. More recent research on audio-visual fusion for emotion recognition has been conducted at either feature level (Kessous et al., 2010) or decision level (Schuller, 2011).\\nWhile there are many research papers on audiovisual fusion for emotion recognition, only a few have been devoted to multimodal emotion or sentiment analysis using textual clues along with visual and audio modalities. Wollmer et al. (Wollmer et al., 2013) and Rozgic et al. (Rozgic et al., 2012a,b) fused information from audio, visual, and textual modalities to extract emotion and sentiment. Metallinou et al. (Metallinou et al., 2008) and Eyben et al. (Eyben et al., 2010a) fused audio and textual modalities for emotion recognition.\\nBoth approaches relied on a feature-level fusion. Wu et al. (Wu and Liang, 2011) fused audio and textual clues at decision level.\\n 3 Method\\n  In this work, we propose a LSTM network that takes as input all utterances in a video and extracts contextual unimodal and multimodal features by modeling the dependencies among the input utterances. Below, we propose an overview of the method -\\n1. Context-Independent Unimodal UtteranceLevel Feature Extraction\\nFirst, the unimodal features are extracted without considering the contextual information of the utterances (Section 3.1). Table 1 presents the feature extraction methods used for each modality.\\n2. Contextual Unimodal and Multimodal Classification\\nThe context-independent unimodal features (from Step 1) are then fed into a LSTM network (termed contextual LSTM) that allows consecutive utterances in a video to share semantic information in the feature extraction process (which provides context-dependent unimodal and multimodal classification of the utterances). We experimentally show that this proposed framework improves the performance of utterance-level sentiment classification over traditional frameworks.\\nVideos, comprising of its constituent utterances, serve as the input. We represent the dataset as U:\\nU = ⎡⎢⎢⎢⎢⎢⎣ u1,1 u1,2 u1,3 ... u1,L1 u2,1 u2,2 u2,3 ... u2,L2 . . . ... . uM,1 uM,2 uM,3 ... uM,LM ⎤⎥⎥⎥⎥⎥⎦ .\\nHere, ui,j denotes the jth utterance of the ith video and L = [L1, L2, ..., LM ] represents the number of utterances per video in the dataset set.\\n 3.1 Extracting Context-Independent Unimodal Features\\n  Initially, the unimodal features are extracted from each utterance separately, i.e., we do not consider the contextual relation and dependency among the utterances (Table 1). Below, we explain the textual, audio, and visual feature extraction methods.\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\n 3.1.1 text-CNN: Textual Features Extraction\\n  For feature extraction from textual data, we use a convolutional neural network (CNN).\\nThe idea behind convolution is to take the dot product of a vector of k weights, wk, known as kernel vector, with each k-gram in the sentence s(t) to obtain another sequence of features c(t) = (c1(t), c2(t), . . . , cL(t)):\\ncj = wTk ⋅ xi∶i+k−1. We then apply a max pooling operation over the feature map and take the maximum value ĉ(t) = max{c(t)} as the feature corresponding to this particular kernel vector. We use varying kernel vectors and window sizes to obtain multiple features.\\nThe process of extracting textual features is as follows -\\nFirst, we represent each sentence as the concatenation of vectors of the constituent words. These vectors are the publicly available 300-dimensional word2vec vectors trained on 100 billion words from Google News (Mikolov et al., 2013). The convolution kernels are thus applied to these word vectors instead of individual words. Each sentence is wrapped to a window of 50 words which serves as the input to the CNN.\\nThe CNN has two convolutional layers - the first layer having a kernel size of 3 and 4, with 50 feature maps each and a kernel size 2 with 100 feature maps for the second. The convolution layers are interleaved with pooling layers of dimension 2. We use ReLU as the activation function. The convolution of the CNN over the sentence learns abstract representations of the phrases equipped with implicit semantic information, which with each successive layer spans over increasing number of words and ultimately the entire sentence.\\nModality Model\\nText text-CNN: Deep Convolutional NeuralNetwork with word embeddings\\nVideo 3d-CNN: 3-dimensional CNNs employed onutterances of the videos Audio openSMILE: Extracts low level audiodescriptors from the audio modality\\nTable 1: Methods for extracting context independent baseline features from different modalities.\\n 3.1.2 Audio Feature Extraction\\n  Audio features are extracted in 30 Hz frame-rate; we use a sliding window of 100 ms. To compute the features, we use the open-source software\\nopenSMILE (Eyben et al., 2010b) which automatically extracts pitch and voice intensity. Voice normalization is performed and voice intensity is thresholded to identify samples with and without voice. Z-standardization is used to perform voice normalization.\\nThe features extracted by openSMILE consist of several low-level descriptors (LLD) and their statistical functionals. Some of the functionals are amplitude mean, arithmetic mean, root quadratic mean, etc. Taking into account all functionals of each LLD, we obtained 6373 features.\\n 3.1.3 Visual Feature Extraction\\n  We use 3D-CNN to obtain visual features from the video. We hypothesize that 3D-CNN will not only be able to learn relevant features from each frame, but will also be able to learn the changes among given number of consecutive frames.\\nIn the past, 3D-CNN has been successfully applied to object classification on 3D data (Ji et al., 2013). Its ability to achieve state-of-the-art results motivated us to use it.\\nLet vid ∈ Rc×f×h×w be a video, where c = number of channels in an image (in our case c = 3, since we consider only RGB images), f = number of frames, h = height of the frames, and w = width of the frames. Again, we consider the 3D convolutional filter filt ∈ Rfm×c×fl×fh×fw, where fm = number of feature maps, c = number of channels, fd = number of frames (in other words depth of the filter), fh = height of the filter, and fw = width of the filter. Similar to 2D-CNN, filt slides across video vid and generates output convout ∈ Rfm×c×(f−fd+1)×(h−fh+1)×(w−fw+1). Next, we apply max pooling to convout to select only relevant features. The pooling will be applied only to the last three dimensions of the array convout.\\nIn our experiments, we obtained best results with 32 feature maps (fm) with the filter-size of 5 × 5 × 5 (or fd × fh × fw). In other words, the dimension of the filter is 32 × 3 × 5 × 5 × 5 (or fm × c × fd × fh × fw). Subsequently, we apply max pooling on the output of convolution operation, with window-size being 3×3×3. This is followed by a dense layer of size 300 and softmax. The activations of this dense layer are finally used as the video features for each utterance.\\n 3.2 Context-Dependent Feature Extraction\\n  We hypothesize that, within a video, there is a high probability of utterance relatedness with respect\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nto their sentimental and emotional clues. Since most videos tend to be about a single topic, the utterances within each video are correlated, e.g., due to the development of the speaker’s idea, coreferences, etc. This calls for a model which takes into account such inter-dependencies and the effect these might have on the current utterance. To capture this flow of informational triggers across utterances, we use a LSTM-based recurrent network scheme (Gers, 2001).\\n 3.2.1 Long Short-Term Memory\\n  LSTM is a kind of recurrent neural network (RNN), an extension of conventional feed-forward neural network. Specifically, LSTM cells are capable of modeling long-range dependencies, which other traditional RNNs fail to do given the vanishing gradient issue. Each LSTM cell consists of an input gate i, an output gate o, and a forget gate f , which enables it to remember the error during the error propagation. Current research (Zhou et al., 2016) indicates the benefit of using such networks to incorporate contextual information in the classification process.\\nIn our case, the LSTM network serves the purpose of context-dependent feature extraction by modeling relations among utterances. We term our architecture ‘contextual LSTM’. We propose several architectural variants of it later in the paper.\\nLSTM\\nsc-L ST M\\nUtterance 1 Utterance 2 Utterance !\\nContext Sensitive Utterance features\\nDense Layer\\nSoftmax Output\\nLSTM LSTM\\nContext Independent Utterance features\\nFigure 1: Contextual LSTM network: input features are passed through an unidirectional LSTM layer, followed by a dense layer and then a softmax layer. Categorical cross entropy loss is taken for training. The dense layer activations serve as the output features.\\n 3.2.2 Contextual LSTM Architecture\\n  Let unimodal features have dimension k, each utterance is thus represented by a feature vector xi,t ∈ Rk, where t represents the tth utterance of the video i. For a video, we collect the vectors for all the utterances in it, to get Xi = [xi,1,xi,2, ...,xi,Li] ∈ RLi×k, where Li represents the number of utterances in the video. This matrix Xi serves as the input to the LSTM. Figure 1 demonstrates the functioning of this LSTM module.\\nIn the procedure getLstmFeatures(Xi) of Algorithm 1, each of these utterance xi,t is passed through a LSTM cell using the equations mentioned in line 32 to 37. The output of the LSTM cell hi,t is then fed into a dense layer and finally into a softmax layer (line 38 to 39). The activations of the dense layer zi,t are used as the contextdependent features of contextual LSTM.\\n 3.2.3 Training\\n  The training of the LSTM network is performed using categorical cross entropy on each utterance’s softmax output per video, i.e.,\\nloss = 1 N\\nN\\n∑ n=1\\nC\\n∑ c=1 yn,c log2( ˆyn,c),\\nwhere N = total number of utterances in a video, yn,c = original output of class c, and ˆyn,c = predicted output.\\nA dropout layer between the LSTM cell and dense layer is introduced to check overfitting. As the videos do not have same the number of utterances, padding is introduced to serve as neutral utterances. To avoid the proliferation of noise within the network, masking is done on these padded utterances to eliminate their effect in the network. Parameter tuning is done on the train set by splitting it into train and validation components with 80/20% split. RMSprop has been used as the optimizer which is known to resolve Adagrad’s radically diminishing learning rates (Duchi et al., 2011). After feeding the train set to the network, the test set is passed through it to generate their context-dependent features.\\nDifferent Network Architectures We consider the following variants of the contextual LSTM architecture in our experiments -\\nsc-LSTM This variant of the contextual LSTM architecture consists of unidirectional\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nLSTM cells. As this is the simple variant of the contextual LSTM, we termed it as simple contextual LSTM (sc-LSTM)\\nh-LSTM We also test on an architecture where the dense layer after the LSTM cell is omitted. Thus, the output of the LSTM cell hi,t provides our context-dependent features and the softmax layer provides the classification. We call this architecture hidden LSTM (h-LSTM).\\nbc-LSTM Bi-directional LSTMs are two unidirectional LSTMs stacked together having opposite directions. Thus, an utterance can get information from other utterances occurring before and after itself in the video. We replaced the regular LSTM with a bi-directional LSTM and named the resulting architecture as bi-directional contextual LSTM (bc-LSTM). The training process of this architecture is similar to sc-LSTM.\\nuni-SVM In this setting, we first obtain the unimodal features as explained in Section 3.1, concatenate them and then send to a SVM for the final classification. It should be noted that using a gated recurrent unit (GRU) instead of LSTM did not improve the performance.\\n 3.3 Fusion of Modalities\\n  We accomplish multimodal fusion in two different ways as explained below -\\n 3.3.1 Non-hierarchical Framework\\n  In non-hierarchical framework, we concatenate context-independent unimodal features (from Section 3.1) and feed that into the contextual LSTM networks, i.e., sc-LSTM, bc-LSTM, and h-LSTM.\\n 3.3.2 Hierarchical Framework\\n  Contextual unimodal features, taken as input, can further improve performance of the multimodal fusion framework explained in Section 3.3.1. To accomplish this, we propose a hierarchical deep network which comprises of two levels –\\nLevel-1: context-independent unimodal features (from 3.1) are fed to the proposed LSTM network (Section 3.2.2) to get context-sensitive unimodal feature representations for each utterance. Individual LSTM networks are used for each modality.\\nLevel-2: consists of a contextual LSTM network similar to Level-1 but independent in training and computation. Output from each LSTM network in Level-1 are concatenated and fed into\\nthis LSTM network, thus providing an inherent fusion scheme - the prime objective of this level (Fig 2). The performance of the second level banks on the quality of the features from the previous level, with better features aiding the fusion process. Algorithm 1 describes the overall computation for utterance classification. For the hierarchical framework, we train Level 1 and Level 2 successively but separately.\\nWeight Bias Wi,Wf ,Wc,Wo ∈ Rd×k bi, bf , bc, bo ∈ Rd Pi, Pf , Pc, PoVo ∈ Rd×d bz ∈ Rm\\nWz ∈ Rm×d bsft ∈ Rc Wsft ∈ Rc×m\\nTable 2: Summary of notations used in Algorithm 1. Note: d - dimension of hidden unit. k - dimension of input vectors to LSTM layer . c - number of classes.\\n 4 Experimental Results\\n  \\n 4.1 Dataset details\\n  Most of the research in multimodal sentiment analysis is performed on datasets with speaker overlap in train and test splits.\\nBecause each individual has a unique way of expressing emotions and sentiments, finding generic, person-independent features for sentimental analysis is very tricky. In real-world applications, the\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nAlgorithm 1 Proposed Architecture 1: procedure TRAINARCHITECTURE( U, V) 2: Train context-independent models with U 3: for i:[1,M] do ▷ extract baseline features 4: for j:[1,Li] do 5: xi,j ← TextFeatures(ui,j) 6: x ′\\ni,j ← V ideoFeatures(ui,j) 7: x”i,j ← AudioFeatures(ui,j) 8: Unimodal: 9: Train LSTM at Level-1 with X,X ′ andX”.\\n10: for i:[1,M] do ▷ unimodal features 11: Zi ← getLSTMFeatures(Xi) 12: Z ′ i ← getLSTMFeatures(X ′\\ni) 13: Z”i ← getLSTMFeatures(X”i ) 14: Multimodal: 15: for i:[1,M] do 16: for j:[1,Li] do 17: if Non-hierarchical fusion then 18: x∗i,j ← (xi,j ∣∣x ′\\ni,j ∣∣x”i,j) ▷ concatenation\\n19: else 20: if Hierarchical fusion then 21: x∗i,j ← (zi,j ∣∣z ′\\ni,j ∣∣z”i,j) ▷ concatenation 22: Train LSTM at Level-2 with X∗. 23: for i:[1,M] do ▷ multimodal features 24: Z∗i ← getLSTMFeatures(X∗i ) 25: testArchitecture( V) 26: return Z∗\\n27: procedure TESTARCHITECTURE( V) 28: Similar to training phase. V is passed through the\\nlearnt models to get the features and classification outputs. Table 2 shows the trainable parameters.\\n29: procedure GETLSTMFEATURES(Xi) ▷ for ith video 30: Zi ← φ 31: for t:[1,Li] do ▷ Table 2 provides notation 32: it ← σ(Wixi,t + Pi.ht−1 + bi) 33: C̃t ← tanh(Wcxi,t + Pcht−1 + bc) 34: ft ← σ(Wfxt + Pfht−1 + bf) 35: Ct ← it ∗ C̃t + ft ∗Ct−1 36: ot ← σ(Woxt + Poht−1 + VoCt + bo) 37: ht ← ot ∗ tanh(Ct) ▷ output of lstm cell 38: zt ← ReLU(Wzht + bz) ▷ dense layer 39: prediction← softmax(Wsftzt + bsft) 40: Zi ← Zi ∪ zt 41: return Zi\\nmodel should be robust to person variance but it is very difficult to come up with a generalized model from the behavior of a limited number of individuals To this end, we perform person-independent experiments to emulate unseen conditions. Our train/test splits of the datasets are completely disjoint with respect to speakers.\\nWhile testing, our models have to classify emotions and sentiments from utterances by speakers they have never seen before.\\nIEMOCAP: The IEMOCAP contains the acts of 10 speakers in a two way conversation segmented into utterances. The database contains the following categorical labels: anger, happiness, sadness, neutral, excitement, frustration, fear, surprise, and other, but we take only the first four so as to compare with the state of the art (Rozgic et al., 2012b) and other authors. Videos by the first 8 speakers are considered in the train set. The train/test split details are provided in table 3.\\nMOSI: The MOSI dataset is a dataset rich in sentimental expressions where 93 persons review topics in English. It contains positive and negative classes as its sentiment labels. The train/validation set comprises of the first 62 individuals in the dataset.\\nMOUD: This dataset contains product review videos provided by around 55 persons. The reviews are in Spanish (we use Google Translate API1 to get the english transcripts). The utterances are labeled to be either positive, negative or neutral. However, we drop the neutral label to maintain consistency with previous work. The first 59 videos are considered in the train/val set.\\nTable 3 provides information regarding train/test split of all the datasets. In these splits it is ensured that 1) No two utterances from the train and test splits belong to the same video. 2) The train/test splits have no speaker overlap. This provides the speaker-independent setting.\\nTable 3 also provides cross dataset split details where the complete datasets of MOSI and MOUD are used for training and testing respectively. The proposed model being used on reviews from different languages allows us to analyze its robustness and generalizability.\\nIt should be noted that the datasets’ individual configuration and splits are same throughout all the experiments (i.e., context-independent unimodal feature extraction, LSTM-based context-\\n1http://translate.google.com\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\ndependent unimodal and multimodal feature extraction and classification).\\n 4.2 Performance of Different Models and Comparisons\\n  In this section, we present unimodal and multimodal sentiment analysis performance of different LSTM network variants as explained in Section 3.2.3 and comparison with the state of the art.\\nHierarchical vs Non-hierarchical Fusion Framework - As expected, trained contextual unimodal features help the hierarchical fusion framework to outperform the non-hierarchical framework. Table 4 demonstrates this by comparing both hierarchical and non-hierarchical framework using the bc-LSTM network. Due to this fact, we provide all further analysis and results using the hierarchical framework. Nonhierarchical model outperforms the performance of the baseline uni-SVM. This further leads us to conclude that it is the context-sensitive learning paradigm which plays the key role in improving performance over the baseline.\\nComparison among Network Variants - It is to be noted that both sc-LSTM and bc-LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets. Since, bcLSTM has access to both the preceding and following information of the utterance sequence, it performs consistently better on all the datasets over sc-LSTM.\\nThe usefulness of the dense layer in improving the performance is prominent from the experimental results as shown in Table 4. The performance improvement is in the range of 0.3% to 1.5% on MOSI and MOUD datasets. On the IEMOCAP dataset, the performance improvement of bc-LSTM and sc-LSTM over h-LSTM is in the range of 1% to 5%.\\nComparison with the Baseline and state of the art - Every LSTM network variant has outperformed the baseline uni-SVM on all the datasets by the margin of 2% to 5%(see Table 4). These results prove our initial hypothesis that modeling the contextual dependencies among utterances, which uni-SVM cannot do, improves the classification. The higher performance improvement on the IEMOCAP dataset indicates the necessity of modeling long-range dependencies among the utterances as continuous emotion recognition\\nis a multiclass sequential problem where a person doesnt frequently change emotions (Wöllmer et al., 2008).\\nWe have implemented and compared with the current state-of-the-art approach proposed by Poria et al. (Poria et al., 2015). In their method, they extracted features from each modality and fed to a multiple kernel learning (MKL) classifier. However, they did not conduct the experiment in speaker-independent manner and also did not consider the contextual relation among the utterances. Experimental results in Table 5 shows that the proposed method has outperformed Poria et al. (Poria et al., 2015) by a significant margin. For the emotion recognition task, we have compared our method with the current state of the art (Rozgic et al., 2012b), who extracted features in a similar fashion to (Poria et al., 2015) did. However, for fusion they used SVM trees.\\n 4.3 Importance of the Modalities\\n  As expected, in all kinds of experiments, bimodal and trimodal models have outperformed unimodal models. Overall, audio modality has performed better than visual on all the datasets. On MOSI and IEMOCAP datasets, textual classifier achieves the best performance over other unimodal classifiers. On IEMOCAP dataset, the unimodal and multimodal classifiers obtained poor performance to classify neutral utterances. Textual modality, combined with non-textual modes boosts the performance in IEMOCAP by a large margin. However, the margin is less in the other datasets.\\nOn the MOUD dataset, textual modality performs worse than audio modality due to the noise introduced in translating Spanish utterances to English. Using Spanish word vectors2 in text-CNN results in an improvement of 10% . Nonetheless, we report results using these translated utterances as opposed to utterances trained on Spanish word vectors, in order to make fair comparison with (Poria et al., 2015).\\n 4.4 Generalization of the Models\\n  To test the generalizability of the models, we have trained our framework on complete MOSI dataset and tested on MOUD dataset (Table 6). The performance was poor for audio and textual modality as the MOUD dataset is in Spanish while the model is trained on MOSI dataset which is in En-\\n2http://crscardellino.me/SBWCE\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nModality MOSI MOUD IEMOCAP hierarchical (%)\\nno n-\\nhi er\\n(% ) hierarchical (%)\\nno n-\\nhi er\\n(% ) hierarchical (%)\\nno n-\\nhi er\\n(% )\\nun i-\\nSV M\\nhL\\nST M\\nsc -L\\nST M\\nbc -L\\nST M\\nun i-\\nSV M\\nhL\\nST M\\nsc -L\\nST M\\nbc -L\\nST M\\nun i-\\nSV M\\nhL\\nST M\\nsc -L\\nST M\\nbc -L\\nST M\\nT 75.5 77.4 77.6 78.1 49.5 50.1 51.3 52.1 65.5 68.9 71.4 73.6 V 53.1 55.2 55.6 55.8 46.3 48.0 48.2 48.5 47.0 52.0 52.6 53.2 A 58.5 59.6 59.9 60.3 51.5 56.3 57.5 59.9 52.9 54.4 55.2 57.1\\nT + V 76.7 78.9 79.9 80.2 78.5 50.2 50.6 51.3 52.2 50.9 68.5 70.3 72.3 75.4 73.2 T + A 75.8 78.3 78.8 79.3 78.2 53.1 56.9 57.4 60.4 55.5 70.1 74.1 75.2 75.6 74.5 V + A 58.6 61.5 61.8 62.1 60.3 62.8 62.9 64.4 65.3 64.2 67.6 67.8 68.2 68.9 67.3 T + V + A 77.9 78.1 78.6 80.3 78.1 66.1 66.4 67.3 68.1 67.0 72.5 73.3 74.2 76.1 73.5\\nTable 4: Comparison of models mentioned in Section 3.2.3. The table reports the accuracy of classification. Note: non-hier ← Non-hierarchical bc-lstm. For remaining fusion hierarchical fusion framework is used (Section 3.3.2)\\nModality Sentiment (%) Emotion on IEMOCAP (%)MOSI MOUD angry happy sad neutral T 78.12 52.17 76.07 78.97 76.23 67.44 V 55.80 48.58 53.15 58.15 55.49 51.26 A 60.31 59.99 58.37 60.45 61.35 52.31\\nT + V 80.22 52.23 77.24 78.99 78.35 68.15 T + A 79.33 60.39 77.15 79.10 78.10 69.14 V + A 62.17 65.36 68.21 71.97 70.35 62.37 A + V + T 80.30 68.11 77.98 79.31 78.30 69.92 State-of 73.551 63.251 73.10 2 72.402 61.902 58.102-the-art 1by (Poria et al., 2015),2by (Rozgic et al., 2012b)\\nTable 5: Accuracy % on textual (T), visual (V), audio (A) modality and comparison with the state of the art. For fusion, hierarchical fusion framework was used (Section 3.3.2)\\nModality MOSI→MOUDuni-SVM h-LSTM sc-LSTM bc-LSTM T 46.5% 46.5% 46.6% 46.9% V 43.3% 45.5% 48.3% 49.6% A 42.9% 46.0% 46.4% 47.2%\\nT + V 49.8% 49.8% 49.8% 49.8% T + A 50.4% 50.9% 51.1% 51.3% V + A 46.0% 47.1% 49.3% 49.6% T + V + A 51.1% 52.2% 52.5% 52.7%\\nTable 6: Cross Dataset comparison. The table reports the accuracy of classification.\\nglish language. However, notably visual modality performs better than other two modalities in this experiment which signifies that in cross-lingual scenarios facial expressions carry more generalized, robust information than audio and textual modalities. We could not carry out the similar experiment for emotion recognition as no other utterance-level dataset apart from the IEMOCAP was available at the time of our experiments.\\n 4.5 Qualitative Analysis\\n  In some cases the predictions of the proposed method are wrong given the difficulty in recognizing the face and noisy audio signal in the utterances. Also, cases where the sentiment is very weak and non contextual, the proposed approach shows some bias towards its surrounding utter-\\nances which further leads to wrong predictions. This can be solved by developing a context aware attention mechanism. In order to have a better understanding on roles of modalities for overall classification, we also have done some qualitative analysis. For example, this utterance - ”who doesn’t have any presence or greatness at all.”, was classified as positive by the audio classifier (“doesn’t” was spoken normally by the speaker, but “presence and greatness at all” was spoken with enthusiasm). However, textual modality caught the negation induced by “doesn’t” and classified correctly. In another utterance “amazing special effects” as there was no jest of enthusiasm in speaker’s voice and face audio-visual classifier failed to identify the positivity of this utterance. On the other textual classifier correctly detected the polarity as positive.\\nOn the other hand, textual classifier classified this sentence - “that like to see comic book characters treated responsibly” as positive, possibly because of the presence of positive phrases such as “like to see”, “responsibly”. However, the high pitch of anger in the person’s voice and the frowning face helps identify this to be a negative utterance.\\n 5 Conclusion\\n  Contextual relationship among the utterances is mostly ignored in the literature. In this paper, we developed a LSTM-based network to extract contextual features from the utterances of a video for multimodal sentiment analysis. The proposed method has outperformed the state of the art and showed significant performance improvement over the baseline. As a part of the future work, we plan to propose LSTM attention model to determine importance of the utterances and contribution of modalities in the sentiment classification.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899\\n Abstract Multimodal sentiment analysis is a developing area of research, which involves identification of emotions and sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the inter-dependencies and relations among utterances of a video. In this paper, we propose an LSTM based model which enables these utterances to capture contextual information from its surroundings in the same video, thus aiding the classification process. Our model shows 5 − 10% improvement over the state of the art and high robustness to generalizability.   \n",
            "134                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  1 000\\n011\\n012\\n013\\n014\\n015\\n016\\n017\\n018\\n019\\n020\\n021\\n022\\n023\\n024\\n025\\n026\\n027\\n028\\n029\\n030\\n031\\n032\\n033\\n034\\n035\\n036\\n037\\n038\\n039\\n040\\n041\\n042\\n043\\n044\\n045\\n046\\n047\\n048\\n049\\n061\\n062\\n063\\n064\\n065\\n066\\n067\\n068\\n069\\n070\\n071\\n072\\n073\\n074\\n075\\n076\\n077\\n078\\n079\\n080\\n081\\n082\\n083\\n084\\n085\\n086\\n087\\n088\\n089\\n090\\n091\\n092\\n093\\n094\\n095\\n096\\n097\\n098\\n099\\n 1 Introduction\\n  Keyphrase or keyword is a piece of short and summative content that expresses the main semantic meaning of a long text. A typical use of keyphrase or keyword is in scientific publications, to provide the core information of a paper. We use the term “keyphrase”, interchangeable as “keyword”, in the rest of this paper, as it implies that it may contain multiple words. High-quality keyphrases can facilitate the understanding, organizing and accessing of document content. As a result, many stud-\\nies have devoted to studying the ways of automatic extracting keyphrases from textual content (Liu et al., 2009; Medelyan et al., 2009a; Witten et al., 1999). Due to the public accessibility, many scientific publication datasets are often used as the test beds for keyphrase extraction algorithms. Therefore, this study also focuses on extracting keyphrases from scientific publications.\\nAutomatically extracting keyphrases from a document is called Keypharase Extraction, and it has been widely exploited in many applications, such as information retrieval (Jones and Staveley, 1999), text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), and opinion mining (Berend, 2011). Most of the existing keyphrase extraction algorithms addressed this problem through two steps (Liu et al., 2009; Tomokiyo and Hurst, 2003). The first step is to acquire a list of keyphrase candidates. Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying the potential candidates (Hulth, 2003; Le et al., 2016; Liu et al., 2010; Wang et al., 2016). The second step is to rank candidates regarding their importance to the document, either through supervised or unsupervised machine learning methods with a set of manuallydefined features (Frank et al., 1999; Liu et al., 2009, 2010; Kelleher and Luz, 2005; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Song et al., 2003; Witten et al., 1999).\\nThere are two major drawbacks for the above keyphrase extraction approaches.\\nFirstly, they can only extract the keyphrases that appear in the source text, whereas they fail at predicting the meaningful keyphrases with a slightly different sequential order or using synonym words. However, it is common in a scientific publication where authors assign keyphrases based on the semantic meaning instead of follow-\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\ning the written content in the publication. In this paper, we denote the phrases that do not match any contiguous subsequence of source text as absent keyphrases, and the ones that fully match a part of the text as present keyphrases. Table 1 shows the proportion of present and absent keyphrases from the document abstract in four commonlyused datasets, where we observe large portions of absent keyphrases in all the datasets. The absent keyphrases cannot be extracted through previous approaches, which further urges the development of a more powerful keyphrase prediction model.\\nSecondly, when ranking phrase candidates, previous approaches often adopted the machine learning features such as TF-IDF and PageRank. However, these features only target to detect the importance of each word in the document based on the statistics of word occurrence and co-occurrence, whereas they can hardly reveal the semantics behind the document content.\\nTable 1: Proportion of the present keyphrases and absent keyphrases in four public datasets\\nDataset # Keyphrase % Present % Absent Inspec 19,275 55.69 44.31\\nKrapivin 2,461 44.74 52.26 NUS 2,834 67.75 32.25\\nSemEval 12,296 42.01 57.99\\nTo overcome the limitations of previous studies, we re-examine the process of Keyphrase Prediction, about how real human annotators would assign keyphrases. Given a document, human annotators will firstly read the text to get a basic understanding of the content, then they try to digest its essential content and summarize into keyphrases. Their generation of keyphrases relies on the understanding of the content, which not necessarily to be the words that occurred in the source text. For example, when human annotators see “Latent Dirichlet Allocation” in the text, they could write down “topic modeling” and/or “text mining” as possible keyphrases. In addition to the semantic understanding, human annotators might also go back and picks up the most important parts based on syntactic features. For example, the phrases following “we propose/apply/use” are supposed to be important in the text. As a result, a better keyphrase prediction model should understand the semantic meaning of the content, as well as capture the contextual features.\\nTo effectively capture semantic and syntactic features, we utilize recurrent neural networks (RNN) (Cho et al., 2014; Gers and Schmidhuber, 2001) to compress the semantic information in the given text into a dense vector (i.e., semantic understanding). Furthermore, we incorporate a copy mechanism (Gu et al., 2016) to equip our model with the capability of finding important parts based on language syntax (i.e., syntactic understanding). Thus, our model can generate keyphrases based on the understanding of the text, no matter whether the keyphrases are present in the text or not; meanwhile, it does not lose important in-text information.\\nThe contribution of this paper is in three-fold: a) we propose to apply an RNN-based generative model to keyphrase prediction, and we also incorporate a copy mechanism in RNN, which enables the model to successfully predict rarely-occurred phrases; b) this is the first work concerning about the problem of absent keyphrase prediction for scientific publications, and our model recalls up to 20% of absent keyphrases; and c) we conduct a comprehensive comparison against six important baselines on a broad range of datasets, and the results show that our proposed model significantly outperforms existing supervised and unsupervised extraction methods.\\nIn the remainder of this paper, we firstly review the related work in Section 2. Then we elaborate the proposed model in Section 3. After that, we present the experiment setting in Section 4 and results in Section 5, followed by our discussion in Section 6. Section 7 concludes the paper.\\n 2 Related Work\\n  \\n 2.1 Automatic Keyphrase Extraction\\n  Keyphrase provides a succinct and accurate way of describing a subject or a subtopic in a document. A number of extraction algorithms have been proposed, and typically the process of extracting can be broken down into two steps.\\nThe first step is to generate a list of phrase candidates with heuristic methods. As these candidates are prepared for further filtering, a considerable amount of candidates are produced in this step to increase the possibility that most of the correct keyphrases are kept. The primary ways of extracting candidates include retaining word sequences that match certain part-of-speech tag patterns (e.g., nouns, adjectives) (Liu et al., 2011;\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\nWang et al., 2016; Le et al., 2016), and extracting important n-grams or noun phrases (Hulth, 2003; Medelyan et al., 2008).\\nThe second step is to score each candidate phrase regarding its likelihood of being a keyphrase in the given document. The top-ranked candidates are returned as keyphrases. Both supervised and unsupervised machine learning methods are widely employed here. For supervised methods, this task is solved as a binary classification problem, and various of learning methods and features have been explored (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009b; Lopez and Romary, 2010; Gollapalli and Caragea, 2014). As for the unsupervised approaches, primary ideas include finding the central nodes in text graph (Mihalcea and Tarau, 2004; Grineva et al., 2009), detecting representative phrases from topical clusters (Liu et al., 2009, 2010) and so on.\\nAside from the commonly adopted two steps process, another two previous studies realized the keyphrase extraction in entirely different ways. Tomokiyo and Hurst (2003) applied two language models to measure the phraseness and informativeness of phrases. Liu et al. (2011) share the most similar idea to our work. They used a word alignment model, which learns the translation from the documents to the keyphrases. This approach alleviates the problem of vocabulary gap between source and target to a certain degree. However, this translation model can hardly deal with semantic meaning. Additionally, they trained the model with the target of title/summary to enlarge the number of training samples, which may diverge away from the real objective of generating keyphrases.\\n 2.2 Encoder-Decoder Model\\n  The RNN Encoder-Decoder model (also referred as Sequence-to-Sequence Learning) is an end-toend approach. It was firstly introduced by Cho et al. (2014) and Sutskever et al. (2014) to solve translation problems. As it provides a powerful tool for modeling variable-length sequence in an end-to-end fashion, it fits many Natural Language Processing tasks and soon achieves great successes (Rush et al., 2015; Vinyals et al., 2015; Serban et al., 2016).\\nDifferent strategies have been explored to improve the performance of Encoder-Decoder model. The attention mechanism (Bahdanau et al.,\\n2014) is a soft alignment approach that allows the model to automatically locate the relevant input component. In order to make use of the important information in the source text, some studies sought ways to copy certain parts of content from the source text and paste them into target text (Allamanis et al., 2016; Gu et al., 2016; Zeng et al., 2016). There exists a discrepancy between the optimizing objective during training and the metrics during evaluation. A few studies attempted to eliminate this discrepancy by incorporating new training algorithm (Marc’Aurelio Ranzato et al., 2016) or modifying optimizing objective(Shen et al., 2016).\\n 3 Methodology\\n  This section will introduce in detail our proposed deep keyphrase generation. Firstly, the task of keyphrase generation is defined, followed by the overview of how we apply the RNN EncoderDecoder model. Details of the framework as well as the copy mechanism will be introduced in Section 3.3 and 3.4.\\n 3.1 Problem Definition\\n  Given a keyphrase dataset consisting of N data samples, the i-th data sample (x(i),p(i)) contains one source text x(i), and Mi target keyphrases p(i) = (p(i,1),p(i,2), . . . ,p(i,Mi)). Both the source text x(i) and keyphrase p(i,j) are sequences of words:\\nx(i) = x (i) 1 , x (i) 2 , . . . , x (i) L xi\\np(i,j) = y (i,j) 1 , y (i,j) 2 , . . . , y (i,j) L p(i,j)\\nLx(i) and Lp(i,j)denotes the length of word sequence of x(i) and p(i,j) respectively.\\nNow for each data sample, there are one source text sequence and multiple target phrase sequences. To apply the RNN Encoder-Decoder model, the data need to be converted into textkeyphrase pairs which contain only one source sequence and one target sequence. One simple way is to split the data sample (x(i),p(i)) intoMi pairs: (x(i),p(i,1)), (x(i),p(i,2)), . . . , (x(i),p(i,Mi)). Then the Encoder-Decoder model is ready to be applied to learn the mapping from source sequence to target sequence. For the purpose of simplicity, (x,y) is used to denote each data pair in the rest of this section, where x is the word sequence of a source text and y is the word sequence of its keyphrase.\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\n 3.2 Encoder-Decoder Model\\n  The basic idea of our keyphrase generation model is to compress the content of source text into a hidden representation with encoder, and generate corresponding keyphrases by the decoder based on the representation . Both the encoder and decoder are implemented with recurrent neural networks (RNN).\\nThe encoder RNN converts the variable-length input sequence x = (x1, x2, ..., xT ) into a set of hidden representation h = (h1, h2, . . . , hT ), by iterating the following equations along the time t:\\nht = f (xt,ht−1) (1)\\nwhere f is a non-linear function. And we get the context vector c, acting as the representation of the whole input x, through a non-linear function q.\\nc = q(h1, h2, ..., hT ) (2)\\nThe decoder is another RNN, decompresses the context vector and generates a variable-length sequence y = (y1, y2, ..., yT ′) word by word, through a conditional language model:\\nst = g(yt−1, st−1, c) p(yt|y1,...,t−1,x) = g(yt−1, st, c) (3)\\nwhere st is the hidden state of decoder RNN at time t. The non-linear function g is a softmax classifier which outputs the probabilities of all the words in the vocabulary. yt is the predicted word at time t, by taking the word with largest probability after g(·).\\nThe encoder and decoder networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence. After training, we use the beam search to generate phrases and a max heap is maintained to get the predictions with highest probabilities.\\n 3.3 Details of Encoder and Decoder\\n  A bidirectional Gated Recurrent Unit (GRU) is applied as our encoder to replace the simple recurrent neural network. Previous studies (Bahdanau et al., 2014; Cho et al., 2014) indicate it can provide a general better performance language modeling than simple RNN and a simpler structure than Long Short-term Memory Networks(Hochreiter and Schmidhuber, 1997). So the above non-linear function f is replaced by the GRU function (see in (Cho et al., 2014)).\\nAnother forward GRU is utilized as the decoder. In addition, an attention mechanism is adopted to improve the performance. The attention mechanism was firstly introduced by Bahdanau et al. (2014) to make the model focus on the important parts in input dynamically. The context vector c is computed as a weighted sum of hidden representation h = (h1, . . . , hT ):\\nci = T∑ j=1 αijhj αij = exp(a(si−1, hj))∑T k=1 exp(a(si−1, hk))\\n(4)\\nwhere a(si−1, hj) is a soft alignment function that measures the similarity between si−1 and hj , namely to which degree the inputs around position j and the output at position i match.\\n 3.4 Copy Mechanism\\n  To ensure the quality of learned representation and reduce the size of vocabulary, typically the RNN model only considers a certain number of frequent words (e.g. 30,000 words in (Cho et al., 2014)), but a large amount of long-tail words are simply ignored. Therefore the RNN is not able to predict any keyphrase which contains out-of-vocabulary words. Actually, the important phrases can also be identified by their syntactic and location features, even though we don’t know the meanings. The copy mechanism is one feasible solution that enables RNN to predict unknown words based on contextual features.\\nBy incorporating the copy mechanism, the probability of predicting each new word yt would consist of two parts. The first term is the probability of generating the term (see Equation 3) and the second one is the probability of copying it from source text:\\np(yt|y1,...,t−1,x) = pg(yt|y1,...,t−1,x) + pc(yt|y1,...,t−1,x) (5)\\nSimilar to attention mechanism, the copy mechanism pinpoints the appearance of yt−1 in source text, and use its location information (ρtτ ) to compute the weighted hidden representation (ζ(yt−1)) of input x.\\nζ(yt−1) = TS∑ τ=1 ρtτhτ (6)\\nρtτ = { 1 K p(xτ , c|st−1) xτ = yt−1 0 Otherwise\\n(7)\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nwhere K is the normalization term. Subsequently, we obtain the probabilities of copying pc(yt|y1,...,t−1) following the Equation 3, in which the target words are replaced by the words in source text, and the vector of yt−1 is replaced by [e(yt−1), ζ(yt−1)]\\nT , where the e(yt−1) is the embedding of last predicted word.\\n 4 Experiment Settings\\n  This section starts with discussing how we design our evaluation experiments, followed by the description of training and testing datasets. Then, we introduce evaluation metrics and baselines.\\n 4.1 Training Dataset\\n  There are several publicly-available datasets for evaluating keyphrase generation. The largest one came from Krapivin et al. (2008), which contained 2,304 scientific publications. However, this amount of data is unable to train a robust recurrent neural network model. In fact, there are millions of scientific papers available online, each of which contains the keyphrases assigned by authors. Therefore, we collected a large amount of high-quality scientific metadata in Computer Science domain from various online digital libraries, including ACM Digital Library, ScienceDirect, Wiley, Web of Science, and so on. In total, we obtained 567,830 articles after removing duplicates and overlaps, which is 200 times larger than the one of Krapivin et al. (2008). Note that our model is only trained on 527,830 articles since 40,000 publications are held out for training baselines and for building a test dataset.\\n 4.2 Testing Datasets\\n  For evaluating the proposed model more comprehensively, four widely-adopted scientific publication datasets are used. In addition, since these datasets only contain few hundreds or thousands of publications, we contribute a new testing dataset KP20k with a much larger number of scientific articles. We take the title and abstract as the source text. Each dataset is described in details in the below text.\\n– Inspec (Hulth, 2003): This dataset provides 2,000 paper abstracts. We adopt the 500 testing papers and the corresponding uncontrolled keyphrases for evaluation, and the remaining 1,500 papers are used for training the supervised baseline models.\\n– Krapivin (Krapivin et al., 2008): This dataset provides 2,304 papers with full-text and author-assigned keyphrases. However, the author did not mention how to split testing data, so we simply select the first 400 papers in alphabetical order as the testing data, and the remaining papers are used to train the supervised baselines.\\n– NUS (Nguyen and Kan, 2007): We use the author-assigned keyphrases and treat all 211 papers as the testing data. Since the NUS dataset did not specifically mention the ways of splitting training and testing data, the results of the supervised baseline models are obtained through a five-fold cross validation.\\n– SemEval-2010 (Kim et al., 2010): 288 articles are collected from ACM Digital Library. 100 articles are used for testing and the rest are for training supervised baselines.\\n– KP20k: We build a new testing dataset that contains the title, abstract and keyphrases of 20,000 scientific articles in Computer Science. They are randomly selected from our obtained 567,830 articles. For the supervised baselines, another 20,000 articles are randomly selected for training. Therefore, our proposed model is trained on 527,830 that holds out these 40,000 articles.\\n 4.3 Implementation Details\\n  In total, there are 2,780,316 〈text, keyphrase〉 pairs for training, in which text refers to the concatenation of the title and abstract of a publication, and keyphrase indicates an author-assigned keyword. The text pre-processing steps including tokenization, lowercasing and replacing all digits with symbol 〈digit〉 are applied. Two encoder-decoder models are trained, one with only attention mechanism (RNN) and one with both attention and copy mechanism enabled (CopyRNN). For both models, we choose the top 50,000 frequently-occurred words as our vocabulary, the dimension of embedding is set to 150, and the dimension of hidden layers is set to 300. Models are optimized using Adam (Kingma and Ba, 2014) with initial learning rate = 10−4, gradient clipping = 0.1 and dropout rate = 0.5. The max depth of beam search is set to 6, and the beam size is set to 200. In the generation of keyphrases, we find that the model tends to assign higher probabilities for shorter keyphrases,\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nwhereas most of keyphrases contain more than two words. To resolve this problem, we apply a simple heuristic by preserving only the first singleword phrase (with the highest generating probability) and removing the rest.\\n 4.4 Baseline Models\\n  Four unsupervised algorithms (Tf-Idf, TextRank (Mihalcea and Tarau, 2004), SingleRank (Wan and Xiao, 2008), ExpandRank (Wan and Xiao, 2008)) and two supervised algorithms (KEA (Witten et al., 1999) and Maui (Medelyan et al., 2009a)) are adopted as baselines. We set up the four unsupervised methods following the optimal settings in (Hasan and Ng, 2010) and the two supervised methods following the default setting as specified in their papers.\\n 4.5 Evaluation Metric\\n  Three evaluation metrics, the macro-averaged precision, recall and F-measure (F1) are employed for measuring the algorithm performance. Follow the standard definition, precision is defined as the number of correctly-predicted keyphrases over the number of all predicted keyphrases, recall is computed by the number of correctly-predicted keyphrases over the total data records. Note that, when determining the match of two keyphrases, we use Porter Stemmer for pre-processing.\\n 5 Results and Analysis\\n  We conduct an empirical study on three different tasks to evaluate our model.\\n 5.1 Predicting Present Keyphrases\\n  This is the same as the keyphrase extraction task in prior studies, in which we would like to analyze how well our proposed model perform on the commonly-defined task. To make a fair comparison, we only consider the present keyphrases for evaluation in this task. Table 2 provides the performances of the six baseline models, as well as our proposed models (i.e., RNN and CopyRNN). For each method, the table lists its F-measure at top 5 and top 10 predictions on the five datasets. The best scores are highlighted in bold and the underlines indicate the second best performances.\\nThe results show that the four unsupervised models (Tf-idf, TextTank, SingleRank and ExpandRank) perform robust across different datasets. The ExpandRank fails to return any result on the KP20k dataset due to its high time com-\\nplexity. The measures on NUS and SemEval here are higher than the ones reported in (Hasan and Ng, 2010) and (Kim et al., 2010), probably because we utilized the paper abstract instead of the full-text for training, which may filter out some noisy information. The performances of the two supervised models (i.e., Maui and KEA) are unstable on some datasets, but Maui achieves the best performances on three datasets among all the baseline models.\\nAs for our proposed keyphrase prediction approaches, the RNN model with the attention mechanism does not perform as well as we expected. It might be because the RNN model only concerns on finding the hidden semantics behind the text, which may tend to generate keyphrases or words that are too general and may not necessarily referring to the source text. In addition, it fails to recall keyphrases that contain out-of-vocabulary words (since the RNN model only takes the top 50,000 words in vocabulary). This indicates that a pure generative model may not fit the extraction task, and we need to further link back to the language usage in the source text. Indeed, the copyRNN model, by considering more contextual information, significantly outperforms not only the RNN model but also all baselines, exceeding the best baselines by more than 20% on average. This result demonstrates the importance of source text for extraction task. Besides, nearly 2% of all the correct predictions contain out-of-vocabulary words.\\nThe example in Figure 1(a) shows the result of predicted present keyphrases by RNN and CopyRNN for an article about video search. We see that both models can generate phrases that related to the topic of information retrieval and video. However most of RNN predictions are high-level terminologies, which are too general to be selected as keyphrases. The CopyRNN, on the other hand, predicts more detailed phrases like “video metadata” and “integrated ranking”. An interesting bad case is, “rich content” is coordinate with a keyphrase “video metadata”, and the CopyRNN puts it into prediction mistakenly.\\n 5.2 Predicting Absent Keyphrases\\n  As stated, one important motivation for this work is that we are interested in the proposed model’s capability for predicting absent keyphrases based on the “understanding” of content. It is worth noting that such prediction is a very challenging task, and, to the best of our knowledge, no ex-\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nMethod Inspec Krapivin NUS SemEval KP20k\\nF1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10\\nTf-Idf 0.221 0.313 0.129 0.160 0.136 0.184 0.128 0.194 0.102 0.126 TextRank 0.223 0.281 0.189 0.162 0.195 0.196 0.176 0.187 0.175 0.147\\nSingleRank 0.214 0.306 0.189 0.162 0.140 0.173 0.135 0.176 0.096 0.119 ExpandRank 0.210 0.304 0.081 0.126 0.132 0.164 0.139 0.170 N/A N/A\\nMaui 0.040 0.042 0.249 0.216 0.249 0.268 0.044 0.039 0.270 0.230 KEA 0.098 0.126 0.110 0.152 0.069 0.084 0.025 0.026 0.171 0.154\\nRNN 0.085 0.064 0.135 0.088 0.169 0.127 0.157 0.124 0.179 0.189 CopyRNN 0.278 0.342 0.311 0.266 0.334 0.326 0.293 0.304 0.333 0.262\\nTable 2: The performance of predicting present keyphrase of various models on five benchmark datasets\\nisting methods can handle this task. Therefore, we only provide the RNN and copyRNN performances in the discussion of the results of this task. Here, we evaluate the performance with the recall of top 10 and top 50 results, to see how many absent keyphrases can be correctly predicted. We use the absent keyphrases in the testing datasets for evaluation.\\nDataset RNN CopyRNN R@10 R@50 R@10 R@50\\nInspec 0.031 0.061 0.047 0.100 Krapivin 0.095 0.156 0.113 0.202\\nNUS 0.050 0.089 0.058 0.116 SemEval 0.041 0.060 0.043 0.067 KP20k 0.083 0.144 0.125 0.211\\nTable 3: Absent keyphrases prediction performance of RNN and CopyRNN on five datasets\\nTable 3 present the recalls of the top 10/50 predicted keyphrases for our RNN and CopyRNN models, in which we observe that the CopyRNN can, on average, recall around 8% (15%) of keyphrases at top 10 (50) predictions. This indicates that, to some extent, both models can capture the hidden semantics behind the textual content and make reasonable predictions. In addition, with the advantage of features from the source text, the CopyRNN model also outperforms the RNN model in this condition, though not improve as much as the present keyphrase extraction task. An example is shown in Figure 1(b), in which we see that two absent keyphrases “video retrieval” and “video indexing” are correctly recalled by both models. The interesting thing is, the term “indexing” does not appear in the text, but the models\\nmay detect the information “index videos” in the first sentence and paraphrase it to the target phrase. And the CopyRNN successfully predicts another two keyphrases by capturing the detailed information from the text (highlighted text segments).\\n 5.3 Transfer to News Articles\\n  The RNN and CopyRNN are supervised models, and they are trained on data in specific domain and writing style. However, with sufficient training on a large-scale dataset, we expect the models to be able to learn universal language features that are effective in other corpus as well. Thus in this task, we will test our model on another type of text, to see whether the model would work when being transferred to a different environment.\\nWe utilize the popular news article dataset DUC-2001 (Wan and Xiao, 2008) for analysis. The dataset consists of 308 news articles and 2,488 manually annotated keyphrases. The result is shown in Table 4, from which we could see that the CopyRNN could extract a portion of correct keyphrases from a unfamiliar text. Compare to the results reported in (Hasan and Ng, 2010), the performance of CopyRNN is better than TextRank (Mihalcea and Tarau, 2004) and KeyCluster (Liu et al., 2009), but lags behind the other\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nthree baselines. As transfered to corpus in a complete strange type and domain, the model encounters more unknown words and has to rely more on the syntactic features in the text. In this experiment, the CopyRNN recalls 766 keyphrases. 14.3% of them contain out-of-vocabulary words and many names of persons and places are correctly predicted.\\n 6 Discussion\\n  Our experimental results demonstrate that the CopyRNN model not only performs well on predicting present keyphrases but also has the ability of generating topical relevant keyphrases that are absent in the text. In a broader sense, this model attempts to map a long text (i.e., paper abstract) with representative short text chunks (i.e., keyphrases), which can potentially be applied to improve information retrieval performance by generating high-quality index terms, as well as assisting user browsing by summarizing long documents into short readable phrases.\\nSo far we have examined our model on scientific publications and news articles, demonstrating that our model has the ability to capture universal language patterns and extract key information from unfamiliar texts. We believe that the models have a greater potential to be generalized to other domains and types, like books, online reviews etc., if it is trained on larger data corpus. Also, we directly apply our model, which is trained on publication dataset, into generating keyphrases for news articles without any adaptive training. We believe that with proper training on news data, the\\nmodel would make further improvement. Additionally, this work mainly studies the problem of discovering core content from textual materials. Here, the encoder-decoder framework is applied to model language; however, such framework can also be extended to locate the core information on other data resources such as to summarize content from images and videos.\\n 7 Conclusion and Future Work\\n  In this paper, we propose an RNN-based generative model for predicting keyphrase in scientific text. To the best of our knowledge, this is the first application of the encoder-decoder model to keyphrase prediction task. Our model summarizes phrases based the deep semantic meaning of the text and it is able to handle rarely-occurred phrases by incorporating a copy mechanism. Comprehensive empirical studies demonstrate the effectiveness of our proposed model for generating both present and absent keyphrases for different types of text. Our future work may include the following two directions.\\n– In this work, we only evaluated the performance of proposed model by conducting offline experiments. In the future, we are interested in comparing the model with human annotators and evaluating the quality of predicted phrases by human judges.\\n– Our current model does not fully consider the correlation among target keyphrases. It would also be interesting to explore the multiple-output optimization on our model.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899\\n Abstract Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divide the to-besummarized content into multiple text chunks, then rank and select the most meaningful ones. These approaches can neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also generates absent keyphrases based on the semantic meaning of the text.   \n",
            "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Neural Machine Translation via Binary Code Prediction  1 000\\n011\\n012\\n013\\n014\\n015\\n016\\n017\\n018\\n019\\n020\\n021\\n022\\n023\\n024\\n025\\n026\\n027\\n028\\n029\\n030\\n031\\n032\\n033\\n034\\n035\\n036\\n037\\n038\\n039\\n040\\n041\\n042\\n043\\n044\\n045\\n046\\n047\\n048\\n049\\n075\\n076\\n077\\n078\\n079\\n080\\n081\\n082\\n083\\n084\\n085\\n086\\n087\\n088\\n089\\n090\\n091\\n092\\n093\\n094\\n095\\n096\\n097\\n098\\n099\\n 1 Introduction\\n  When handling broad or open domains, machine translation systems usually have to handle a large vocabulary as their inputs and outputs. This is particularly a problem in neural machine translation (NMT) models (Sutskever et al., 2014), such as the attention-based model (Bahdanau et al., 2014; Luong et al., 2015) shown in Figure 1. In these models, the output layer is required to generate a specific word from an internal vector, and a large vocabulary size tends to require a large amount of computation to predict each of the candidate word probabilities.\\nBecause this is a significant problem for neural language and translation models, there are a number of methods proposed to resolve this problem, which we detail in Section 2.2. However, none of these previous methods simultaneously satisfies the following desiderata, all of which, we argue, are desirable for practical use in NMT systems:\\nMemory efficiency: The method should not require large memory to store the parameters and calculated vectors to maintain scalability in resource-constrained environments.\\nTime efficiency: The method should be able to train the parameters efficiently, and possible to perform decoding efficiently with choosing the candidate words from the full probability distribution. In particular, the method should be performed fast on general CPUs to suppress physical costs of computational resources for actual production systems.\\nCompatibility with parallel computation: It should be easy for the method to be minibatched and optimized to run efficiently on GPUs, which are essential for training large NMT models.\\nIn this paper, we propose a method that satisfies all of these conditions: requires significantly less memory, fast, and is easy to implement minibatched on GPUs. The method works by not pre-\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\ndicting a softmax over the entire output vocabulary, but instead by encoding each vocabulary word as a vector of binary variables, then independently predicting the bits of this binary representation. In order to represent a vocabulary size of 2n, the binary representation need only be at least n bits long, and thus the amount of computation and size of parameters required to select an output word is only O(log V ) in the size of the vocabulary V , a great reduction from the standard linear increase of O(V ) seen in the original softmax.\\nWhile this idea is simple and intuitive, we found that it alone was not enough to achieve competitive accuracy with real NMT models. Thus we make two improvements: First, we propose a hybrid model, where the high frequency words are predicted by a standard softmax, and low frequency words are predicted by the proposed binary codes separately. Second, we propose the use of convolutional error correcting codes with Viterbi decoding (Viterbi, 1967), which add redundancy to the binary representation, and even in the face of localized mistakes in the calculation of the representation, are able to recover the correct word.\\nIn experiments on two translation tasks, we find that the proposed hybrid method with error correction is able to achieve results that are competitive with standard softmax-based models while reducing the output layer to a fraction of its original size.\\n 2 Problem Description and Prior Work\\n  \\n 2.1 Formulation and Standard Softmax\\n  Most of current NMT models use one-hot representations to represent the words in the output vocabulary – each word w is represented by a unique sparse vector eid(w) ∈ RV , in which only one element at the position corresponding to the word ID id(w) ∈ {x ∈ N | 1 ≤ x ≤ V } is 1, while others are 0. V represents the vocabulary size of the target language. NMT models optimize network parameters by treating the one-hot representation eid(w) as the true probability distribution, and minimizing the cross entropy between it and the softmax probability v:\\nLH(v, id(w)) := H(eid(w),v), (1) = log sum expu− uid(w), (2)\\nv := expu/ sum expu, (3)\\nu := Whuh+ βu, (4)\\nwhere sumx represents the sum of all elements in x, xi represents the i-th element of x, Whu ∈\\nRV×H and βu ∈ RV are trainable parameters and H is the total size of hidden layers directly connected to the output layer.\\nAccording to Equation (4), this model clearly requires time/space computation in proportion to O(HV ), and the actual load of the computation of the output layer is directly affected by the size of vocabulary V , which is typically set around tens of thousands (Sutskever et al., 2014).\\n 2.2 Prior Work on Suppressing Complexity of NMT Models\\n  Several previous works have proposed methods to reduce computation in the output layer. The hierarchical softmax (Morin and Bengio, 2005) predicts each word based on binary decision and reduces computation time to O(H log V ). However, this method still requires O(HV ) space for the parameters, and requires calculation much more complicated than the standard softmax, particularly at test time. The differentiated softmax (Chen et al., 2016) divides words into clusters, and predicts words using separate part of the hidden layer for each word clusters. This method make the conversion matrix of the output layer sparser than a fully-connected softmax, and can reduce time/space computation amount by ignoring zero part of the matrix. However, this method restricts the usage of hidden layer, and the size of the matrix is still in proportion to V .\\nSampling-based approximations (Mnih and Teh, 2012; Mikolov et al., 2013) to the denominator of the softmax have also been proposed to reduce calculation at training. However, these methods are basically not able to be applied at test time, still require heavy computation like the standard softmax.\\nOther methods using characters (Ling et al., 2015) or subwords (Sennrich et al., 2016; Chitnis and DeNero, 2015) can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency.\\n 3 Binary Code Prediction Models\\n  \\n 3.1 Representing Words using Bit Arrays\\n  Figure 2(a) shows the conventional softmax prediction, and Figure 2(b) shows the binary code prediction model proposed in this study. Unlike the conventional softmax, the proposed method predicts each output word indirectly using dense\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\nFigure 2: Designs of output layers.\\nbit arrays that correspond to each word. Let b(w) := [b1(w), b2(w), · · · , bB(w)] ∈ {0, 1}B be the target bit array obtained for word w, where each bi(w) ∈ {0, 1} is an independent binary function given w, and B is the number of bits in whole array. For convenience, we introduce some constraints on b. First, a wordw is mapped to only one bit array b(w). Second, all unique words can be discriminated by b, i.e., all bit arrays satisfy that:\\nid(w) 6= id(w′)⇒ b(w) 6= b(w′). (5)\\nThird, multiple bit arrays can be mapped to the same word as described in Section 3.5. By considering second constraint, we can also constrain B ≥ dlog2 V e, because b should have at least V unique representations to distinguish each word. The output layer of the network independently predicts B probability values q := [q1(h), q2(h), · · · , qB(h)] ∈ [0, 1]B using the current hidden values h by logistic regressions:\\nq(h) = σ(Whqh+ βq), (6) σ(x) := 1/(1 + exp(−x)), (7)\\nwhere Whq ∈ RB×H and βq ∈ RB are trainable parameters. When we assume that each qi is the probability that “the i-th bit becomes 1,” the joint probability of generating word w can be represented as:\\nPr(w|h) := B∏ i=1 ( biqi + b̄iq̄i ) , (8)\\nAlgorithm 1 Mapping words to bit arrays. Require: w ∈ V Ensure: b ∈ {0, 1}B = Bit array representing w\\nx :=  0, if w = UNK 1, if w = BOS 2, if w = EOS 2 + rank(w), otherwise bi := bx/2i−1c mod 2 b← [b1, b2, · · · , bB ]\\nwhere x̄ := 1 − x. We can easily obtain the maximum-probability bit array from q by simply assuming bi = 1 if qi ≥ 1/2, and bi = 0 otherwise. However, this calculation may generate invalid bit arrays which do not correspond to actual words according to the mapping between words and bit arrays. For now, we simply assume that w = UNK (unknown) when such bit arrays are obtained, and discuss alternatives later in Section 3.5.\\nThe constraints described here are very general requirements for bit arrays, which still allows us to choose between a wide variety of mapping functions. However, designing the most appropriate mapping method for NMT models is not a trivial problem. In this study, we use a simple mapping method described in Algorithm 1, which was empirically effective in preliminary experiments.1 Here, V is the set of V target words including 3 extra markers: UNK, BOS (begin-of-sentence), and EOS (end-of-sentence), and rank(w) ∈ N>0 is the rank of the word according to their frequencies in the training corpus. Algorithm 1 is one of the minimal mapping methods (i.e., satisfying B = dlog2 V e), and generated bit arrays have the characteristics that their higher bits roughly represents the frequency of corresponding words (e.g., if w is frequently appeared in the training corpus, higher bits in b(w) tend to become 0).\\n 3.2 Loss Functions\\n  For learning correct binary representations, we can use any loss function that satisfies a constraint that:\\nLB(q, b) { = 0, if q = b, ≥ 0, otherwise. (9)\\nFor example, the squared-distance:\\nLB(q, b) := B∑ i=1 (qi − bi)2, (10)\\n1Other methods examined included random codes, Huffman codes (Huffman, 1952) and Brown clustering (Brown et al., 1992) with zero-padding to adjust code lengths, and some original allocation methods based on the word2vec embeddings (Mikolov et al., 2013).\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nor the cross-entropy:\\nLB(q, b) := − B∑ i=1 ( bi log qi + b̄i log q̄i ) , (11)\\nare candidates for the loss function. We also examined both loss functions in the preliminary experiments, and in this paper, we only used the squared-distance function (Equation (10)), because this function achieved higher translation accuracies than Equation (11).\\n 3.3 Efficiency of the Binary Code Prediction\\n  The computational complexity for the parameters Whq and βq is O(HB). This is equal to O(H log V ) when using a minimal mapping method like that shown in Algorithm 1, and is significantly smaller than O(HV ) when using standard softmax prediction. For example, if we chose V = 65536 = 216 and use Algorithm 1’s mapping method, then B = 16 and total amount of computation in the output layer could be suppressed to 1/4096 of its original size.\\nOn a different note, the binary code prediction model proposed in this study shares some ideas with the hierarchical softmax (Morin and Bengio, 2005) approach. Actually, when we used a binarytree based mapping function for b, our model can be interpreted as the hierarchical softmax with two strong constraints for guaranteeing independence between all bits: all nodes in the same level of the hierarchy share their parameters, and all levels of the hierarchy are predicted independently of each other. By these constraints, all bits in b can be calculated in parallel. This is particularly important because it makes the model conducive to being calculated on GPGPUs.\\nHowever, the binary code prediction model also introduces problems of robustness due to these strong constraints. As the experimental results show, the simplest prediction model which directly maps words into bit arrays seriously decreases translation quality. In Sections 3.4 and 3.5, we introduce two additional techniques to prevent reductions of translation quality and improve robustness of the binary code prediction model.\\n 3.4 Hybrid Softmax/Binary Model\\n  According to the Zipf’s law (Zipf, 1949), the distribution of word appearances in an actual corpus is biased to a small subset of the vocabulary. As a result, the proposed model mostly learns\\ncharacteristics for frequent words and cannot obtain enough opportunities to learn for rare words. To alleviate this problem, we introduce a hybrid model using both softmax prediction and binary code prediction as shown in Figure 2(c). In this model, the output layer calculates a standard softmax for the N − 1 most frequent words and an OTHER marker which indicates all rare words. When the softmax layer predicts OTHER, then the binary code layer is used to predict the representation of rare words. In this case, the actual probability of generating a particular word is separated into two equations according to the frequency of words:\\nPr(w|h) := { v′id(w), if id(w) < N,\\nv′N · π(w,h), otherwise, (12)\\nv′ := expu′/ sum expu′, (13) u′ := Whu′h+ βu′ , (14)\\nπ(w,h) := B∏ i=1 ( biqi + b̄iq̄i ) , (15)\\nwhere Whu′ ∈ RN×H and βu′ ∈ RN are trainable parameters, and id(w) assumes that the value corresponds to the rank of frequency of each word. We also define the loss function for the hybrid model using both softmax and binary code losses:\\nL := { lH(id(w)), if id(w) < N, lH(N) + lB, otherwise, (16) lH(i) := λHLH(v ′, i), (17)\\nlB := λBLB(q, b), (18)\\nwhere λH and λB are hyper-parameters to determine strength of both softmax/binary code losses. These also can be adjusted according to the training data, but in this study, we only used λH = λB = 1 for simplicity.\\nThe computational complexity of the hybrid model is O(H(N + log V )), which is larger than the original binary code modelO(H log V ). However,N can be chosen asN V because the softmax prediction is only required for a few frequent words. As a result, we can control the actual computation for the hybrid model to be much smaller than the standard softmax complexity O(HV ),\\nThe idea of separated prediction of frequent words and rare words comes from the differentiated softmax (Chen et al., 2016) approach. However, our output layer can be configured as a fullyconnected network, unlike the differentiated soft-\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nFigure 3: Example of the classification problem using redundant bit array mapping.\\nFigure 4: Training and generation processes with error-correcting code.\\nmax, because the actual size of the output layer is still small after applying the hybrid model.\\n 3.5 Applying Error-correcting Codes\\n  The 2 methods proposed in previous sections impose constraints for all bits in q, and the value of each bit must be estimated correctly for the correct word to be chosen. As a result, these models may generate incorrect words due to even a single bit error. This problem is the result of dense mapping between words and bit arrays, and can be avoided by creating redundancy in the bit array. Figure 3 shows a simple example of how this idea works when discriminating 2 words using 3 bits. In this case, the actual words are obtained by estimating the nearest centroid bit array according to the Hamming distance between each centroid and the predicted bit array. This approach can predict correct words as long as the predicted bit arrays are in the set of neighbors for the correct centroid (gray regions in the Figure 3), i.e., up to a 1-bit error in the predicted bits can be corrected. This ability to be robust to errors is a central idea behind error-correcting codes (Shannon, 1948). In general, an error-correcting code has the ability to correct up to b(d−1)/2c bit errors when all centroids differ d bits from each other (Golay, 1949). d is known as the free distance determined\\nby the design of error-correcting codes. Errorcorrecting codes have been examined in some previous work on multi-class classification tasks, and have reported advantages from the raw classification (Dietterich and Bakiri, 1995; Klautau et al., 2003; Liu, 2006; Kouzani and Nasireding, 2009; Kouzani, 2010; Ferng and Lin, 2011, 2013). In this study, we applied an error-correcting algorithm to the bit array obtained from Algorithm 1 to improve robustness of the output layer in an NMT system. A challenge in this study is trying a large classification (#classes > 10,000) with error-correction, unlike previous studies focused on solving comparatively small tasks (#classes < 100). And this study also tries to solve a generation task unlike previous studies. As shown in the experiments, we found that this approach is highly effective in these tasks.\\nFigure 4 (a) and (b) illustrate the training and generation processes for the model with errorcorrecting codes. In the training, we first convert the original bit arrays b(w) to a center bit array b′ in the space of error-correcting code: b′(b) := [b′1(b), b ′ 2(b), · · · , b′B′(b)] ∈ {0, 1}B ′ , where B′(B) ≥ B is the number of bits in the error-correcting code. The NMT model learns its parameters based on the loss between predicted probabilities q and b′. Note that typical errorcorrecting codes satisfy O(B′/B) = O(1), and this characteristic efficiently suppresses the increase of actual computation cost in the output layer due to the application of the error-correcting code. In the generation of actual words, the decoding method of the error-correcting code converts the redundant predicted bits q into a dense representation q̃ := [q̃1(q), q̃2(q), · · · , q̃B(q)], and uses q̃ as the bits to restore the word, as is done in the method described in the previous sections.\\nIt should be noted that the method for performing error correction directly affects the quality of the whole NMT model. For example, the mapping shown in Figure 3 has only 3 bits and it is clear that these bits represent exactly the same information as each other. In this case, all bits can be estimated using exactly the same parameters, and we can not expect that we will benefit significantly from applying this redundant representation. Therefore, we need to choose an error correction method in which the characteristics of original bits should be distributed in various positions of the resulting bit arrays so that errors in bits are\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nAlgorithm 2 Encoding into a convolutional code. Require: b ∈ {0, 1}B Ensure: b′ ∈ {0, 1}2(B+6) = Redundant bit array\\nx[t] := { bt, if 1 ≤ t ≤ B 0, otherwise y1t := x[t− 6 .. t] · [1001111] mod 2 y2t := x[t− 6 .. t] · [1101101] mod 2 b′ ← [y11 , y21 , y12 , y22 , · · · , y1B+6, y2B+6]\\nAlgorithm 3 Decoding from a convolutional code. Require: q ∈ (0, 1)2(B+6) Ensure: q̃ ∈ {0, 1}B = Restored original bit array g(q, b) := b log q + (1− b) log(1− q)\\nφ0[s | s ∈ {0, 1}6]← {\\n0, if s = [000000] −∞, otherwise\\nfor t = 1→ B + 6 do for scur ∈ {0, 1}6 do\\nsprev(x) := [x] ◦ scur[1 .. 5] o1(x) := ([x] ◦ scur) · [1001111] mod 2 o2(x) := ([x] ◦ scur) · [1101101] mod 2 g′(x) := g(q2t−1, o1(x)) + g(q2t, o2(x)) φ′(x) := φt−1[s\\nprev(x)] + g′(x) x̂← arg maxx∈{0,1} φ′(x) rt[s\\ncur]← sprev(x̂) φt[s\\ncur]← φ′(x̂) end for\\nend for s′ ← [000000] for t = B → 1 do\\ns′ ← rt+6[s′] q̃t ← s′1\\nend for q̃ ← [q̃1, q̃2, · · · , q̃B ]\\nnot highly correlated with each-other. In addition, it is desirable that the decoding method of the applied error-correcting code can directly utilize the probabilities of each bit, because q generated by the network will be a continuous probabilities between zero and one.\\nIn this study, we applied convolutional codes (Viterbi, 1967) to convert between original and redundant bits. Convolutional codes perform a set of bit-wise convolutions between original bits and weight bits (which are hyper-parameters). They are well-suited to our setting here because they distribute the information of original bits in different places in the resulting bits, work robustly for random bit errors, and can be decoded using bit probabilities directly.\\nAlgorithm 2 describes the particular convolutional code that we applied in this study, with two convolution weights [1001111] and [1101101] as fixed hyper-parameters.2 Where x[i .. j] :=\\n2We also examined many configurations of convolutional codes which have different robustness and computation costs, and finally chose this one.\\n[xi, · · · , xj ] and x · y := ∑\\ni xiyi. On the other hand, there are various algorithms to decode convolutional codes with the same format which are based on different criteria. In this study, we use the decoding method described in Algorithm 3, where x ◦ y represents the concatenation of vectors x and y. This method is based on the Viterbi algorithm (Viterbi, 1967) and estimates original bits by directly using probability of redundant bits. Although Algorithm 3 looks complicated, this algorithm can be performed efficiently on CPUs at test time, and is not necessary at training time when we are simply performing calculation of Equation (6). Algorithm 2 increases the number of bits from B intoB′ = 2(B+6), but does not restrict the actual value of B.\\n 4 Experiments\\n  \\n 4.1 Experimental Settings\\n  We examined the performance of the proposed methods on two English-Japanese bidirectional translation tasks which have different translation difficulties: ASPEC (Nakazawa et al., 2016) and BTEC (Takezawa, 1999). Table 1 describes details of two corpora. To prepare inputs for training, we used tokenizer.perl in Moses (Koehn et al., 2007) and KyTea (Neubig et al., 2011) for English/Japanese tokenizations respectively, applied lowercase.perl from Moses, and replaced out-of-vocabulary words such that rank(w) > V − 3 into the UNK marker.\\nWe implemented each NMT model using C++ in the DyNet framework (Neubig et al., 2017) and trained/tested on 1 GPU (GeForce GTX TITAN X). Each test is also performed on CPUs to compare its processing time. We used the concat local attention model proposed in Luong et al. (2015) constructed using a 1-layer LSTM (input/forget/output gates and non-peepholes) (Gers et al., 2000) encoder/decoder with 30% dropout (Srivastava et al., 2014) for the input/output vectors of the LSTMs. Only output layers and loss functions are replaced, and other network architectures are identical for the conventional/proposed models. We used the Adam optimizer (Kingma and Ba, 2014) with fixed hyper-parameters α = 0.001, β1 = 0.9β2 = 0.999, ε = 10\\n−8, and minibatches with 64 sentences sorted according to their sequence lengths. For evaluating the quality of each model, we calculated BLEU (Papineni et al., 2002) every 1000 mini-batches. Table 2 lists all\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nTable 1: Details of the corpus.\\nName ASPEC BTEC\\nLanguages En↔ Ja\\n#sentences Train 2.00 M 465. k Dev 1,790 510 Test 1,812 508 Vocabulary size V 65536 25000\\nTable 2: Evaluated models.\\nName Summary Softmax Softmax prediction (Figure 2(a)) Binary Figure 2(b) w/ raw bit array Hybrid-N Figure 2(c) w/ softmax size N Binary-EC Binary w/ error-correction Hybrid-N-EC Hybrid-N w/ error-correction\\nmodels we examined in experiments.\\n 4.2 Results and Discussion\\n  Table 3 shows the BLEU on the test set, number of bits B (or B′) for the binary code, actual size of the output layer #out, number of parameters in the output layer #W,β, as well as the ratio of #W,β or amount of whole parameters compared with Softmax, and averaged processing time at training (per mini-batch on GPUs) and test (per sentence on GPUs/CPUs), respectively. Figure 5(a) and 5(b) shows training curves up to 180,000 epochs about some English→Japanese settings. To relax instabilities of translation qualities while training (as shown in Figure 5(a) and 5(b)), each BLEU in Table 3 is calculated by averaging actual test BLEU of 5 consecutive results around the epoch that has the highest dev BLEU.\\nFirst, we can see that each proposed method largely suppresses the actual size of the output layer from ten to one thousand times compared with the standard softmax. By looking at the total number of parameters, we can see that the proposed models require only 70% of the actual memory, and the proposed model reduces the total number of parameters for the output layers to a practically negligible level. Note that most of remaining parameters are used for the embedding lookup at the input layer in both encoder/decoder. These still occupy O(EV ) memory, where E represents the size of each embedding layer and usually O(E/H) = O(1). These are not targets to be reduced in this study because these values rarely are accessed at test time because we only need to access them for input words, and do not need them to always be in the physical memory. It might be\\npossible to apply a similar binary representation as that of output layers to the input layers as well, then express the word embedding by multiplying this binary vector by a word embedding matrix. This is one potential avenue of future work.\\nTaking a look at the BLEU for the simple Binary method, we can see that it is far lower than other models for all tasks. This is expected, as described in Section 3, because using raw bit arrays causes many one-off estimation errors at the output layer due to the lack of robustness of the output representation. In contrast, Hybrid-N and Binary-EC models clearly improve BLEU from Binary, and they approach that of Softmax. This demonstrates that these two methods effectively improve the robustness of binary code prediction models. Especially, Binary-EC generally achieves higher quality than Hybrid-512 despite the fact that it suppress the number of parameters by about 1/10. These results show that introducing redundancy to target bit arrays is more effective than incremental prediction. In addition, the Hybrid-NEC model achieves the highest BLEU in all proposed methods, and in particular, comparative or higher BLEU than Softmax in BTEC. This behav-\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nTable 3: Comparison of BLEU, size of output layers, number of parameters and processing time.\\nCorpus Method BLEU % B #out #W,β\\nRatio of #params Time (En→Ja) [ms] EnJa JaEn #W,β All Train Test: GPU / CPU\\nASPEC Softmax 31.13 21.14 — 65536 33.6 M 1/1 1 1026. 121.6 / 2539. Binary 13.78 6.953 16 16 8.21 k 1/4.10 k 0.698 711.2 73.08 / 122.3 Hybrid-512 22.81 13.95 16 528 271. k 1/124. 0.700 843.6 81.28 / 127.5 Hybrid-2048 27.73 16.92 16 2064 1.06 M 1/31.8 0.707 837.1 82.28 / 159.3 Binary-EC 25.95 18.02 44 44 22.6 k 1/1.49 k 0.698 712.0 78.75 / 164.0 Hybrid-512-EC 29.07 18.66 44 556 285. k 1/118. 0.700 850.3 80.30 / 180.2 Hybrid-2048-EC 30.05 19.66 44 2092 1.07 M 1/31.4 0.707 851.6 77.83 / 201.3\\nBTEC Softmax 47.72 45.22 — 25000 12.8 M 1/1 1 325.0 34.35 / 323.3 Binary 31.83 31.90 15 15 7.70 k 1/1.67 k 0.738 250.7 27.98 / 54.62 Hybrid-512 44.23 43.50 15 527 270. k 1/47.4 0.743 300.7 28.83 / 66.13 Hybrid-2048 46.13 45.76 15 2063 1.06 M 1/12.1 0.759 307.7 28.25 / 67.40 Binary-EC 44.48 41.21 42 42 21.5 k 1/595. 0.738 255.6 28.02 / 69.76 Hybrid-512-EC 47.20 46.52 42 554 284. k 1/45.1 0.744 307.8 28.44 / 56.98 Hybrid-2048-EC 48.17 46.58 42 2090 1.07 M 1/12.0 0.760 311.0 28.47 / 69.44\\nFigure 6: BLEU changes in the Hybrid-N methods according to the softmax size (En→Ja).\\nior clearly demonstrates that these two methods are orthogonal, and combining them together can be effective. We hypothesize that the lower quality of Softmax in BTEC is caused by an over-fitting due to the large number of parameters required in the softmax prediction.\\nProposed methods also improve actual computation time in both training and test. In particular, proposed methods can be performed significantly faster than Softmax at testing on CPUs by x5 to x20, which are directly affected by the complexity of output layers. In addition, we can also see that applying error-correcting code is also efficient at the point of the decoding speed.\\nFigure 6 shows the trade-off between the translation quality and the size of softmax layers in the hybrid prediction model (Figure 2(c)) without error-correction. According to the model definition in Section 3.4, the softmax prediction and\\nraw binary code prediction can be assumed to be the upper/lower-bound of the hybrid prediction model. The curves in Figure 6 move between Softmax and Binary models, and this behavior intuitively explains the characteristics of the hybrid prediction. In addition, we can see that the BLEU score in BTEC quickly improves, and saturates at N = 1024 in contrast to the ASPEC model, which is still improving at N = 2048. We presume that the shape of curves in Figure 6 is also affected by the difficulty of the corpus, i.e., when we train the hybrid model for easy datasets (e.g., BTEC is easier than ASPEC), it is enough to use a small softmax layer (e.g. N ≤ 1024).\\n 5 Conclusion\\n  In this study, we proposed neural machine translation models which indirectly predict output words via binary codes, and two model improvements: a hybrid prediction model using both softmax and binary codes, and introducing error-correcting codes to introduce robustness of binary code prediction. Experiments show that the proposed model can achieve comparative translation qualities to standard softmax prediction, while significantly suppressing the amount of parameters in the output layer, and improving calculation speeds while training and especially testing.\\nOne interesting avenue of future work is to automatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here. In Algorithms 2 and 3 we use convolutions that were determined heuristically, and it is likely that learning these along with the model could result in improved accuracy or better compression capability.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899\\n Abstract In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word, and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments show the proposed model achieves translation accuracies that approach the softmax, while reducing memory usage on the order of 1/10 to 1/1000, and also improving decoding speed on CPUs by x5 to x20.   \n",
            "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Using Ontology-Grounded Token Embeddings To Predict Prepositional Phrase Attachments  1 000\\n011\\n012\\n013\\n014\\n015\\n016\\n017\\n018\\n019\\n020\\n021\\n022\\n023\\n024\\n025\\n026\\n027\\n028\\n029\\n030\\n031\\n032\\n033\\n034\\n035\\n036\\n037\\n038\\n039\\n040\\n041\\n042\\n043\\n044\\n045\\n046\\n047\\n048\\n049\\n061\\n062\\n063\\n064\\n065\\n066\\n067\\n068\\n069\\n070\\n071\\n072\\n073\\n074\\n075\\n076\\n077\\n078\\n079\\n080\\n081\\n082\\n083\\n084\\n085\\n086\\n087\\n088\\n089\\n090\\n091\\n092\\n093\\n094\\n095\\n096\\n097\\n098\\n099\\n 1 Introduction\\n  Type-level word embeddings map a word type (i.e., a surface form) to a dense vector of real numbers such that similar word types have similar embeddings. When pre-trained on a large corpus of unlabeled text, they provide an effective mechanism for generalizing statistical models to words which do not appear in the labeled training data for a downstream task.\\nIn this paper, we make the following distinction between types and tokens: By word types, we mean the surface form of the word, whereas by tokens we mean the instantiation of the surface form in a context. For example, the same word type ‘pool’ occurs as two different tokens in the sentences “He sat by the pool,” and “He played a game of pool.”\\nMost word embedding models define a single vector for each word type. However, a fundamen-\\ntal flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word ‘pool’ has different meanings, but the same representation is typically used for both of them. Similarly, the fact that ‘pool’ and ‘lake’ are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014; Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space.\\nPrevious work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Schütze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings.\\nIn this paper, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.e., weighted sum) of the concept embeddings as the token representation (see §2). In addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings.\\nOur approach to context-sensitive embeddings assumes the availability of a lexical ontology such\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nFigure 1: An example grounding for the word ‘pool’. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word ‘pool’ regardless of its context. Other WordNet senses for ‘pool’ were removed from the figure for simplicity.\\nas WordNet, but it does not require a tagger for word senses. We use the proposed embeddings to predict prepositional phrase (PP) attachments (see §3), a challenging problem which emphasizes the selectional preferences between words in the PP and each of the candidate head words. Our empirical results and detailed analysis (see §4) show that the proposed embeddings effectively use WordNet to improve the accuracy of PP attachment predictions.\\n 2 WordNet-Grounded Context-Sensitive Token Embeddings\\n  In this section, we focus on defining our contextsensitive token embeddings. We first describe our grounding of word types using WordNet concepts. Then, we describe our model of contextsensitive token-level embeddings as a weighted sum of WordNet concept embeddings.\\n 2.1 WordNet Grounding\\n  We use WordNet to map each word type to a set of synsets, including possible generalizations or abstractions. Among the labeled relations defined in WordNet between different synsets, we focus on the hypernymy relation to help model generaliza-\\ntion and selectional preferences between words, which is especially important for predicting PP attachments (Resnik, 1993). To ground a word type, we identify the the set of (direct and indirect) hypernyms of the WordNet senses of that word. A simplified grounding of the word ‘pool’ is illustrated in Figure 1. This grounding is key to our model of token embeddings, to be described in the following subsections.\\n 2.2 Context-Sensitive Token Embeddings\\n  Our goal is to define a context-sensitive model of token embeddings which can be used as a dropin replacement for traditional type-level word embeddings.\\nNotation. Let Senses(w) be the list of synsets defined as possible word senses of a given word type w in WordNet, and Hypernyms(s) be the list of hypernyms for a synset s.1 For example, according to Figure 1:\\nSenses(pool) = [pond.n.01, pool.n.09], and\\nHypernyms(pond.n.01) = [pond.n.01, lake.n.01,\\nbody of water.n.01, entity.n.01] (1)\\nEach WordNet synset s is associated with a set of parameters vs ∈ Rn which represent its embedding. This parameterization is similar to that of Rothe and Schütze (2015).\\nEmbedding model. Given a sequence of tokens t and their corresponding word types w, let ui ∈ Rn be the embedding of the word token ti at position i. Unlike most embedding models, the token embeddings ui are not parameters. Rather, ui is computed as the expected value of concept embeddings used to ground the word type wi corresponding to the token ti: ui = ∑\\ns∈Senses(wi) ∑ s′∈Hypernyms(s) p(s, s′ | t,w, i) vs′\\n(2)\\nsuch that∑ s∈Senses(wi) ∑ s′∈Hypernyms(s) p(s′, s | t,w, i) = 1\\n(3)\\n1For notational convenience, we assume that s ∈ Hypernyms(s).\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\nFigure 2: Steps for computing the contextsensitive token embedding for the word ‘pool’, as described in §2.2.\\nThe distribution which governs the expectation over synset embeddings factorizes into two components:\\np(s, s′ | t,w, i) ∝λwi exp−λwi rank(s,wi)× MLP([vs′ ; context(i, t)]) (4)\\nThe first component, λwi exp −λwi rank(s,wi), is a sense prior which reflects the prominence of each word sense for a given word type. Note that this is defined for each word type (wi), and is shared across all tokens which have the same word type. The parameterization of the sense prior is similar to an exponential distribution since WordNet senses are organized in descending order of their frequency. The scalar parameter (λwi) controls the decay of the probability mass.\\nThe second component, MLP([vs′ ; context(i, t)]) is what makes the token representations context-sensitive. It scores each concept in the WordNet grounding of wi by feeding the concatenation of the concept embedding and a dense vector that summarizes the textual context into a multilayer perceptron (MLP) with two tanh layers followed by a softmax layer. This component is inspired by the soft attention often used in neural machine translation (Bahdanau et al., 2014).2 The definition of the\\n2Although soft attention mechanism is typically used to\\ncontext function is dependent on the encoder used to encode the context. We describe a specific instantiation of this function in §3.\\nTo summarize, Figure 2 illustrates how to compute the embedding of a word token ti = ‘pool’ in a given context:\\n1. compute a summary of the context context(i, t),\\n2. enumerate related concepts for ti,\\n3. compute p(s, s′ | t,w, i) for each pair (s, s′), and\\n4. compute ui = E[vs′ ].\\nIn the following section, we describe our model for predicting PP attachments, including our definition for context.\\n 3 PP Attachment\\n  Disambiguating PP attachments is an important and challenging NLP problem, and is a good fit for evaluating our WordNet-grounded contextsensitive embeddings since modeling hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993).\\nFigure 3, reproduced from Belinkov et al. (2014), illustrates an example of the PP attachment prediction problem. The accuracy of a\\nexplicitly represent the importance of each item in a sequence, it can also be applied to non-sequential items.\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\ncompetitive English dependency parser at predicting the head word of an ambiguous prepositional phrase is 88.5%, significantly lower than the overall unlabeled attachment accuracy of the same parser (94.2%).3\\nThis section formally defines the problem of PP attachment disambiguation, describes our baseline model, then shows how to integrate the token-level embeddings in the model.\\n 3.1 Problem definition\\n  We follow Belinkov et al. (2014)’s definition of the PP attachment problem. Given a preposition p and its direct dependent d in the prepositional phrase (PP), our goal is to predict the correct head word for the PP among an ordered list of candidate head words h. Each example in the train, validation, and test sets consists of an input tuple 〈h, p, d〉 and an output index k to identify the correct head among the candidates in h.\\n 3.2 Model definition\\n  Both our proposed and baseline models for PP attachment use bidirectional RNN with LSTM cells (bi-LSTM) to encode the sequence t = 〈h1, . . . , hK , p, d〉.\\nWe score each candidate head by feeding the concatenation of the output bi-LSTM vectors for the head hk, the preposition p and the direct dependent d through an MLP, with a fully connected tanh layer to obtain a non-linear projection of the concatenation, followed by a fully-connected softmax layer:\\np(hkis head) = MLPattach([lstm out(hk);\\nlstm out(p);\\nlstm out(d)]) (5)\\nTo train the model, we use cross-entropy loss at the output layer for each candidate head in the training set. At test time, we predict the candidate head with the highest probability according to the model in Eq. 5, i.e.,\\nk̂ = argmax k p(hkis head = 1). (6)\\nThis model is inspired by the Head-Prep-ChildTernary model of Belinkov et al. (2014). The main difference is that we replace the input features for each token with the output bi-RNN vectors.\\n3See Table 2 in §4 for detailed results.\\nWe now describe the difference between the proposed model and the baseline. Generally, let lstm in(ti) and lstm out(ti) represent the input and output vectors of the bi-LSTM for each token ti ∈ t in the sequence.\\nBaseline model. In the baseline model, we use type-level word embeddings to represent the input vector lstm in(ti) for a token ti in the sequence. The word embedding parameters are initialized with pre-trained vectors, then tuned along with the parameters of the bi-LSTM and MLPattach. We call this model LSTM-PP.\\nProposed model. In the proposed model, we use token level word embedding as described in §2 as the input to the bi-LSTM, i.e., lstm in(ti) = ui. The context used for the attention component is simply the hidden state from the previous timestep. However, since we use a bi-LSTM, the model essentially has two RNNs, and accordingly we have two context vectors, and associated attentions. That is, contextf (i, t) = lstm in(ti−1) for the forward RNN and contextb(i, t) = lstm in(ti+1) for the backward RNN. The synset embedding parameters are initialized with pretrained vectors and tuned along with the sense decay (λw) and MLP parameters from Eq. 4, the parameters of the bi-LSTM and those of MLPattach. We call this model OntoLSTM-PP.\\n 4 Experiments\\n  Dataset and evaluation. We used the English PP attachment dataset created and made available by Belinkov et al. (2014). The training and test splits contain 33,359 and 1951 labeled examples respectively. As explained in §3.1, the input for each example is 1) an ordered list of candidate head words, 2) the preposition, and 3) the direct dependent of the preposition. The head words are either nouns or verbs and the dependent is always a noun. All examples in this dataset have at least two candidate head words. As discussed in Belinkov et al. (2014), this dataset is a more realistic PP attachment task than the RRR dataset (Ratnaparkhi et al., 1994). The RRR dataset is a binary classification task with exactly two head word candidates in all examples. The context for each example in the RRR dataset is also limited which defeats the purpose of our context-sensitive embeddings.\\nModel specifications and hyperparameters. For efficient implementation, we use mini-batch\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nupdates with the same number of senses and hypernyms for all examples, padding zeros and truncating senses and hypernyms as needed. For each word type, we use a maximum of S senses and H indirect hypernyms from WordNet. In our initial experiments on a held-out development set (10% of the training data), we found that values greater than S = 3 and H = 5 did not improve performance. We also used the development set to tune the number of layers in MLPattach separately for the OntoLSTM-PP and LSTM-PP, and the number of layers in the attention MLP in OntoLSTM-PP. When a synset has multiple hypernym paths, we use the shortest one. Finally, words types which do not appear in WordNet are assumed to have one unique sense per word type with no hypernyms. Since the POS tag for each word is included in the dataset, we exclude WordNet synsets which are incompatible with the POS tag. The synset embedding parameters are initilized using the synset vectors obtained by running AutoExtend (Rothe and Schütze, 2015) on 100-dimensional GloVe (Pennington et al., 2014) vectors for WordNet 3.1. Representation for the OOV word types in LSTMPP and OOV synset types in OntoLSTM-PP were randomly drawn from a uniform 100-d distribution. Initial sense prior parameters (λw) were also drawn from a uniform 1-d distribution.\\nBaselines. In our experiments, we compare our proposed model, OntoLSTM-PP with three baselines – LSTM-PP initialized with GloVe embedding, LSTM-PP initialized with GloVe vectors retrofitted to WordNet using the approach of Faruqui et al. (2015), and finally the best performing standalone PP attachment system from Belinkov et al. (2014), referred to as “HPCD (full)” in the paper. “HPCD (full)” is a neural network model that learns to compose the vector representations of each of the candidate heads with those of the preposition and the dependent, and predict attachments. The input representations are enriched using syntactic context information, POS, WordNet and VerbNet (Kipper et al., 2008) information and the distance of the head word from the PP is explicitly encoded in composition architecture. In contrast, we do not use syntactic context, VerbNet and distance information, and do not explicitly encode POS information.\\n 4.1 PP Attachment Results\\n  Table 1 shows that our proposed token level embedding scheme “OntoLSTM-PP” outperforms the better variant of our baseline “LSTM-PP” (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%. “OntoLSTM-PP’ also outperforms “HPCD (full)”, the previous best result on this dataset.\\nInitializing the word embeddings with GloVeretro (which uses WordNet as described in Faruqui et al. (2015)) instead of “textitGloVe” amounts to a small improvement, compared to the improvements obtained using “OntoLSTM-PP”. This result illustrates that our approach of dynamically choosing a context sensitive distribution over synsets is a more effective way of making use of WordNet.\\nEffect on dependency parsing. Following Belinkov et al. (2014), we used RBG parser (Lei et al., 2014), and modified it by adding a binary feature indicating the PP attachment predictions from our model.\\nWe compare four ways to compute the additional binary features: 1) the predictions of the best standalone system “HPCD (full)” in Belinkov et al. (2014), 2) the predictions of our baseline model “LSTM-PP”, 3) the predictions of our improved model “OntoLSTM-PP”, and 4) the gold labels “Oracle PP”.\\nTable 2 shows the effect of using the PP attachment predictions as features within a dependency parser. We note there is a relatively small difference in unlabeled attachment accuracy for all dependencies (not only PP attachments), even when gold PP attachments are used as additional features to the parser. However, when gold PP attachment are used, we note a large potential improvement of 10.46 points (between the PPA accuracy for “RBG” and “RBG + Oracle PP”), which confirms that adding PP predictions as features is\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nSystem Full UAS PPA Acc. RBG 94.17 88.51 RBG + HPCD (full) 94.19 89.59 RBG + LSTM-PP 94.14 86.35 RBG + OntoLSTM-PP 94.30 90.11 RBG + Oracle PP 94.60 98.97\\nTable 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.\\nan effective approach. Our proposed model “RBG + OntoLSTM-PP” recovers 15% of this potential improvement, while “RBG + HPCD (full)” recovers 10%, which illustrates that PP attachment remains a difficult problem with plenty of room for improvements even when using a dedicated model to predict PP attachments and using its predictions in a dependency parser.\\nWe also note that, although we use the same predictions of the “HPCD (full)” model in Belinkov et al. (2014),4 we report different results than Belinkov et al. (2014). For example, the unlabeled attachment score (UAS) of the baselines “RBG” and “RBG + HPCD (full)” are 94.17 and 94.19, respectively, in Table 2, compared to 93.96 and 94.05, respectively, in Belinkov et al. (2014). This is due to the use of different versions of the RBG parser.5\\n 4.2 Analysis\\n  In this subsection, we analyze different aspects of our model in order to develop a better understanding of its behavior.\\nEffect of context sensitivity and sense priors. We now show some results that indicate the relative strengths of two components of our contextsensitive token embedding model. The second row in Table 3 shows the test accuracy of a system trained without sense priors (that is, making p(s|wi) from Eq. 2 a uniform distribution), and the third row shows the effect of making the token representations context-insensitive by giving a similar attention score to all related concepts, essentially making them type level representations,\\n4The authors kindly provided their predictions for 1942 test examples (out of 1951 examples in the full test set). In Table 2, we use the same subset of 1942 test examples and will include a link to the subset in the final draft.\\n5We use the latest commit (SHA: e07f74dd2ba47348fd548935155ded38eea20809) on the GitHub repository of the RGB parser.\\nbut still grounded in WordNet. As it can be seen, removing context sensitivity has an adverse effect on the results. This illustrates the importance of the sense priors and the attention mechanism.\\nIt is interesting that, even without sense priors and attention, the results with WordNet grounding is still higher than that of the two LSTM-PP systems in Table 1. This result illustrates the regularization behavior of sharing concept embeddings across multiple words, which is especially important for rare words.\\nEffect of training data size. Since “OntoLSTM-PP” uses external information, the gap between the model and “LSTM-PP” is expected to be more pronounced when the training data sizes are smaller. To test this hypothesis, we trained the two models with different amounts of training data and measured their accuracies on the test set. The plot is shown in Figure 4. As expected, the gap tends to be larger at lower data sizes. Surprisingly, even with 2000 sentences in the training data set, OntoLSTM-PP outperforms LSTM-PP trained with the full data set. When both the models are trained with the fill dataset, LSTM-PP reaches a training accuracy of 95.3%, whereas OntoLSTM-PP reaches 93.5%. The fact that LSTM-PP is overfitting the training data more, indicates the regularization capability of OntoLSTM-PP.\\nQualitative analysis. To better understand the effect of WordNet grounding, we took a sample of sentences from the test set whose PP attachments were correctly predicted by OntoLSTM-PP but not by LSTM-PP. A common pattern observed was\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nFigure 5: Two examples from the test set where OntoLSTM-PP predicts the head correctly and LSTM-PP does not, along with weights by OntoLSTM-PP to synsets that contribute to token representations of infrequent word types. The prepositions are shown in bold, LSTM-PP’s predictions in red and OntoLSTMPP’s predictions in green. Words that are not candidate heads or dependents are shown in brackets.\\nModel PPA Acc. full 89.7 - sense priors 88.4 - attention 87.5\\nTable 3: Effect of removing sense priors and context sensitivity (attention) from the model\\nthat those sentences contained words not seen frequently in the training data. Figure 5 shows two such cases. In both cases, the weights assigned by OntoLSTM-PP to infrequent words are also shown. The word types soapsuds and buoyancy do not occur in the training data, but OntoLSTMPP was able to leverage the parameters learned for the synsets that contributed to their token representations. Another important observation is that the word type buoyancy has four senses in WordNet (we consider the first three), none of which is the metaphorical sense that is applicable to markets as shown in the example here. Selecting a combination of relevant hypernyms from various senses may have helped OntoLSTM-PP make the right prediction. This shows the value of using hypernymy information from WordNet. Moreover, this indicates the strength of the hybrid nature of the model, that lets it augment ontological information with distributional information.\\nParameter space We note that the vocabulary sizes in OntoLSTM-PP and LSTM-PP are comparable as the synset types are shared across word types. In our experiments with the full PP attachment dataset, we learned embeddings for 18k synset types with OntoLSTM-PP and 11k word types with LSTM-PP. Since the biggest contribution to the parameter space comes from the embedding layer, the complexities of both the models are comparable.\\nImplementation and code availability. The models are implemented using Keras (Chollet, 2015), and the functionality is available in the form of Keras layers to make it easier for other researchers to use the proposed embeddingn model.\\nFuture work. This approach may be extended to other NLP tasks that can benefit from using encoders that can access WordNet information. WordNet also has some drawbacks, and may not always have sufficient coverage given the task at hand. As we have shown in §4.2, our model can deal with missing WordNet information by augmenting it with distributional information. Moreover, the methods described in this paper can be extended to other kinds of structured knowledge sources like Freebase which may be more suitable for tasks like question answering.\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\n 5 Related Work\\n  This work is related to various lines of research within the NLP community: dealing with synonymy and homonymy in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distributional and knowledge based semantics; and selectional preferences and their relation with syntactic and semantic relations.\\nThe need for going beyond a single vector per word-type has been well established for a while, and many efforts were focused on building multi-prototype vector space models of meaning (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Jauhar et al., 2015; Neelakantan et al., 2015, etc.). However, the target of all these approaches is obtaining multi-sense word vector spaces, either by incorporating sense tagged information or other kinds of external context. The number of vectors learned is still fixed, based on the preset number of senses. In contrast, our focus is on learning a context dependent distribution over those concept representations. Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task.\\nRelated to the idea of concept embeddings is Rothe and Schütze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings. In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept embeddings.\\nThere is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology.\\nFaruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest.\\nResnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al. (2013) applied models of selectional preferences automatically learned from WordNet and distributional information, to the problem of semantic role labeling. Resnik (1993); Brill and Resnik (1994); Agirre (2008) and others have used WordNet information towards improving prepositional phrase attachment predictions.\\n 6 Conclusion\\n  In this paper, we proposed a grounding of lexical items which acknowledges the semantic ambiguity of word types using WordNet and a method to learn a context-sensitive distribution over their representations. We also showed how to integrate the proposed representation with recurrent neural networks for disambiguating prepositional phrase attachments, showing that the proposed WordNetgrounded context-sensitive token embeddings outperforms standard type-level embeddings for predicting PP attachments. We provided a detailed qualitative and quantitative analysis of the proposed model.\\n Abstract Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Abstract  \\\n",
            "0                                                                                                                                                   We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machinegenerated poem to be the best amongst all evaluated.   \n",
            "1                            We propose a new, simple, yet effective framework, phrasal recurrent neural networks (pRNN), for language modeling and machine translation. Different from previous RNN-based language models, pRNNs store the sentential history as a set of candidate phrases with different lengths that precede the word to predict. To represent phrases as fix-length realvalued vectors, we build the RNN pyramid, which is composed of shifted parallel RNN sequences. When predicting the next word, pRNNs employ a soft attention mechanism to selective and combine the suggestions of candidate phrases. We test our model on language model and machine translation tasks. Our model leads to an improvement of over 10 points in perplexity both on standard Penn Treebank and FBIS English data set over a state-of-the-art LSTM language modeling baseline. We also apply pRNNs to encode the source sentence of machine translation besides a conventional bi-direction encoder, which improves over the Moses (phrase-based statistical model) and a strong sequence-to-sequence baseline in the Chinese-English machine translation task.   \n",
            "2                                                                                                                                                                                                                                                                                                                                                           Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks.   \n",
            "3    We compare three recent models of referential word meaning that link visual object representations to lexical representations in a distributional vector space, either directly through cross-modal mapping or indirectly through visual predictors for individual words. We use these models to predict object names as they could be used in naturalistic referring expressions. We find that cross-modal mapping generally produces semantically appropriate and mutually highly similar object names in its topn list, but sometimes fails to make desired distinctions. Visual word predictors, on the other hand, can react to more subtle visual distinctions and select specific terms, but sometimes stray taxonomically very far from the correct one. Combination of the approaches improves over the individual predictions in a standard naming task. All approaches can be extended to the zero-shot naming case, where the correct name is one for which no instances were seen during training; again they show complementary strengths and weaknesses, depending on the setup and the lexical relation of the unattested object name to known ones.   \n",
            "4                                                                                                                                                                  We study response selection for multi-turn conversation in retrieval based chatbots. Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among the utterances or important information in the context. We propose a sequential matching network (SMN) to address both problems. SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among the utterances. The final matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.   \n",
            "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...   \n",
            "132                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a lowresource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.   \n",
            "133                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Multimodal sentiment analysis is a developing area of research, which involves identification of emotions and sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the inter-dependencies and relations among utterances of a video. In this paper, we propose an LSTM based model which enables these utterances to capture contextual information from its surroundings in the same video, thus aiding the classification process. Our model shows 5 − 10% improvement over the state of the art and high robustness to generalizability.   \n",
            "134                                                                                                  Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divide the to-besummarized content into multiple text chunks, then rank and select the most meaningful ones. These approaches can neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also generates absent keyphrases based on the semantic meaning of the text.   \n",
            "135                                                                                                                                                                                                                                                                                                                                                                                                                                                              In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word, and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments show the proposed model achieves translation accuracies that approach the softmax, while reducing memory usage on the order of 1/10 to 1/1000, and also improving decoding speed on CPUs by x5 to x20.   \n",
            "136                                                                                                                                                                                                                                                                                                                                                                                                                                   Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Introducción  \\\n",
            "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Poetry is an advanced form of linguistic communication, in which a message is conveyed that satisfies both aesthetic and semantic constraints. As poetry is one of the most expressive forms of language, the automatic creation of texts recognisable as poetry is difficult. In addition to requiring an understanding of many aspects of language including phonetic patterns such as rhyme, rhythm and alliteration, poetry composition also requires a deep understanding of the meaning of language.\\nPoetry generation can be divided into two subtasks, namely the problem of content, which is concerned with a poem’s semantics, and the problem of form, which is concerned with the aesthetic rules that a poem follows. These rules may describe aspects of the literary devices used, and are usually highly prescriptive. Examples of different forms of poetry are limericks, ballads and sonnets. Limericks, for example, are characterised by their strict rhyme scheme (AABBA), their rhythm (two unstressed syllables followed by one stressed syllable) and their shorter third and fourth lines. Creating such poetry requires not only an understanding of the language itself, but also of how it sounds when spoken aloud. Statistical text generation usually requires the construction of a generative language model that explicitly learns the probability of any given word given previous context. Neural language models (Schwenk and Gauvain, 2005; Bengio et al., 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014; Kim et al., 2015). Poetry generation is an interesting application, since performing this task automatically requires the creation of models that not only focus on what is being written (content), but also on how it is being written (form). We experiment with two novel methodologies for solving this task. The first involves training a model to learn an implicit representation of content and form through the use of a phonological encoding. The second involves training a generative language model to represent content, which is then constrained by a discriminative pronunciation model, representing form. This second model is of particular interest because poetry with arbitrary rhyme, rhythm, repetition and themes can be generated by tuning the pronunciation model.\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        There are latent nest structures beyond sequential surface words in natural language (Chomsky, 1957). In the last two decades, researchers incorporated more and more rich structural information into conventional language model (Jelinek and Lafferty, 1991; Chelba, 1997; Chelba and Jelinek, 2000; Emami and Jelinek, 2005) and\\nmore recently on neural network-based language modeling (Dyer et al., 2016). Among the effort, one direction is to explore sub-word structures(Costa-Jussà and Fonollosa, 2016; Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015), such as characters, mostly to handle out-of-vocabulary word problem. A somewhat opposite direction is to explore hyper-word structure. For example, (Eriguchi et al., 2016) adopts parsing tree in encoder phase in machine translation, (Stahlberg et al., 2016) proposes to use hierarchical phrase-based (HPB) model to guide the search in decoding. Both models, however, rely heavily on human labeled data on the language structures, which is extremely expensive and limited in scale.\\nIn this paper, we propose phrasal recurrent neural networks (pRNNs; §2), a general framework of RNNs (Elman, 1990) that explicitly models task-specific nested phrases from plain text. Here we use “phrase” as its definition in phrase-based statistical machine translation (PBSMT(Zens et al., 2002; Koehn et al., 2003)), which indicates any continues sequences of words. What different here are pRNNs permit phrases with arbitrary lengths instead of limiting them for the computational issue. The phrases in pRNNs are composed and selected in a way that is jointly learned in the language modeling, therefore requiring no human-labeled data or external model such as word alignment. In previous RNN-based language modeling, the hidden state of RNN before the word to predict summarizes the history of all previous words. Similarly, in pRNNs, we use the all state of all parallel RNNs (with the same parameters) to capture the history of all subsequence of words that precede the word to predict, with the starting word shifting from the first word the one right before the word to predict.\\nThis set of RNNs applied parallelly to different choices of word sequences are called RNN pyra-\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nmid. While most of those RNNs’ status deal with incorrect word sequences: they could either start in the middle of a chunk, or in a place too early or too late for the prediction tasks, we left it to an attention mechanism to select and combine, therefore eliminate the need for external knowledge on chunking and composition. This mechanism will be trained jointly with the composition models in pRNNs in optimizing a designed objective function, e.g, perplexity or likelihood. With proper composition function in pRNNs, the RNN pyramid provides a “phrase forest”, which could potentially contain a fairly deep nested structure in some of its members.\\nOur pRNN models have two merits:\\n• They represent all phrases in the same vector space in an explicit and unsupervised way. Which shows the potential to discover and utilize hidden structures of surface word sequences.\\n• They explore the possibility of network construction in another dimension: Parallel. Instead of stacking deeper and deeper layers of RNNs.\\nExperiments show that pRNNs are effective for language modeling (§4). Our model obtains significant better perplexities than state-of-the-art sequential Long-Short Term (LSTM) model on language modeling task, both on PTB and FBIS English data set. We also apply pRNNs to encode the source sentence of machine translation besides a conventional bi-direction encoder, which improves over the Moses (phrase-based statistical model) and a strong sequence-to-sequence baseline in the Chinese-English machine translation task.   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Multi-task learning is an effective approach to improve the performance of a single task with the help of other related tasks. Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information frommultiple tasks.\\nHowever, most existing work on multi-task learning attempts to divide the features of different tasks into private and shared spaces, merely based on whether parameters of some components should be shared. As shown in Figure 1- (a), the general shared-private model introduces two feature spaces for any task: one is used to\\nstore task-dependent features, the other is used to capture shared features. The major limitation of this framework is that the shared feature space could contain some unnecessary task-specific features, while some sharable features could also be mixed in private space, suffering from feature redundancy. Taking the following two sentences as examples, which are extracted from two different sentiment classification tasks: Movie reviews and Baby products reviews.\\nThe infantile cart is simple and easy to use. This kind of humour is infantile and boring.\\nThe word “infantile” indicates negative sentiment in Movie task while it is neutral in Baby task. However, the general shared-private model could place the task-specific word “infantile” in a shared space, leaving potential hazards for other tasks. Additionally, the capacity of shared space could also be wasted by some unnecessary features. To address this problem, in this paper we propose an adversarial multi-task framework, in which the shared and private feature spaces are inherently disjoint by introducing orthogonality constraints. Specifically, we design a generic sharedprivate learning framework to model the text se-\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nquence. To prevent the shared and private latent feature spaces from interfering with each other, we introduce two strategies: adversarial training and orthogonality constraints. The adversarial training is used to ensure that the shared feature space simply contains common and task-invariant information, while the orthogonality constraint is used to eliminate redundant features from the private and shared spaces.\\nThe contributions of this paper can be summarized as follows.\\n1. Proposed model divides the task-specific and shared space in a more precise way, rather than roughly sharing parameters. 2. We extend the original binary adversarial training to multi-class, which not only enables multiple tasks to be jointly trained, but allows us to utilize unlabeled data. 3. We can condense the shared knowledge among multiple tasks into an off-the-shelf neural layer, which can be easily transferred to new tasks.   \n",
            "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Expressions referring to objects in visual scenes typically include a word naming the type of the object: E.g., “house” in Figure 1 (a), or, as a very general type, “thingy” in Figure 1 (d). Determining such a name is is a crucial step for referring expression generation (REG) systems, as many other decisions, concerning e.g. the selection of attributes, follow from it (Dale and Reiter,\\n1995; Krahmer and Van Deemter, 2012). For a long time, however, research on REG mostly assumed the availability of symbolic representations of referent and scene, and sidestepped questions about how speakers actually choose these names, due to the lack of models capable of capturing what a word like house refers to in the real world.\\nRecent advances in image processing promise to fill this gap, with state-of-the-art computer vision systems being able to classify images into thousands of different categories (eg. Szegedy et al. (2015)). However, classification is not naming (Ordonez et al., 2016). Classification schemes are typically designed to be “flat”, with labels being on the same ontological level and, ideally, having disjunct extensions. In contrast, humans seem to be more flexible as to the chosen level of generality. Depending on the prototypicality of the object to name, and possibly other visual properties, a general name might be more or less appropriate. For instance, a robin can be named “bird”, but a penguin is better referred to as “penguin” (Rosch, 1978); along the same lines, the rather unusual building in Figure 1 that is not easy to otherwise\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\ncategorise was named “structure”. Other work at the intersection of image and language processing has investigated models that learn to directly associate visual objects with a representation of word meaning, for example through cross-modal transfer into distributional vector spaces. Under the assumption that such semantic spaces represent, in some form at least, taxonomic knowledge, this makes labels on different levels of specificity available for a given object. Moreover, if the mapping is sufficiently general, it should be able to map objects to an appropriate label, even if during training of the mapping this label has not been seen (zero-shot learning). While indeed performing with some promise on this task (Lazaridou et al., 2014), this approach does not generally outperform standard object classification with known categories (Frome et al., 2013; Norouzi et al., 2013).\\nThis paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Anonymous, in press) that treats words as individual predictors capturing referential appropriateness. We explore different ways of linking these predictors to distributional knowledge, during application and during training. We find that these improve over direct cross-modal mapping and direct visual classification in a standard and a zero-shot setup of an object naming task, as they allow for a more flexible combination of lexical and visual information when modeling referential meaning.   \n",
            "4    Conversational agents include task-oriented dialog systems and non-task-oriented chatbots. Dialog systems focus on helping people complete specific tasks in vertical domains (Young et al., 2010), while chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter et al., 2011). Existing work on building chatbots includes generation based methods and retrieval based methods. Retrieval based chatbots enjoy the advantage of informative and fluent responses, because they select a proper response for the current conversation from a repository with re-\\nsponse selection algorithms. While most existing work on retrieval based chatbots studies response selection for single-turn conversation (Wang et al., 2013) which only considers the last input message, we consider the problem in a multi-turn scenario. In a chatbot, multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context.\\nThe key to response selection lies in inputresponse matching. Different from single-turn conversation, multi-turn conversation requires matching between a response and a conversation context in which one needs to consider not only the matching between the response and the input message but also the matching between the response and the utterances in previous turns. The challenges of the task include (1) how to identify important information (words, phrases, and sentences) in the context that is crucial to selecting a proper response and how to leverage the information in matching; and (2) how to model relationships among the utterances in the context. Table 1 illustrates the challenges with an example. First, “hold a drum class” and “drum” in the context are very important. Without them, one may find responses relevant to the message (i.e., the last turn of the context) but nonsense in the context (e.g., “what lessons do you want?”). Second,\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nthe message highly depends on the second turn in the context, and the order of the utterances matters in response selection: exchanging the third turn and the last turn may lead to different responses. Existing work, however, either ignores relationships among utterances when concatenating them together (Lowe et al., 2015), or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.g., by a hierarchical RNN (Zhou et al., 2016)).\\nWe propose a sequential matching network (SMN), a new context based matching model that can tackle both challenges in an end-to-end way. The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector. Thus responses in these models cannot meet the context until the final step in matching. To avoid information loss, SMN matches a response with each utterance in the context at the beginning and encodes important information in each pair into a matching vector. The matching vectors are then accumulated in the temporal order of the utterances to model their relationships. The final matching degree is computed with the accumulation of the matching vectors. Specifically, for each utteranceresponse pair, the model constructs a word-word similarity matrix and a sequence-sequence similarity matrix by the embedding of words and the hidden states of a recurrent neural network with gated recurrent unites (GRU) (Chung et al., 2014) respectively. The two matrices capture important matching information in the pair on a word level and a segment level respectively, and the information is distilled and fused as a matching vector through an alternation of convolution and pooling operations on the matrices. By this means, important information from multiple levels of granularity in the context is recognized under sufficient supervision from the response and carried into matching with minimal loss. The matching vectors are then uploaded to another GRU to form a matching score for the context and the response. The GRU accumulates the pair matching in its hidden states in the chronological order of the utterances in the context. It models relationships and dependencies among the utterances in a matching fashion and has the utterance order supervise the accumulation of pair matching. The match-\\ning degree of the context and the response is computed by a logit model with the hidden states of the GRU. SMN extends the powerful “2D” matching paradigm in text pair matching for single-turn conversation to context based matching for multi-turn conversation, and enjoys the advantage that both important information in utterance-response pairs and relationships among utterances are sufficiently preserved and leveraged in matching.\\nWe test our model on the Ubuntu dialogue corpus (Lowe et al., 2015) which is a large scale public English data set for research in multi-turn conversation. The results show that our model can significantly outperform state-of-the-art methods, and improvement to the best baseline model on R10@1 is over 6%. In addition to the Ubuntu corpus, we create a human labeled Chinese data set, namely Douban Conversation Corpus, and test our model on it. Different from the Ubuntu corpus in which data is collected from a specific domain and negative candidates are randomly sampled, conversations in this data come from open domain, and response candidates in this data set are collected from a retrieval engine and labeled by three human judges. On this data, our model improves the best baseline model over 3% on R10@1 and 4% on P@1. As far as we know, Douban Conversation Corpus is the first human labeled data set for multi-turn response selection and could be a good complement to the Ubuntu corpus. We release Douban Conversation Corups and our source code at an anonymous url for blind review. We have uploaded code and data with this paper.\\nOur contributions in this paper are three-folds: (1) proposal of a new context based matching model for multi-turn response selection in retrieval based chatbots; (2) publication of a large human labeled data set to research communities. (3) empirical verification of the effectiveness of the model on public data sets;   \n",
            "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
            "132                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Low-resource natural language processing remains an open problem for many tasks of interest. Furthermore, for most languages in the world, high-cost linguistic annotation and resource creation are unlikely to be undertaken in the near future. In the case of morphology, out of the 7000 currently spoken (Lewis, 2009) languages, only about 200 have computer-readable annotations (Sylak-Glassman et al., 2015) – although morphology is easy to annotate compared to syntax and semantics. Transfer learning is one solution to this problem: it exploits annotations in a high-resource language to train a system for a low-resource language. In this work, we present a method for cross-lingual transfer of inflectional morphology using an encoder-decoder recurrent neural network (RNN). This allows for the development of tools for computational morphology with limited annotated data.\\nIn morphologically rich languages, individual lexical entries may be realized as distinct inflec-\\ntions of a single lemma depending on the syntactic context. For example, the 3SgPresInd of the English verbal lemma to bring is brings. In many languages, a lemma can have hundreds of individual forms. Thus, both generation and analysis of such morphological inflections are active areas of research in NLP and morphological processing has been shown to be a boon to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and Çetinoğlu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7→ brought (with Past being the target tag).\\nRNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Schütze, 2016a; Cotterell et al., 2016a). However, these models require a large amount of data to achieve competitive performance; this makes them unsuitable for out-of-thebox application to paradigm completion in the low-resource scenario. To mitigate this, we consider transfer learning: we train an end-to-end neural system jointly with limited data from a lowresource language and a larger amount of data from a high-resource language. This technique allows the model to apply knowledge distilled from the high-resource training data to the low-resource language as needed.\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nWe conduct experiments on 21 language pairs from four language families, emulating a lowresource setting. Our results demonstrate successful transfer of morphological knowledge. We show improvements in accuracy and edit distance of up to 58% (accuracy) and 4.62 (edit distance) over the same model with only in-domain language data on the paradigm completion task. We further obtain up to 44% (resp. 14%) improvement in accuracy for the one-shot (resp. zero-shot) setting, i.e., one (resp. zero) in-domain language sample per target tag. We also show that the effectiveness of morphological transfer depends on language relatedness, measured by lexical similarity.   \n",
            "133                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Emotion recognition and sentiment analysis have become a new trend in social media, helping users to automatically extract the opinions expressed in user-generated content, especially videos. Thanks to the high availability of computers and smartphones, and the rapid rise of social media, consumers tend to record their reviews and opinions about products or films and upload them on social media platforms, such as YouTube or Facebook. Such videos often contain comparisons, which can aid prospective buyers make an informed decision.\\nThe primary advantage of analyzing videos over text is the surplus of behavioral cues present in vocal and visual modalities. The vocal modulations and facial expressions in the visual data, along with textual data, provide important cues to better identify affective states of the opinion holder. Thus, a combination of text and video data helps to create a better emotion and sentiment analysis model (Poria et al., 2017).\\nRecently, a number of approaches to multimodal sentiment analysis, producing interesting results, have been proposed (Pérez-Rosas et al., 2013; Wollmer et al., 2013; Poria et al., 2015). However, there are major issues that remain unaddressed, such as the role of speaker-dependent versus speaker-independent models, the impact of each modality across the dataset, and generalization ability of a multimodal sentiment classifier. Leaving these issues unaddressed has presented difficulties in effective comparison of different multimodal sentiment analysis methods.\\nAn utterance is a unit of speech bound by breathes or pauses. Utterance-level sentiment analysis focuses on tagging every utterance of a video with a sentiment label (instead of assigning a unique label to the whole video). In particular, utterance-level sentiment analysis is useful to understand the sentiment dynamics of different aspects of the topics covered by the speaker throughout his/her speech. The true meaning of an utterance is relative to its surrounding utterances.\\nIn this paper, we consider such surrounding utterances to be the context, as the consideration of temporal relation and dependency among utterances is key in human-human communication. For example, the MOSI dataset (Zadeh et al., 2016) contains a video, in which a girl reviews the movie ‘Green Hornet’. At one point, she says “The Green Hornet did something similar”. Normally, doing something similar, i.e., monotonous or repetitive might be perceived as negative. However, the nearby utterances “It engages the audience more”, “they took a new spin on it”, “and I just loved it” indicate a positive context.\\nIn this paper, we discard the oversimplifying hypothesis on the independence of utterances and develop a framework based on long short-term memory (LSTM) to extract utterance features that also consider surrounding utterances.\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nOur model enables consecutive utterances to share information, thus providing contextual information in the classification process. Experimental results show that the proposed framework has outperformed the state of the art on benchmark datasets by 5−10%. The paper is organized as follows: Section 2 provides a brief literature review on multimodal sentiment analysis; Section 3 describes the proposed method in detail; experimental results and discussion are shown in Section 4; finally, Section 5 concludes the paper.   \n",
            "134                                                                                                                                     Keyphrase or keyword is a piece of short and summative content that expresses the main semantic meaning of a long text. A typical use of keyphrase or keyword is in scientific publications, to provide the core information of a paper. We use the term “keyphrase”, interchangeable as “keyword”, in the rest of this paper, as it implies that it may contain multiple words. High-quality keyphrases can facilitate the understanding, organizing and accessing of document content. As a result, many stud-\\nies have devoted to studying the ways of automatic extracting keyphrases from textual content (Liu et al., 2009; Medelyan et al., 2009a; Witten et al., 1999). Due to the public accessibility, many scientific publication datasets are often used as the test beds for keyphrase extraction algorithms. Therefore, this study also focuses on extracting keyphrases from scientific publications.\\nAutomatically extracting keyphrases from a document is called Keypharase Extraction, and it has been widely exploited in many applications, such as information retrieval (Jones and Staveley, 1999), text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), and opinion mining (Berend, 2011). Most of the existing keyphrase extraction algorithms addressed this problem through two steps (Liu et al., 2009; Tomokiyo and Hurst, 2003). The first step is to acquire a list of keyphrase candidates. Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying the potential candidates (Hulth, 2003; Le et al., 2016; Liu et al., 2010; Wang et al., 2016). The second step is to rank candidates regarding their importance to the document, either through supervised or unsupervised machine learning methods with a set of manuallydefined features (Frank et al., 1999; Liu et al., 2009, 2010; Kelleher and Luz, 2005; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Song et al., 2003; Witten et al., 1999).\\nThere are two major drawbacks for the above keyphrase extraction approaches.\\nFirstly, they can only extract the keyphrases that appear in the source text, whereas they fail at predicting the meaningful keyphrases with a slightly different sequential order or using synonym words. However, it is common in a scientific publication where authors assign keyphrases based on the semantic meaning instead of follow-\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\ning the written content in the publication. In this paper, we denote the phrases that do not match any contiguous subsequence of source text as absent keyphrases, and the ones that fully match a part of the text as present keyphrases. Table 1 shows the proportion of present and absent keyphrases from the document abstract in four commonlyused datasets, where we observe large portions of absent keyphrases in all the datasets. The absent keyphrases cannot be extracted through previous approaches, which further urges the development of a more powerful keyphrase prediction model.\\nSecondly, when ranking phrase candidates, previous approaches often adopted the machine learning features such as TF-IDF and PageRank. However, these features only target to detect the importance of each word in the document based on the statistics of word occurrence and co-occurrence, whereas they can hardly reveal the semantics behind the document content.\\nTable 1: Proportion of the present keyphrases and absent keyphrases in four public datasets\\nDataset # Keyphrase % Present % Absent Inspec 19,275 55.69 44.31\\nKrapivin 2,461 44.74 52.26 NUS 2,834 67.75 32.25\\nSemEval 12,296 42.01 57.99\\nTo overcome the limitations of previous studies, we re-examine the process of Keyphrase Prediction, about how real human annotators would assign keyphrases. Given a document, human annotators will firstly read the text to get a basic understanding of the content, then they try to digest its essential content and summarize into keyphrases. Their generation of keyphrases relies on the understanding of the content, which not necessarily to be the words that occurred in the source text. For example, when human annotators see “Latent Dirichlet Allocation” in the text, they could write down “topic modeling” and/or “text mining” as possible keyphrases. In addition to the semantic understanding, human annotators might also go back and picks up the most important parts based on syntactic features. For example, the phrases following “we propose/apply/use” are supposed to be important in the text. As a result, a better keyphrase prediction model should understand the semantic meaning of the content, as well as capture the contextual features.\\nTo effectively capture semantic and syntactic features, we utilize recurrent neural networks (RNN) (Cho et al., 2014; Gers and Schmidhuber, 2001) to compress the semantic information in the given text into a dense vector (i.e., semantic understanding). Furthermore, we incorporate a copy mechanism (Gu et al., 2016) to equip our model with the capability of finding important parts based on language syntax (i.e., syntactic understanding). Thus, our model can generate keyphrases based on the understanding of the text, no matter whether the keyphrases are present in the text or not; meanwhile, it does not lose important in-text information.\\nThe contribution of this paper is in three-fold: a) we propose to apply an RNN-based generative model to keyphrase prediction, and we also incorporate a copy mechanism in RNN, which enables the model to successfully predict rarely-occurred phrases; b) this is the first work concerning about the problem of absent keyphrase prediction for scientific publications, and our model recalls up to 20% of absent keyphrases; and c) we conduct a comprehensive comparison against six important baselines on a broad range of datasets, and the results show that our proposed model significantly outperforms existing supervised and unsupervised extraction methods.\\nIn the remainder of this paper, we firstly review the related work in Section 2. Then we elaborate the proposed model in Section 3. After that, we present the experiment setting in Section 4 and results in Section 5, followed by our discussion in Section 6. Section 7 concludes the paper.   \n",
            "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         When handling broad or open domains, machine translation systems usually have to handle a large vocabulary as their inputs and outputs. This is particularly a problem in neural machine translation (NMT) models (Sutskever et al., 2014), such as the attention-based model (Bahdanau et al., 2014; Luong et al., 2015) shown in Figure 1. In these models, the output layer is required to generate a specific word from an internal vector, and a large vocabulary size tends to require a large amount of computation to predict each of the candidate word probabilities.\\nBecause this is a significant problem for neural language and translation models, there are a number of methods proposed to resolve this problem, which we detail in Section 2.2. However, none of these previous methods simultaneously satisfies the following desiderata, all of which, we argue, are desirable for practical use in NMT systems:\\nMemory efficiency: The method should not require large memory to store the parameters and calculated vectors to maintain scalability in resource-constrained environments.\\nTime efficiency: The method should be able to train the parameters efficiently, and possible to perform decoding efficiently with choosing the candidate words from the full probability distribution. In particular, the method should be performed fast on general CPUs to suppress physical costs of computational resources for actual production systems.\\nCompatibility with parallel computation: It should be easy for the method to be minibatched and optimized to run efficiently on GPUs, which are essential for training large NMT models.\\nIn this paper, we propose a method that satisfies all of these conditions: requires significantly less memory, fast, and is easy to implement minibatched on GPUs. The method works by not pre-\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\ndicting a softmax over the entire output vocabulary, but instead by encoding each vocabulary word as a vector of binary variables, then independently predicting the bits of this binary representation. In order to represent a vocabulary size of 2n, the binary representation need only be at least n bits long, and thus the amount of computation and size of parameters required to select an output word is only O(log V ) in the size of the vocabulary V , a great reduction from the standard linear increase of O(V ) seen in the original softmax.\\nWhile this idea is simple and intuitive, we found that it alone was not enough to achieve competitive accuracy with real NMT models. Thus we make two improvements: First, we propose a hybrid model, where the high frequency words are predicted by a standard softmax, and low frequency words are predicted by the proposed binary codes separately. Second, we propose the use of convolutional error correcting codes with Viterbi decoding (Viterbi, 1967), which add redundancy to the binary representation, and even in the face of localized mistakes in the calculation of the representation, are able to recover the correct word.\\nIn experiments on two translation tasks, we find that the proposed hybrid method with error correction is able to achieve results that are competitive with standard softmax-based models while reducing the output layer to a fraction of its original size.   \n",
            "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Type-level word embeddings map a word type (i.e., a surface form) to a dense vector of real numbers such that similar word types have similar embeddings. When pre-trained on a large corpus of unlabeled text, they provide an effective mechanism for generalizing statistical models to words which do not appear in the labeled training data for a downstream task.\\nIn this paper, we make the following distinction between types and tokens: By word types, we mean the surface form of the word, whereas by tokens we mean the instantiation of the surface form in a context. For example, the same word type ‘pool’ occurs as two different tokens in the sentences “He sat by the pool,” and “He played a game of pool.”\\nMost word embedding models define a single vector for each word type. However, a fundamen-\\ntal flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word ‘pool’ has different meanings, but the same representation is typically used for both of them. Similarly, the fact that ‘pool’ and ‘lake’ are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014; Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space.\\nPrevious work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Schütze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings.\\nIn this paper, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.e., weighted sum) of the concept embeddings as the token representation (see §2). In addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings.\\nOur approach to context-sensitive embeddings assumes the availability of a lexical ontology such\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nFigure 1: An example grounding for the word ‘pool’. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word ‘pool’ regardless of its context. Other WordNet senses for ‘pool’ were removed from the figure for simplicity.\\nas WordNet, but it does not require a tagger for word senses. We use the proposed embeddings to predict prepositional phrase (PP) attachments (see §3), a challenging problem which emphasizes the selectional preferences between words in the PP and each of the candidate head words. Our empirical results and detailed analysis (see §4) show that the proposed embeddings effectively use WordNet to improve the accuracy of PP attachment predictions.   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Contenido  \\\n",
            "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        **2 Related Work**\\nAutomatic poetry generation is an important task due to the significant challenges involved. Most systems that have been proposed can loosely be categorised as rule-based expert systems, or statistical approaches. Rule-based poetry generation attempts include case-based reasoning (Gervás, 2000), templatebased generation (Colton et al., 2012), constraint satisfaction (Toivanen et al., 2013) and text mining (Netzer et al., 2009). These approaches are often inspired by how humans might generate poetry. Statistical approaches, conversely, make no assumptions about the creative process. Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used generate new poetic variants (Yi et al., 2016; Greene et al., 2010). The work of Zhang and Lapata (2014) is similar to ours, in that they make use of neural language models. For the task of automatic generation of classical Chinese poetry, they were able to outperform all other Chinese poetry generation systems with both manual and automatic evaluation.\\n**3 Phonetic-level Model**\\nOur first model is a pure neural language model, trained on a phonetic encoding of poetry in order to represent both form and content. Phonetic encodings of language represent information as sequences of around 40 basic acoustic symbols. Training on phonetic symbols allows the model to learn effective representations of pronunciation, including rhyme and rhythm. However, just training on a large corpus of poetry data is not enough. Specifically, two problems need to be overcome. 1) Phonetic encoding results in information loss: words that have the same pronunciation (homophones) cannot be perfectly reconstructed from the corresponding phonemes. This means that we require an additional probabilistic model in order to determine the most likely word given a sequence of phonemes. 2) The variety of poetry and poetic devices one can use— e.g., rhyme, rhythm, repetition—means that poems sampled from a model trained on all poetry would be unlikely to maintain internal consistency of meter and rhyme. It is therefore important to train the model on poetry which has its own internal consistency.\\nThus, the model comprises three steps: transliterating an ortographic sequence to its phonetic representation, training a neural language model on the phonetic encoding, and decoding the generated sequence back from phonemes to orthographic symbols.\\nPhonetic encoding To solve the first step, we apply a combination of word lookups from the CMU pronunciation dictionary (Weide, 2005) with letter-to-sound rules for handling out-ofvocabulary words. These rules are based on the CART techniques described by Black et al. (1998), and are represented with a simple Finite State Transducer1. The number of letters and number of phones in a word are rarely a one-to-one match: letters may match with up to three phones. In addition, virtually all letters can, in some contexts, map to zero phones, which is known as ‘wild’ or epsilon. Expectation Maximisation is used to compute the probability of a single letter matching a single phone, which is maximised through the application of Dynamic Time Warping (Myers et al., 1980) to determine the most likely position of epsilon characters. Although this approach offers full coverage over the training corpus—even for abbreviated words like ask’d and archaic words like renewest—it has several limitations. Irregularities in the English language result in difficulty determining general letter-to-sound rules that can manage words with unusual pronunciations such as “colonel” and “receipt” 2.\\nIn addition to transliterating words into phoneme sequences, we also represent word break characters as a specific symbol. This makes decipherment, when converting back into an orthographic representation, much easier. Phonetic transliteration allows us to construct a phonetic poetry corpus comprising 1,046,536 phonemes.\\nNeural language model We train a Long-Short Term Memory Network (Hochreiter and Schmidhuber, 1997) on the phonetic representation of our poetry corpus. The model is trained using stochastic gradient descent to predict the next phoneme given a sequence of phonemes. Specifically, we\\n1Implemented using FreeTTS (Walker et al., 2010) 2An evaluation of models in American English, British English, German and French was undertaken by Black et al. (1998), who reported an externally validated per token accuracy on British English as low as 67%. Although no experiments were carried out on corpora of early-modern English, it is likely that this accuracy would be significantly lower.\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\nmaximize a multinomial logistic regression objective over the final softmax prediction. Each phoneme is represented as a 256-dimensional embedding, and the model consists of two hidden layers of size 256. We apply backpropagationthrough-time (Werbos, 1990) for 150 timesteps, which roughly equates to four lines of poetry in sonnet form. This allows the network to learn features like rhyme even when spread over multiple lines. Training is preemptively stopped at 25 epochs to prevent overfitting.\\nOrthographic decoding When decoding from phonemes back to orthographic symbols, the goal is to compute the most likely word corresponding to a sequence of phonemes. That is, we compute the most probable hypothesis word W given a phoneme sequence ρ:\\nargmaxi P (Wi | ρ ) (1)\\nWe can consider the phonetic encoding of plaintext to be a homophonic cipher; that is, a cipher in which each symbol can correspond to one or more possible decodings. The problem of homophonic decipherment has received significant research attention in the past; with approaches utilising Expectation Maximisation (Knight et al., 2006), Integer Programming (Ravi and Knight, 2009) and A* search (Corlett and Penn, 2010). Transliteration from phonetic to an orthographic representation is done by constructing a Hidden Markov Model using the CMU pronunciation dictionary (Weide, 2005) and an n-gram language model. We calculate the transition probabilities (using the n-gram model) and the emission matrix (using the CMU pronunciation dictionary) to determine pronunciations that correspond to a single word. All pronunciations are naively considered equiprobable. We perform Viterbi decoding to find the most likely sequence of words. This means finding the most likely word wt+1 given a previous word sequence (wt−n, ..., wt).\\nargmaxwt+1 P ( wt+1 | w1, ... , wt ) (2)\\nIf a phonetic sequence does not map to any word, we apply the heuristic of artificially breaking the sequence up into two subsequences at index n, such that nmaximises the n-gram frequency of the subsequences.\\nAnd humble and their fit flees are wits size but that one made and made thy step me lies\\n————————————— Cool light the golden dark in any way the birds a shade a laughter turn away\\n————————————— Then adding wastes retreating white as thine\\nShe watched what eyes are breathing awe what shine —————————————\\nBut sometimes shines so covered how the beak Alone in pleasant skies no more to seek\\nFigure 1: Example output of the phonetic-level model trained on Iambic Pentameter poetry (grammatical errors are emphasised).\\nOutput A popular form of poetry with strict internal structure is the sonnet. Popularised in English by Shakespeare, the sonnet is characterised by a strict rhyme scheme and exactly fourteen lines of Iambic Pentameter (Greene et al., 2010). Since the 17,134 word tokens in Shakespeare’s 153 sonnets are insufficient to train an effective model, we augment this corpus with poetry taken from the website sonnets.org, yielding a training set of 288,326 words and 1,563,457 characters. An example of the output when training on this sonnets corpus is provided in Figure 1. Not only is it mostly in strict Iambic Pentameter, but the grammar of the output is mostly correct and the poetry contains rhyme.\\n**4 Constrained Character-level Model**\\nAs the example shows, phonetic-level language models are effective at learning poetic form, despite small training sets and relatively few parameters. However, the fact that they require training data with internal poetic consistency implies that they do not generalise to other forms of poetry. That is, in order to generate poetry in Dactylic Hexameter (for example), a phonetic model must be trained on a corpus of Dactylic poetry. Not only is this impractical, but in many cases no corpus of adequate size even exists. Even when such poetic corpora are available, a new model must be trained for each type of poetry. This precludes tweaking the form of the output, which is important when generating poetry automatically. We now explore an alternative approach. Instead of attempting to represent both form and content in a single model, we construct a pipeline containing a generative language model represent-\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\ning content, and a discriminative model representing form. This allows us to represent the problem of creating poetry as a constraint satisfaction problem, where we can modify constraints to restrict the types of poetry we generate.\\nCharacter Language Model Rather than train a model on data representing features of both content and form, we now use a simple character-level model (Sutskever et al., 2011) focused solely on content. This approach offers several benefits over the word-level models that are prevalent in the literature. Namely, their more compact vocabulary allows for more efficient training; they can learn common prefixes and suffixes to allow us to sample words that are not present in the training corpus and can learn effective language representations from relatively small corpora; and they can handle archaic and incorrect spellings of words. As we no longer need the model to explicitly represent the form of generated poetry, we can loosen our constraints when choosing a training corpus. Instead of relying on poetry only in sonnet form, we can instead construct a generic corpus of poetry taken from online sources. This corpus is composed of 7.56 million words and 34.34 million characters, taken largely from 20th Century poetry books found online. The increase in corpus size facilitates a corresponding increase in the number of permissible model parameters. This allows us to train a 3-layer LSTM model with 2048- dimensional hidden layers. The model was trained to predict the next character given a sequence of characters, using stochastic gradient descent. We attenuate the learning rate over time, and by 20 epochs the model converges.\\nRhythm Modeling Although a character-level language model trained on a corpus of generic poetry allows us to generate interesting text, internal irregularities and noise in the training data prevent the model from learning important features such as rhythm. Hence, we require an additional classifier to constrain our model by either accepting or rejecting sampled lines based on the presence or absence of these features. As the presence of meter (rhythm) is the most characteristic feature of poetry, it therefore must be our primary focus. Pronunciation dictionaries have often been used to determine the syllabic stresses of words (Colton et al., 2012; Manurung et al., 2000; Misztal and Indurkhya, 2014), but suffer from some limitations\\nfor constructing a classifier. All word pronunciations are considered equiprobable, including archaic and uncommon pronunciations, and pronunciations are provided context free, despite the importance of context for pronunciation3. Furthermore, they are constructed from American English, meaning that British English may be misclassified. These issues are circumvented by applying lightly supervised learning to determine the contextual stress pattern of any word. That is, we exploit the latent structure in our corpus of sonnet poetry, namely, the fact that sonnets are composed of lines in rigid Iambic Pentameter, and are therefore exactly ten syllables long with alternating syllabic stress. This allows us to derive a syllablestress distribution. Although we use the sonnets corpus for this, it is important to note that any corpus with such a latent structure could be used. By representing a line as a cascade of Weighted Finite State Transducers (WFST), we can perform Expectation Maximisation over the poetry corpus to obtain a probabilistic classifier which enables us to determine the most likely stress patterns for each word. Every word is represented by a single transducer. Since weights can be assigned to state transitions, we can model the probability that a given input string maps to a particular output. In each cascade, a sequence of input words is mapped onto a sequence of stress patterns ⟨×, /⟩ where each pattern is between 1 and 5 syllables in length4. We initially set all transition probabilities equally, as we make no assumptions about the stress distributions in our training set. We then iterate over each line of the sonnet corpus, using Expectation Maximisation to train the cascades. In practice, there are several de facto variations of Iambic meter which are permissible, as shown in Figure 2. We train the rhythm classifier by converging the cascades to whatever output is the most likely given the line.\\nConstraining the model To generate poetry using this model, we sample sequences of characters from the character-level language model. To impose rhythm constrains on the language model, we first represent these sampled characters at the word level and pool sampled characters into word\\n3For example, the independent probability of stressing the single syllable word at is 40%, but this increases to 91%when the following word is the (Greene et al., 2010)\\n4Words of more than 5 syllables comprise less than 0.1% of the lexicon (Aoyama and Constable, 1998).\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\n× / × / × / × / × / / × × / × / × / × / × / × / × / × / × / × / × × / × / × / × / ×\\nFigure 2: Permissible variations of Iambic Pentameter in Shakespeare’s sonnets.\\ntokens in an intermediary buffer. We then apply the separately trained word-level WFSTs to construct a cascade of this buffer and perform Viterbi decoding over the cascade. This defines the distribution of stress-patterns over our word tokens. We can represent this cascade as a probabilistic classifier, and accept or reject the buffered output based on how closely it conforms to the desired meter. While sampling sequences of words from this model, the entire generated sequence is passed to the classifier each time a new word is sampled. The pronunciation model then returns the probability that the entire line is within the specified meter. If a new word is rejected by the classifier, the state of the network is rolled back to the state of the last formulaically acceptable line, removing the rejected word from memory. The constraint on rhythm can be controlled by adjusting the acceptability threshold of the classifier. By increasing the threshold, output focuses on form over content. Conversely, decreasing the criterion puts greater emphasis on content.\\nGeneric poetry\\nSonnet poetry\\nLSTM\\nWFST\\nRhythmic Output\\nTrained\\nTrained\\nBuffer\\n**4.1 Themes and Poetic devices**\\nIt is important for any generative poetry model to include themes and poetic devices. One way to achieve this would be by constructing a corpus that exhibits the desired themes and devices. To create a themed corpus about ‘love’, for instance,\\nThemed Training Set\\nPoetry LSTM\\nThemed Output\\nTraining Set\\nPoetry LSTM\\nThemed Output\\nThematic Boosting\\nImplicit Explicit\\nFigure 3: Two approaches for generating themed poetry.\\nwe would aggregate love poetry to train the model, which would thus learn an implicit representation of love. However, this forces us to generate poetry according to discrete themes and styles from pretrained models, requiring a new training corpus for each model. In other words, we would suffer from similar limitations as with the phonetic-level model, in that we require a dedicated corpus. Alternatively, we can manipulate the language model by boosting character probabilities at sample time to increase the probability of sampling thematic words like ‘love’. This approach is more robust, and provides us with more control over the final output, including the capacity to vary the inclusion of poetic devices in the output.\\nThemes In order to introduce thematic content, we heuristically boost the probability of sampling words that are semantically related to a theme word from the language model. First, we compile a list of similar words to a key theme word by retrieving its semantic neighbours from a distributional semantic model (Mikolov et al., 2013). For example, the theme winter might include thematic words frozen, cold, snow and frosty. We represent these semantic neighbours at the character level, and heuristically boost their probability by multiplying the sampling probability of such character strings by a function of their cosine similarity to the key word. Thus, the likelihood of sampling a thematically related word is artificially increased, while still constraining the model rhythmically.\\nPoetic devices A similar method may be used for poetic devices such as assonance, consonance and alliteration. Since these devices can be orthographically described by the repetition of identical sequences of characters, we can apply the\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nErrors per line 1 2 3 4 Total\\nPhonetic Model 11 2 3 1 28 Character Model + WFST 6 5 1 1 23 Character Model 3 8 7 7 68\\nTable 1: Number of lines with n errors from a set of 50 lines generated by each of the three models.\\nWord Line Coverage\\nWikipedia 64.84% 83.35% 97.53% Sonnets 85.95% 80.32% 99.36%\\nTable 2: Error when transliterating text into phonemes and reconstructing back into text.\\nsame heuristic to boost the probability of sampling character strings that have previously been sampled. That is, to sample a line with many instances of alliteration (multiple words with the same initial sound) we record the historical frequencies of characters sampled at the beginning of each previous word. After a word break character, we boost the probability that those characters will be sampled again in the softmax. We only keep track of frequencies for a fixed number of time steps. By increasing or decreasing the size of this window, we can manipulate the prevalence of alliteration. Variations of this approach are applied to invoke consonance (by boosting intra-word consonants) and assonance (by boosting intra-word vowels). An example of two sampled lines with high degrees of alliteration, assonance and consonance is given in Figure 4c.\\n**5 Evaluation**\\nIn order to examine how effective our methodologies for generating poetry are, we evaluate the proposed models in two ways. First, we perform an intrinsic evaluation where we examine the quality of the models and the generated poetry. Second, we perform an extrinsic evaluation where we evaluate the generated output using human annotators, and compare it to human-generated poetry.\\n**5.1 Intrinsic evaluation**\\nTo evaluate the ability of both models to generate formulaic poetry that adheres to rhythmic rules, we compared sets of fifty sampled lines from each model. The first set was sampled from the phonetic-level model trained on Iambic poetry.\\nThe second set was sampled from the characterlevel model, constrained to Iambic form. For comparison, and to act as a baseline, we also sampled from the unconstrained character model. We created gold-standard syllabic classifications by recording each line spoken-aloud, and marking each syllable as either stressed or unstressed. We then compared these observations to loose Iambic Pentameter (containing all four variants), to determine how many syllabic misclassifications existed on each line. This was done by speaking each line aloud, and noting where the speaker put stresses. As Table 1 shows, the constrained character level model generated the most formulaic poetry. Results from this model show that 70% of lines had zero mistakes, with frequency obeying an inverse power-law relationship with the number of errors. We can see that the phonetic model performed similarly, but produced more subtle mistakes than the constrained character model: many of the errors were single mistakes in an otherwise correct line of poetry. In order to investigate this further, we examined to what extent these errors are due to transliteration (i.e., the phonetic encoding and orthographic decoding steps). Table 2 shows the reconstruction accuracy per word and per line when transliterating either Wikipedia or Sonnets to phonemes using the CMU pronunciation dictionary and subsequently reconstructing English text using the ngram model5. Word accuracy reflects the frequency of perfect reconstruction, whereas per line tri-gram similarity (Kondrak, 2005) reflects the overall reconstruction. Coverage captures the percentage of in-vocabulary items. The relatively low per-word accuracy achieved on the Wikipedia corpus is likely due to the high frequency of out-ofvocabulary words. The results show that a significant number of errors in the phonetic-level model are likely to be caused by transliteration mistakes.\\n**5.2 Extrinsic evaluation**\\nWe conducted an indistinguishability study with a selection of automatically generated poetry and human poetry. As extrinsic evaluations are expensive and the phonetic model was unlikely to do well (as illustrated in Figure 4e: the model generates good Iambic form, but not very good English),\\n5Obviously, calculating this value for the character-level model makes no sense, since no transliteration occurs in that case.\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\n(a) The crow crooked on more beautiful and free, He journeyed off into the quarter sea. his radiant ribs girdled empty and very - least beautiful as dignified to see.\\n(c) Man with the broken blood blue glass and gold. Cheap chatter chants to be a lover do.\\n(e) The son still streams and strength and spirit. The ridden souls of which the fills of.\\n(b) Is that people like things (are the way we to figure it out) and I thought of you reading and then is your show or you know we will finish along will you play.\\n(d) How dreary to be somebody, How public like a frog To tell one’s name the livelong day To an admiring bog.\\nFigure 5: The experimental environment for asking participants to distinguish between automatically generated and human poetry.\\nwe only evaluate on the constrained characterlevel model.\\nThe aim of the study was to determine whether participants could distinguish between human and generated poetry, and if so to what extent. A set of 70 participants (of whom 61 were English native speakers) were each shown a selection of randomly chosen poetry segments, and were invited to classify them as either human or generated. Participants were recruited from friends and people within poetry communities, with an age range of 17 to 80, and a mean age of 29. Our participants were not financially incentivised, perceiving the evaluation as an intellectual challenge.\\nIn addition to the classification task, each partic-\\nipant was also invited to rate each poem on a 1-5 scale with respect to three criteria, namely readability, form and evocation (how much emotion did a poem elicit). We naively consider the overall quality of a poem to be the mean of these three measures. We used a custom web-based environment, built specifically for this evaluation6, which is illustrated in Figure 5. Based on human judgments, we can determine whether the models presented in this work can produce poetry of a similar quality to humans. To select appropriate human poetry that could be meaningfully compared with the machinegenerated poetry, we performed a comprehension test on all poems used in the evaluation, using the Dale-Chall readability formula (Dale and Chall, 1948). This formula represents readability as a function of the complexity of the input words. We selected nine machine-generated poems with a high readability score. The generated poems produced an average score of 7.11, indicating that readers over 15 years of age should easily be able to comprehend them. For our human poems, we focused explicitly on poetry where greater consideration is placed on prosodic elements like rhythm and rhyme than semantic content (known as “nonsense verse”). We randomly selected 30 poems belonging to that category from the website poetrysoup.com, of which\\n6[URL-ANONYMIZED]\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nPoet Title Human Readability Emotion Form\\nGenerated Best 0.66 0.60 -0.77 0.90 G. M. Hopkins Carrion Comfort 0.62 -1.09 1.39 -1.55\\nJ. Thornton Delivery of Death 0.60 0.26 -1.38 -0.65\\nGenerated All 0.54 -0.28 -0.30 0.23 M. Yvonne Intricate Weave 0.53 2.38 0.94 -1.67\\nE. Dickinson I’m Nobody 0.52 -0.46 0.92 0.44\\nG. M. Hopkins The Silver Jubilee 0.52 0.71 -0.33 0.65\\nR. Dryden Mac Flecknoe 0.51 -0.01 0.35 -0.78\\nA. Tennyson Beautiful City 0.48 -1.05 0.97 -1.26\\nW. Shakespeare A Fairy Song 0.45 0.65 1.30 1.18\\nTable 3: Proportion of people classifying each poem as ’human’, as well as the relative qualitative scores of each poem as deviations from the mean.\\neight were selected for the final comparison based on their comparable readability score. The selected poems were segmented into passages of between four and six lines, to match the length of the generated poetry segments. An example of such a segment is shown in Figure 4d. The human poems had an average score of 7.52, requiring a similar level of English aptitude to the generated texts. The performance of each human poem, alongside the aggregated scores of the generated poems, is illustrated in Table 3. For the human poems, our group of participants guessed correctly that they were human 51.4% of the time. For the generated poems, our participants guessed correctly 46.2% of the time that they were machine generated. To determine whether our results were statistically significant, we performed a Chi2 test. This resulted in a p-value of 0.718. This indicates that our participants were unable to tell the difference between human and generated poetry in any significant way. Although our participants generally considered the human poems to be of marginally higher quality than our generated poetry, they were unable to effectively distinguish between them. Interestingly, our results seem to suggest that our participants consider the generated poems to be more ‘human-like’ than those actually written by humans. Furthermore, the poem with the highest overall quality rating is a machine generated one. This shows that our approach was effective at generating high-quality rhythmic verse. It should be noted that the poems that were most ‘human-like’, most aesthetic and most emotive re-\\nspectively (though not the most readable) were all generated by the neural character model. Generally the set of poetry produced by the neural character model was slightly less readable and emotive than the human poetry, but had above average form. All generated poems included in this evaluation can be found in the supplementary material.\\n   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      **2 Phrasal RNNs**\\nWe assume that, in the task of language model and machine translation, selecting the appropriate hidden structures for one sentence is highly related to the performance of the task.\\nFormally, a typical pRNN consist of three subnetworks: phrasal part P , attention part A and sequential part S. Each sub-network plays its own role and collaborates with others. P (§2.1) constructs the neural structures (realvalued vectors) which corresponding to natural linguistic structures (phrases). It takes embed-\\ndings (x?) of all words in a candidate phrase, as input, then output one fix-length real-valued vector p as the distributed representation (Hinton et al., 1986) of the phrase;\\np = P (xji ) (1)\\nWhere xji = [xi, · · · , xj ] (2)\\nA (§2.2) compares the candidate phrase regards to the current situation, give a probabilistic distribution over them, then provides the weighted sum of their representations. It takes previous calculated candidate set {p} as input, then output a weighted sum of {p} as p̂ with the help of current hidden state (ht) at time step t given by (S).\\np̂ = A(h, pk, · · · , pl) (3)\\nS (§2.2) combine the weighted sum of candidate phrases into original RNN, forces RNN taking the structural history information into consideration. It is similar to original RNN except for one point: when predict next hidden state ht+1, besides ht and xt, it takes p̂ as input too.\\nht = S(ht−1, xt, p̂t) (4)\\n**2.1 Represent Phrases**\\nThere are many types of neural networks which can transfer phrase into distributed representation. However, when we need to handle arbitrary length phrases, another way of saying, the entire history\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\nof words, the choices are very limited to a few RNNs’ variants.\\nEven when we choose RNNs to construct structure vectors, we are still facing a big problem. Because the hidden state ht are considered to encode the entire history information from the beginning of the sentence xt1, they can be utilized as representations of phrases begin at the sentence head {xj1|1 ≤ j ≤ t}. But they do not provide the representations of phrases which do not begin with the first word of the sentence {xji |1 < i < j ≤ t}.\\nTo represent all candidate phrases in a sentence with n words, we build a RNN pyramid (RNNP (Fig. 3)), with n horizontal parallel RNNs {RNNn}Nn=1. RNNn indicates that it begins at the n-th word of the sentence. With all N(N + 1)/2 hidden status generated by RNN pyramid, we obtain distributed representation of all candidate phrases/structures of a sentence.\\nTo keep consistent among these parallel RNNs in the pyramid and to limit the number of parameters for keeping the model simple, we let all parallel RNNs share the same network parameters (W,U, b).\\nhnt = σ(Wxt + Uh n t−1 + b) (5)\\nWhere hnt of the RNNn indicates the hidden state of the t-th word in the sentence.\\nThis method is kind of similar to the sharing parameters between filters of convolutional neural network (Cun et al., 1990), except for it working on the time axis, which recognizes the local invariant along each time steps.\\nWith RNN pyramid built on a sentence, we can map all potential phrases with varying lengths into real-valued fix-length vectors. These vectors are representations of candidate structures we plan to compare at next stage.\\n**2.2 Utilize Phrases**\\nWith the candidate structures represented by a fixlength vector (Fig. 2), we can easily apply attention mechanism on these vectors, and soft combine them to output a weighted sum as the best structure selected:\\nŝt = ∑ t,n αt,nh n t−1 (6)\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nFigure 3: On a 4-word sentence, RNN pyramid (dashed line triangle) generated by 4 horizontal parallel RNNs, each begins at 1 of the 4 words in the sentence. Initial status are indicated by circles. Because hidden state is considered containing all history information. The set of all hidden status in the pyramid can be mapped one-to-one to the representation of all candidate phrases of the sentence.\\nWhere the weight of hidden state (structure) αt,n can be represented by the following form:\\nαt,n = exp(et,n)∑\\nt,n exp(et,n)\\n(7)\\nIn which we define et,n as:\\net,n = a(hk, ht,n) (8)\\nHere we combine a(hk, ht,n) with one layer of feedforward neural network, where hk is the k-th word of sequential part S.\\nWe adopt the attention mechanism from Bahdanau et. al. (2014). As we showed in Fig 4. We put ŝ into R part of the network, let the network to combine it with h and x to predict next hidden state. We also apply our model on machine translation task within successful EncoderDecoder framework as in Fig 5.\\nFigure 4: Combine best structure ŝ given by attention partA in each predicting step of sequential part S. To reduce calculation, we limit the candidate phrase set at each step to the newly generated ones (the blue rectangles with solid boundaries), which means to ignore structures generated at previous steps (grey ones with dashed boundaries), therefore reduce the scale of candidate phrases set from O(N2) to O(N). The Pyramid and Seq part share the same embedding in experiments, we draw them separately in the diagram just for clearance.\\n**3 LM Experiment**\\n\\n**3.1 Data**\\nTo make experiment comparable with other methods, we apply our models on language model task, evaluate it in perplexity on the widely used English Penn Treebank (PTB) (Marcus et al., 1993), which pre-processing and splitting by Mikolov (2010). The data is utilized as following: sections (0-20) with 929k tokens are used for training, sections (21-22) with 73k tokens are held out as validation, and sections (23-24) with 82k tokens are used for testing. There are only top 10,000 highfrequency words are kept in the corpus. All rest low-frequency words are replaced with UNK tag. This version of data is widely used among the language modeling community. It is publicly avail-\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nFigure 5: We use our pRNNs as the pyramid encoder to represent more structural information (all candidate phrases) of source sentences, just beside the original bi-direction encoder which only represents surface word sequence explictly. Then we join two context vectors from two encoders into a larger one. Then we allow the decoder to choose which portion of the larger context (which source words or candidate phrases) is more relevant to the next generated word of the target sentence. We adopt two settings of pyramid encoder, one takes only last status (the blue rectangles with solid boundaries) of each RNN as the input of attention part (src-pyr-last in Fig 4), the other takes all status (all rectangles including the grey ones with dashed boundaries) of all RNNs as the input of attention part (src-pyr-all in Fig 4).\\nable.1\\nBecause the scale of PTB corpus is relatively small, we also train our model on larger FBIS English(LDC2003E14). Accordingly, we only keep top 40,000 high-frequency words in the corpus, replace rest low-frequency word with UNK tag. We use NIST MT06 as the validation set, NIST MT08 as the test set.\\ntrain valid test FBIS MT06 MT08\\nSequences 219,280 6,560 5,424 Tokens 7,877,650 190,065 166,937 Types 49,210 8,476 9,576\\nTable 1: FBIS and NIST MT Corpus statistics.\\n**3.2 Model Configuration**\\nBaseline In this paper, we utilize state-of-theart LSTM framework on language model proposed by Zaremba (2014) as the baseline model. Firstly, we stack 2 layers of LSTMs, to explore more abstracted patterns which are not supposed to be discovered by a single layer. Secondly, to increase the model’s capability of noise tolerance and reduce the overfitting to training data, we introduce dropout (Hinton et al., 2012) before and after each recurrent layer. We choose dropout rate as 0.5 without tuning. For training, we used AdaDelta (Zeiler, 2012). To handle nan and inf which occasionally occur in gradient, we normalize the gradient of each batch to 1.0 and drop the parameter updates on such batches. To determine when to stop training, we set patience to 100. We set the dimension of both word embedding and hidden dimensions to 200. We initialized all parameters according to recommendations given in (Zaremba et al., 2014) and blocks(van Merriënboer et al., 2015). For all models, we used word embedding, hidden dimensions of 200 and 2-layer LSTMs. For both models, we choose dropout rate as 0.5. For training, we used AdaDelta and normalize the gradient to 1.0, set patience to 100. We initialized all parameters according to recommendations given in Zaremba et al. (2014).\\nPhrasal RNN We configure our model exactly as the baseline model, except adding an extra RNN\\n1http://www.fit.vutbr.cz/˜imikolov/ rnnlm/\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\npyramid layer above baseline’s 2-layer LSTM. We also add dropout between 2nd LSTM layer and RNN pyramid layer. We set dimension of hidden state in RNN pyramid layer as 200 either. We utilize Gated Recurrent Unit (GRU) (Chung et al., 2014) to construct RNN pyramid layer. We also tried a simplified version of GRU to build the pyramid (pRNNv in table 2), which achieves the best result.\\n**3.3 Results**\\nModel Perplexity 5-gram, KN5 141.2 FFNN-LM 140.2 RNN 124.7 LSTM 126 genCNN 116.4 LSTM (baseline) 106.9 pRNN 97.6 pRNNv 94.5\\nTable 2: Perplexity on PENN TREEBANK, where the top 5 rows of numbers are results reported in previous work, our baseline and new pRNN model are in last three rows.\\nModel Perplexity 5-gram, KN5 278.6 FFNN-LM(5-gram) 248.3 FFNN-LM(20-gram) 228.2 RNN 223.4 LSTM 206.9 genCNN 181.2 LSTM (baseline) 171.8 pRNN 161.5\\nTable 3: Perplexity on FBIS data set, where the top 5 rows of numbers are results reported in previous work, our baseline and new pRNN model are in last two rows.\\nWe report our perplexities result of language model in table (2 and 3). We calculate perplexity over a sequence [w1, . . . , wn] with\\nPPL = exp ( − log(Prob(w n 1 ))\\nn\\n) (9)\\n(including the end of sentence (EOS) symbol). pRNN and its variant outperform over 10 points of\\nppl over a strong baseline on both PTB and FBIS English data set.\\n**4 MT Experiment**\\n\\n**4.1 Data**\\nWe evaluate all three models, PBSMT, RNNsearch, pRNN on the same data set. We utilize 1.25M sentence pairs, which are extracted from LDC corpora as training data. There are 34.5 English words and 27.9M Chinese words in the training data. We select NIST 2002 (MT02) data set as our development set, the NIST 2003 (MT03), NIST 2004 (MT04), NIST 2005 (MT05), NIST 2006 (MT06) and NIST 2008 (MT08) as test sets. When training neural networks, we limit the size of vocabularies of both source and target side to the most frequent 16K words. All rest low-frequency words are replaced with UNK tag. Chinese vocabulary covers approximately 95.8% of the corpora. English vocabulary covers approximately 98.3% of the corpora.\\n**4.2 Model Configuration**\\nBaseline In this paper, we use an open-source implementation (Meng et al., 2015) of RNNsearch (Bahdanau et al., 2014) as baseline model. To increase the model’s capability of noise tolerance and reduce the overfitting to training data, we introduce dropout (Hinton et al., 2012) before softmax layer, and set the dropout rate equal to 0.5. We choose dropout rate as 0.5 without tuning. For training, we used AdaDelta (Zeiler, 2012) and normalize the gradient to 1.0, . To handle nan and inf which occasionally occur in gradient, we normalize the gradient of each batch to 1.0 and drop the parameter updates on such batches. We set the dimension of both source and target word embedding as 620, and hidden dimensions to 1000. We initialized all parameters according to recommendations given in (Bahdanau et al., 2014). We also introduce the phrase-based model of Moses (Koehn et al., 2007) as a secondary baseline too.\\nPhrase-based NMT We configure our model exactly as the baseline model, except adding an extra RNN pyramid as a secondary source encoder. As the limitation on the memory of GPU, we keep only phrases ended at eos into consideration. Thus we name it src-pyr-last in table 4. We set the dimension of hidden dimensions inside pyramid as 1000 too. We utilize Gated Recur-\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nModels MT02 MT03 MT04 MT05 MT06 MT08 Test Avg. Diff moses 33.41 31.61 33.48 30.75 31.07 23.37 30.056 +0.754 RNNsearch (groundhog) 32.32 29.02 31.25 28.32 27.99 20.29 27.374 -1.928 RNNsearch (baseline) 34.28 30.61 33.24 30.66 29.83 22.17 29.302 +0.000 pRNN (src-pyr-last) 35.48 31.61 34.40 31.96 31.36 22.82 30.430 +1.128 pRNN (src-pyr-all) 35.49 32.08 34.51 31.81 30.91 22.86 30.434 +1.132\\nTable 4: BLEU score on 1.25M training corpus with 16k dictionary on both source and target side. The above two lines of the table are results of open-source machine translation systems. Bold numbers indicate the best results on the data set (column). pRNNs are better than original RNNsearch model baseline (in-house reimplemented). We find it is interesting that results of src-pyr-all are only slightly better than src-pyr-last, we guess this is due to the limited discriminative power of simple attention mechanism when meeting large number of complex candidates.\\nrent Unit (GRU) (Chung et al., 2014) to construct RNN pyramid layer.\\nFor a fair comparison, we run baseline system (RNNsearch) many times (not epoch) and report only the best one. We only run PBNMT once.\\n**4.3 Results**\\nWe report our BLEU result of three models in table (4). We use the case-insensitive 4-gram NIST BLEU (Papineni et al., 2002) score given by mteval v11.pl pRNNs outperforms both PBSMT and Encoder-Decoder model.\\n**5 Related Work**\\n\\n**5.1 Relation to Previous Attempts on**\\nStructural Information\\nIn the last two decades, to achieve better performance, researchers incorporated more and more rich structural information into conventional model (Jelinek and Lafferty, 1991; Chelba, 1997; Chelba and Jelinek, 2000; Emami and Jelinek, 2005). This trend is more clear in statistical machine translation (SMT) community, from wordbased SMT (Brown et al., 1993) to phrase-based SMT (Koehn et al., 2003), hierarchical phrasebased SMT (Chiang, 2005) and syntax-based SMT (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Xie et al., 2011) (forest-based (Mi and Huang, 2008; Mi et al., 2008)). among them, phrase-based SMT (Zens et al., 2002; Koehn et al., 2003) was the most widely adopted translation model. The Same trend can be observed in neural network strand. There were many works which successfully modeled structure in neural network on parsing (Dyer et al., 2016; Emami and Jelinek, 2005; Henderson, 2004; Titov and Hender-\\nson, 2007; Buys and Blunsom, 2015) or language modeling tasks (Dyer et al., 2016; Chelba and Jelinek, 2000; Emami and Jelinek, 2005; Chelba, 1997).\\nIn neural machine translation (NMT) area, the situation is more complex. In recent years, the most popular and success NMT model is EncoderDecoder model (Bahdanau et al., 2014; Sutskever et al., 2014). Which has achieved competitive or better results in many translation tasks(Luong et al., 2015a,b). However, beyond sequential surface words, there are latent nest structures in natural language (Chomsky, 1957). One direction to explore is to introduce sub-word structures(CostaJussà and Fonollosa, 2016; Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015), such as characters. The most important reason to dig into sub-word is to handle out-of-vocabulary word problem. This problem is rooted in the limited size of vocabulary, which utilized by NMT mapping symbols to real-valued dense vector.\\nAnother direction is to explore hyper-word structure. Authors of (Eriguchi et al., 2016) adopted parsing tree in encoder phase. However, this method depends heavily on human-labeled data, which is always expensive and limited in scale. Authors of (Stahlberg et al., 2016) introduce hierarchical phrase-based (HPB) model as the guider of search space. However, the HPB model and NMT model are trained separately and combined only when decoding. Compare this to the previous method, (Eriguchi et al., 2016) can be categorized into introducing external data, (Stahlberg et al., 2016) can be categorized into introducing external model. In an ideal situation, all external model can be replaced by a neural net-\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nwork with equal ability.\\n**5.2 Similarity to Deep Memory Network**\\nDeep Memory Network is an effective implementation of the neural turing machine. When they use state machine such as GRU or LSTM to read from one memory and write to another, the memory IO addresses are either content-based (via attention mechanism) or location-based (actually sequentially cell-by-cell) (Meng et al., 2015). However, there are much more other methods in the location-based category.\\nCell-by-cell vs. Incremental If we consider the hidden of one time step inside pyramid RNN as memory, we can name the operations as incremental read and write. The intuition behind incremental addressing is, when we read little, we know little, we only have the ability to write little. But when we read more, we know more, we are gone to have the ability to write more.\\n**6 Discussion**\\nOur experiments clearly show that the proposed pRNN model is quite effective in language modeling and machine translation. This is the because of:\\n• when model predicting, it is provided with all candidate phrases as structure information rather than just surface sequential words.\\n• utilizing attention mechanism to compare and combine to get the weighted sum which best fit for predicting next hidden state.\\nThe most significant question that remains is how well the quality of forest generated as a by-product of pRNN, will it get a better result than other supervised parsing model trained on human label data.\\n   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         **2 Recurrent Models for Text Classification**\\nThere are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks.\\nLong Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the issue of learning long-term dependencies. While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al., 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.\\nWe define the LSTM units at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. d is the number of the LSTM units. The elements of the gating vectors it, ft and ot are in [0, 1].\\nThe LSTM is precisely specified as follows.\\n c̃t ot it ft  =  tanh σ σ σ (Wp [ xtht−1 ] + bp ) , (1)\\nct = c̃t ⊙ it + ct−1 ⊙ ft, (2) ht = ot ⊙ tanh (ct) , (3)\\nwhere xt ∈ Re is the input at the current time step; Wp ∈ R4d×(d+e) and bp ∈ R4d are parameters of affine transformation; σ denotes the logistic sigmoid function and ⊙ denotes elementwise multiplication. The update of each LSTM unit can be written precisely as follows:\\nht = LSTM(ht−1,xt, θp). (4)\\nHere, the function LSTM(·, ·, ·, ·) is a shorthand for Eq. (1-3), and θp represents all the parameters of LSTM.\\nText Classification with LSTM Given a text sequence x = {x1, x2, · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi. The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes.\\nŷ = softmax(WhT + b) (5)\\nwhere ŷ is prediction probabilities, W is the weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi, yi), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions.\\nL(ŷ, y) = − N∑ i=1 C∑ j=1 yji log(ŷ j i ), (6)\\nwhere yji is the ground-truth label; ŷ j i is prediction probabilities, and C is the class number.\\n**3 Multi-task Learning for Text Classification**\\nThe goal of multi-task learning is to utilizes the correlation among these related tasks to improve\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\nsoftmax Lmtask\\nLSTM\\nsoftmax Lntask\\nxm xn\\n(a) Fully Shared Model (FS-MTL)\\nxm\\nxn\\nLSTM\\nLSTM\\nLSTM\\nsoftmax\\nsoftmax\\nLmtask\\nLntask\\n(b) Shared-Private Model (SP-MTL)\\nFigure 2: Two architectures for learning multiple tasks. Yellow and gray boxes represent shared and private LSTM layers respectively.\\nclassification by learning tasks in parallel. To facilitate this, we give some explanation for notations used in this paper. Formally, we refer to Dk as a dataset with Nk samples for task k. Specifically,\\nDk = {(xki , yki )} Nk i=1 (7)\\nwhere xki and y k i denote a sentence and corresponding label for task k.\\n**3.1 Two Sharing Schemes for Sentence Modeling**\\nThe key factor of multi-task learning is the sharing scheme in latent feature space. In neural network based model, the latent features can be regarded as the states of hidden neurons. Specific to text classification, the latent features are the hidden states of LSTM at the end of a sentence. Therefore, the sharing schemes are different in how to group the shared features. Here, we first introduce two sharing schemes with multi-task learning: fully-shared scheme and shared-private scheme.\\nFully-SharedModel (FS-MTL) In fully-shared model, we use a single shared LSTM layer to extract features for all the tasks. For example, given two tasks m and n, it takes the view that the features of taskm can be totally shared by task n and vice versa. This model ignores the fact that some features are task-dependent. Figure 2a illustrates the fully-shared model.\\nShared-Private Model (SP-MTL) As shown in Figure 2b, the shared-private model introduces two feature spaces for each task: one is used to store task-dependent features, the other is used\\nto capture task-invariant features. Accordingly, we can see each task is assigned a private LSTM layer and shared LSTM layer. Formally, for any sentence in task k, we can compute its shared representation skt and task-specific representation h k t as follows:\\nskt = LSTM(xt, s k t−1, θs), (8)\\nhkt = LSTM(xt,h m t−1, θk) (9)\\nwhere LSTM(., θ) is defined as Eq. (4). The final features are concatenation of the features from private space and shared space.\\n**3.2 Task-Specific Output Layer**\\nFor a sentence in task k, its feature h(k), emitted by the deep muti-task architectures, is ultimately fed into the corresponding task-specific softmax layer for classification or other tasks. The parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions on all the tasks. The loss Ltask can be computed as:\\nLTask = K∑ k=1 αkL(ŷ (k), y(k)) (10)\\nwhere αk is the weights for each task k respectively. L(ŷ, y) is defined as Eq. 6.\\n**4 Incorporating Adversarial Training**\\nAlthough the shared-private model separates the feature space into the shared and private spaces, there is no guarantee that sharable features can not exist in private feature space, or vice versa. Thus, some useful sharable features could be ignored in shared-private model, and the shared feature space is also vulnerable to contamination by some taskspecific information. Therefore, a simple principle can be applied into multi-task learning that a good shared feature space should contain more common information and no task-specific information. To address this problem, we introduce adversarial training into multi-task framework as shown in Figure 3 (ASPMTL).\\n**4.1 Adversarial Network**\\nAdversarial networks have recently surfaced and are first used for generative model (Goodfellow\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nxm\\nxn\\nLSTM\\nLSTM\\nLSTM\\nLDiff LAdvLDiff\\nsoftmax\\nsoftmax\\nLmtask\\nLntask\\nFigure 3: Adversarial shared-private model. Yellow and gray boxes represent shared and private LSTM layers respectively.\\net al., 2014). The goal is to learn a generative distribution pG(x) that matches the real data distribution Pdata(x) Specifically, GAN learns a generative network G and discriminative model D, in which G generates samples from the generator distribution pG(x). and D learns to determine whether a sample is from pG(x) or Pdata(x). This min-max game can be optimized by the following risk:\\nϕ = min G max D\\n( Ex∼Pdata [logD(x)]\\n+ Ez∼p(z)[log(1−D(G(z)))] )\\n(11)\\nWhile originally proposed for generating random samples, adversarial network can be used as a general tool to measure equivalence between distributions (Taigman et al., 2016). Formally, (Ajakan et al., 2014) linked the adversarial loss to the H-divergence between two distributions and successfully achieve unsupervised domain adaptation with adversarial network. Motivated by theory on domain adaptation (Ben-David et al., 2010, 2007; Bousmalis et al., 2016) that a transferable feature is one for which an algorithm cannot learn to identify the domain of origin of the input observation.\\n**4.2 Task Adversarial Loss for MTL**\\nInspired by adversarial networks (Goodfellow et al., 2014), we proposed an adversarial sharedprivate model for multi-task learning, in which a shared recurrent neural layer is working adversarially towards a learnable multi-layer perceptron, preventing it from making an accurate prediction about the types of tasks. This adversarial training encourages shared space to be more pure and ensure the shared representation not be contaminated by task-specific features.\\nTask Discriminator Discriminator is used to map the shared representation of sentences into a\\nprobability distribution, estimating what kinds of tasks the encoded sentence comes from.\\nD(skT , θD) = softmax(b+Us k T ) (12)\\nwhereU ∈ Rd×d is a learnable parameter and b ∈ Rd is a bias.\\nAdversarial Loss Different with most existing multi-task learning algorithm, we add an extra task adversarial loss LAdv to prevent task-specific feature from creeping in to shared space. The task adversarial loss is used to train a model to produce shared features such that a classifier cannot reliably predict the task based on these features. The original loss of adversarial network is limited since it can only be used in binary situation. To overcome this, we extend it to multi-class form, which allow our model can be trained together with multiple tasks:\\nLAdv = min θs\\n( λmax\\nθD ( K∑ k=1 Nk∑ i=1 dki log[D(E(x k))])\\n) (13)\\nwhere dki denotes the ground-truth label indicating the type of the current task. Here, there is a minmax optimization and the basic idea is that, given a sentence, the shared LSTM generates a representation to mislead the task discriminator. At the same time, the discriminator tries its best to make a correct classification on the type of task. After the training phase, the shared feature extractor and task discriminator reach a point at which both cannot improve and the discriminator is unable to differentiate among all the tasks.\\nSemi-supervised Learning Multi-task Learning We notice that the LAdv requires only the input sentence x and does not require the corresponding label y, which makes it possible to combine our model with semi-supervised learning. Finally, in this semi-supervised multi-task learning framework, our model can not only utilize the data from related tasks, but can employ abundant unlabeled corpora.\\n**4.3 Orthogonality Constraints**\\nWe notice that there is a potential drawback of the above model. That is, the task-invariant features can appear both in shared space and private space. Motivated by recently work(Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nDataset Train Dev. Test Unlab. Avg. L Vocab.\\nBooks 1400 200 400 2000 159 62K Elec. 1398 200 400 2000 101 30K DVD 1400 200 400 2000 173 69K Kitchen 1400 200 400 2000 89 28K Apparel 1400 200 400 2000 57 21K Camera 1397 200 400 2000 130 26K Health 1400 200 400 2000 81 26K Music 1400 200 400 2000 136 60K Toys 1400 200 400 2000 90 28K Video 1400 200 400 2000 156 57K Baby 1300 200 400 2000 104 26K Mag. 1370 200 400 2000 117 30K Soft. 1315 200 400 2000 129 26K Sports 1400 200 400 2000 94 30K IMDB 1400 200 400 2000 269 44K MR 1400 200 400 2000 21 12K\\nTable 1: Statistics of the 16 datasets. The columns 2-5 denote the number of samples in training, development, test and unlabeled sets. The last two columns represent the average length and vocabulary size of corresponding dataset.\\non shared-private latent space analysis, we introduce orthogonality constraints, which penalize redundant latent representations and encourages the shared and private extractors to encode different aspects of the inputs.\\nAfter exploring many optional methods, we find below loss is optimal, which is used by Bousmalis et al. (2016) and achieve a better performance:\\nLdiff = K∑ k=1 ∥∥∥Sk⊤Hk∥∥∥2 F , (14)\\nwhere ∥ · ∥2F is the squared Frobenius norm. S k and Hk are two matrics, whose rows are the output of shared extractor Es(, ; θs) and task-specific extrator Ek(, ; θk) of a input sentence.\\n**4.4 Put It All Together**\\nThe final loss function of our model can be written as:\\nL = LTask + λLAdv + γLDiff (15)\\nwhere λ and γ are hyper-parameter. The networks are trained with backpropagation and this minimax optimization becomes possible via the use of a gradient reversal layer (Ganin and Lempitsky, 2015).\\n**5 Experiment**\\n\\n**5.1 Dataset**\\nTo make an extensive evaluation, we collect 16 different datasets from several popular review corpora. The first 14 datasets are product reviews, which contain Amazon product reviews from different domains, such as Books, DVDs, Electronics, ect. The goal is to classify a product review as either positive or negative. These datasets are collected based on the raw data 1 provided by (Blitzer et al., 2007). Specifically, we extract the sentences and corresponding labels from the unprocessed original data 2. The only preprocessing operation of these sentences is tokenized using the Stanford tokenizer 3. The remaining two datasets are about movie reviews. The IMDB dataset4 consists of movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. The MR dataset also consists of movie reviews from rotten tomato website with two classes 5(Pang and Lee, 2005). All the datasets in each task are partitioned randomly into training set, development set and testing set with the proportion of 70%, 20% and 10% respectively. The detailed statistics about all the datasets are listed in Table 1.\\n**5.2 Competitor Methods for Multi-task Learning**\\nThe multi-task frameworks proposed by previous works are various while not all can be applied to the tasks we focused. Nevertheless, we chose two most related neural models for multi-task learning and implement them as competitor methods.\\n• MT-CNN: This model is proposed by Collobert and Weston (2008) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific.\\n1https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/\\n2Blitzer et al. (2007) also provides two extra processed datasets with the format of Bag-of-Words, which are not proper for neural-based models.\\n3http://nlp.stanford.edu/software/ tokenizer.shtml\\n4https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/unprocessed.tar.gz\\n5https://www.cs.cornell.edu/people/ pabo/movie-review-data/.\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nTask Single Task Multiple Tasks\\nLSTM BiLSTM sLSTM Avg. MT-DNN MT-CNN FS-MTL SP-MTL ASP-MTL\\nBooks 20.5 19.0 18.0 19.2 17.7(−1.5) 15.6(−3.6) 17.5(−1.7) 18.7(−0.5) 13.0(−6.2) Electronics 19.5 21.5 23.3 21.4 18.2(−3.2) 16.9(−4.5) 14.3(−7.1) 12.3(−9.1) 11.0(−10.4) DVD 18.3 19.5 22.0 19.9 15.8(−4.1) 16.1(−3.8) 16.5(−3.4) 16.1(−3.8) 12.6(−7.3) Kitchen 22.0 18.8 19.5 20.1 19.2(−0.9) 16.8(−3.3) 14.0(−6.1) 14.8(−5.3) 12.8(−7.3) Apparel 16.8 14.0 16.3 15.7 14.9(−0.8) 16.1(+0.4) 15.5(−0.2) 13.4(−2.3) 11.3(−4.4) Camera 14.8 14.0 15.0 14.6 13.7(−0.9) 14.0(−0.6) 13.5(−1.1) 12.1(−2.5) 8.7(−5.9) Health 15.5 21.3 16.5 17.8 14.3(−3.5) 12.9(−4.9) 12.0(−5.8) 12.8(−5.0) 10.9(−6.9) Music 23.3 22.8 23.0 23.0 15.3(−7.7) 16.3(−6.7) 18.8(−4.2) 17.0(−6.0) 17.4(−5.6) Toys 16.8 15.3 16.8 16.3 12.1(−4.2) 10.9(−5.4) 15.5(−0.8) 14.9(−1.4) 11.2(−5.1) Video 18.5 16.3 16.3 17.0 15.0(−2.0) 18.7(+1.7) 16.3(−0.7) 16.8(−0.2) 14.5(−2.5) Baby 15.3 16.5 15.8 15.9 12.1(−3.8) 12.4(−3.5) 12.0(−3.9) 13.2(−2.7) 10.2(−5.7) Magazines 10.8 8.5 12.3 10.5 10.6(+0.1) 12.3(+1.8) 7.5(−3.0) 8.1(−2.4) 7.6(−2.9) Software 15.3 14.3 14.5 14.7 14.4(−0.3) 13.4(−1.3) 13.8(−0.9) 13.1(−1.6) 12.7(−2.0) Sports 18.3 16.0 17.5 17.3 16.8(−0.5) 16.1(−1.2) 14.5(−2.8) 12.7(−4.6) 13.3(−4.0) IMDB 18.3 15.0 18.5 17.3 16.7(−0.6) 13.7(−3.6) 17.5(+0.2) 15.2(−2.1) 14.2(−3.1) MR 27.3 25.3 28.0 26.9 24.5(−2.4) 25.5(−1.4) 25.3(−1.6) 24.1(−2.8) 22.7(−4.2)\\nAVG 18.2 17.4 18.3 18.0 15.7(−2.3) 15.5(−2.5) 15.3(−2.7) 14.7(−3.3) 12.8(−5.2)\\nTable 2: Error rates of our models on 16 datasets against typical baselines. The numbers in brackets represent the improvements relative to the average performance (Avg.) of three single task baselines.\\n• MT-DNN: The model is proposed by Liu et al. (2015) with bag-of-words input and multi-layer perceptrons, in which a hidden layer is shared.\\n**5.3 Hyperparameters**\\nThe word embeddings for all of the models are initialized with the 200d GloVe vectors (840B token version, (Pennington et al., 2014)). The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. The minibatch size is set to 16.\\nFor each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.1, 0.01], λ ∈ [0.01, 0.1], and γ ∈ [0.01, 0.1]. Finally, we chose the learning rate as 0.01, λ as 0.05 and γ as 0.01.\\n**5.4 Performance Evaluation**\\nTable 2 shows the error rates on 16 text classification tasks. The column of “Single Task” shows the results of vanilla LSTM, bidirectional LSTM (BiLSTM), stacked LSTM (sLSTM) and the average error rates of previous three models. The column of “Multiple Tasks” shows the results achieved by corresponding multi-task models. From this table, we can see that the performance of most tasks can be improved with a large margin with the help of multi-task learning, in which our model achieves the lowest error rates.\\nMore concretely, compared with SP-MTL, ASPMTL achieves 5.2% average improvement surpassing SP-MTL with 1.9%, which indicates the importance of adversarial learning. It is noteworthy that for FS-MTL, the performances of some tasks are degraded, since this model puts all private and shared information into a unified space.\\n**5.5 Shared Knowledge Transfer**\\nWith the help of adversarial learning, the shared feature extractor Es can generate more pure taskinvariant representations, which can be considered as off-the-shelf knowledge and then be used for unseen new tasks. To test the transferability of our learned shared extractor, we also design an experiment, in which we take turns choosing 15 tasks to train our model MS with multi-task learning, then the learned shared layer are transferred to a second network MT that is used for the remaining one task. The parameters of transferred layer are kept frozen, and the rest of parameters of the network MT are randomly initialized. More formally, we investigate two mechanisms towards the transferred shared extractor. As shown in Figure 4. The first one Single Channel (SC) model consists of one shared feature extractor Es from MS , then the extracted representation will be sent to an output layer. By contrast, the BiChannel (BC) model introduces an extra LSTM layer to encode more task-specific information. To evaluate the effectiveness of our introduced adver-\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nSource Tasks Single Task Transfer Models\\nLSTM BiLSTM sLSTM Avg. SP-MTL-SC SP-MTL-BC ASP-MTL-SC ASP-MTL-BC\\nϕ (Books) 20.5 19.0 18.0 19.2 17.8(−3.6) 16.2(−3.0) 16.7(−2.5) 13.3(−5.9) ϕ (Electronics) 19.5 21.5 23.3 21.4 15.1(−4.5) 14.6(−6.8) 15.1(−6.3) 14.9(−6.5) ϕ (DVD) 18.3 19.5 22.0 19.9 14.7(−3.8) 15.5(−4.4) 12.1(−7.8) 12.4(−7.5) ϕ (Kitchen) 22.0 18.8 19.5 20.1 15.0(−3.3) 16.6(−3.5) 14.6(−5.5) 14.1(−6.0) ϕ (Apparel) 16.8 14.0 16.3 15.7 14.9(+0.4) 12.3(−3.4) 11.6(−4.1) 13.6(−2.1) ϕ (Camera) 14.8 14.0 15.0 14.6 13.1(−0.6) 12.1(−2.5) 11.6(−3.0) 10.3(−4.3) ϕ (Health) 15.5 21.3 16.5 17.8 14.1(−4.9) 14.2(−3.6) 12.2(−5.6) 10.5(−7.3) ϕ (Music) 23.3 22.8 23.0 23.0 19.9(−6.7) 17.9(−5.1) 16.4(−6.6) 18.2(−4.8) ϕ (Toys) 16.8 15.3 16.8 16.3 13.8(−5.4) 12.2(−4.1) 13.0(−4.7) 11.2(−5.1) ϕ (Video) 18.5 16.3 16.3 17.0 14.2(+1.7) 15.1(−1.9) 14.8(−2.2) 14.8(−2.2) ϕ (Baby) 15.3 16.5 15.8 15.9 16.6(−3.5) 16.9(+1.0) 11.5(−4.4) 10.0(−5.9) ϕ (Magazines) 10.8 8.5 12.3 10.5 10.6(+1.8) 10.2(−0.3) 8.6(−1.9) 9.7(−0.8) ϕ (Software) 15.3 14.3 14.5 14.7 13.0(−1.3) 12.7(−2.0) 14.3(−0.4) 11.1(−3.6) ϕ (Sports) 18.3 16.0 17.5 17.3 16.3(−1.2) 16.2(−1.1) 13.4(−3.9) 13.6(−3.7) ϕ (IMDB) 18.3 15.0 18.5 17.3 12.4(−3.6) 12.8(−4.5) 12.5(−4.8) 13.3(−4.0) ϕ (MR) 27.3 25.3 28.0 26.9 26.0(−1.4) 26.5(−0.4) 22.7(−4.2) 23.5(−3.4)\\nAVG 18.2 17.4 18.3 18.0 15.5(−2.5) 15.1(−2.9) 13.6(−4.2) 13.4(−4.6)\\nTable 3: Error rates of our models on 16 datasets against vanilla multi-task learning. ϕ (Books) means that we transfer the knowledge of the other 15 tasks to the target task Books.\\nxt LSTM softmax\\nEs\\n(a) Single Channel\\nxt LSTM\\nLSTM\\nsoftmax\\nEs\\n(b) Bi-Channel\\nFigure 4: Two transfer strategies using a pretrained shared LSTM layer. Yellow box denotes shared feature extractor Es trained by 15 tasks.\\nsarial training framework, we also make a comparison with vanilla multi-task learning method.\\nResults and Analysis As shown in Table 3, we can see the shared layer from ASP-MTL achieves a better performance compared with SP-MTL. Besides, for the two kinds of transfer strategies, the Bi-Channel model performs better. The reason is that the task-specific layer introduced in the BiChannel model can store some private features. Overall, the results indicate that we can save the existing knowledge into a shared recurrent layer using adversarial multi-task learning, which is quite useful for a new task.\\n**5.6 Visualization**\\nTo get an intuitive understanding of how the introduced orthogonality constraints worked compared with vanilla shared-private model, we design an experiment to examine the behaviors of neurons from private layer and shared layer. More concretely, we refer to htj as the activation of the j-\\nneuron at time step t, where t ∈ {1, . . . , n} and j ∈ {1, . . . , d}. By visualizing the hidden state hj and analyzing the maximum activation, we can find what kinds of patterns the current neuron focuses on. Figure 5 illustrates this phenomenon. Here, we randomly sample a sentence from the validation set of Baby task and analyze the changes of the predicted sentiment score at different time steps, which are obtained by SP-MTL and our proposed model. Additionally, to get more insights into how neurons in shared layer behave diversely towards different input word, we visualize the activation of two typical neurons. For the positive sentence “Five stars, my baby can fall asleep soon in the stroller”, both models capture the informative pattern “Five stars” 6. However, SP-MTL makes a wrong prediction due to misunderstanding of the word “asleep”. By contrast, our model makes a correct prediction and the reason can be inferred from the activation of Figure 5-(b), where the shared layer of SP-MTL is so sensitive that many features related to other tasks are included, such as ”asleep”, which misleads the final prediction. This indicates the importance of introducing adversarial learning to prevent the shared layer from being contaminated by task-specific features.\\n6For this case, the vanilla LSTM also give a wrong answer due to ignoring the feature “Five stars”.\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nFive stars , my baby can fall asleep soon in the stroller 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 SP-MTL Ours\\n(a) Predicted Sentiment Score by Two Models (b) Behaviours of Neuron hs18 and h s 21\\nFigure 5: (a) The change of the predicted sentiment score at different time steps. Y-axis represents the sentiment score, while X-axis represents the input words in chronological order. The darker grey horizontal line gives a border between the positive and negative sentiments. (b) The blue heat map describes the behaviour of neuron hs18 from shared layer of SP-MTL, while the purple one is used to show the behaviour of neuron hs21, which belongs to the shared layer of our model.\\nModel Shared Layer Task-Movie Task-Baby\\nSP-MTL\\ngood, great bad, love, simple, cut, slow, cheap, infantile good, great, well-directed, pointless, cut, cheap, infantile love, bad, cute, safety, mild, broken simple\\nASP-MTL good, great, love, bad poor well-directed, pointless, cut, cheap, infantile cute, safety, mild, broken simple\\nTable 4: Typical patterns captured by shared layer and task-specific layer of SP-MTL and ASP-MTL models on Movie and Baby tasks.\\nWe also list some typical patterns captured by neurons from shared layer and task-specific layer in Table 4, and we have observed that: 1) for SP-MTL, if some patterns are captured by taskspecific layer, they are likely to be placed into shared space. Clearly, suppose we have many tasks to be trained jointly, the shared layer bear much pressure and must sacrifice substantial amount of capacity to capture the patterns they actually do not need. Furthermore, some typical taskinvariant features also go into task-specific layer. 2) for ASP-MTL, we find the features captured by shared and task-specific layer have a small amount of intersection, which allows these two kinds of layers can work effectively.\\n**6 Related Work**\\nThere are two threads of related work. One thread is multi-task learning with neural network. Neural networks based multi-task learning has been proven effective in many NLP problems (Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2015, 2016). In most of these models, the\\nlower layers are shared across all tasks, while top layers are task-specific. These work has potential limitation of just learning a shared space solely on sharing parameters, while our model introduce two strategies to learn the clear and non-redundant shared-private space. Another thread of work is adversarial network. Adversarial networks have recently surfaced as a general tool measure equivalence between distributions and it has proven to be effective in a variety of tasks. Ajakan et al. (2014); Bousmalis et al. (2016) applied adverarial training to domain adaptation, aiming at transferring the knowledge of one source domain to target domain. Park and Im (2016) proposed a novel approach for multimodal representation learning which uses adversarial back-propagation concept. Different from these models, our model aims to find task-invariant sharable information for multiple related tasks using adversarial training strategy. Moreover, we extend binary adversarial training to multi-class, which enable multiple tasks to be jointly trained.\\n   \n",
            "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 **2 Related Work**\\nGrounding and Reference An early example for work in REG that goes beyond Dale and Reiter (1995)’s dominant symbolic paradigm is Deb Roy’s work from the early 2000s (Roy et al., 2002; Roy, 2002, 2005). More recently, research on REG, which has traditionally been done on small toy data sets, is being scaled up to realworld images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015). In this paper, we focus on a particular problem posed by REG on real-world images,\\nnamely generating the appropriate head noun for a given object. Similarly, Ordonez et al. (2016) have studied the problem of deriving appropriate object names, or so-called entry-level categories, from the output of an object recognizer. Their approach focusses links abstract object categories in ImageNet to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction.\\nMulti-modal and cross-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016). Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. More related to our work are cross-modal mapping models (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014), that learn to transfer a representation of an object or image in the visual space to a vector in a distributional space. When tested on standard object recognition tasks, transfer, however, comes at a price. Frome et al. (2013) and Norouzi et al. (2013) both find that it slightly degrades performance as compared to a plain object classification using standard accuracy metrics (called flat “hit @k metric” in their paper). Interestingly though, Frome et al. (2013) report better performance using “hierarchical precision”, which essentially means that transfer predicts words that are ontologically closer to the gold label and makes “semantically more reasonable errors”. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors observed during training (Lazaridou et al., 2015a). In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning.\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\n**3 Task and Data**\\nWe define object naming as follows: Given an object x in an image, the task is to predict a word w that could be used as the head noun of a realistic referring expression. (Cf. discussion above: “bird” when naming a robin, but “penguin” when naming a penguin.) To get at this, we develop our approach using a corpus of referring expressions produced by human users under natural, interactive conditions (Kazemzadeh et al., 2014), and train and test on the corresponding head nouns in these REs. This is similar to picture naming setups used in psycholinguistic research (cf. Levelt et al. (1991)) and based on the simplifying assumption that the name used for referring to an object can be determined successfully without looking at other objects in the image.\\nWe now summarise the details of our setup:\\nCorpus We train and test on the REFERIT corpus (Kazemzadeh et al., 2014), which is based on the SAIAPR image collection (Grubinger et al., 2006) (99.5k image regions;120K REs). We follow (Schlangen et al., 2016) and select words with a minimum frequency of 40 in these two data sets, which gives us a vocabulary of 793 words.\\nNames For most of our experiments, we only use a subset of this vocabulary, namely the set of object names. As the REs contain nouns that cannot be considered to be names (background, bottom, etc.), we extract from the semantically annotated portion of the REFERIT corpus a list of names which correspond to ‘entry-level’ nouns in terms of (Kazemzadeh et al., 2014). This gives us a list of 159 names. Thus, our experiments are on a smaller scale as compared to (Ordonez et al., 2016). Nevertheless, the data is challenging, as the corpus contains references to objects that fall outside of the object labeling scheme that available object recognition systems are typically optimized for, cf. Hu et al. (2015)’s discussion on “stuff” entities such “sky” or “grass” in the REFERIT data. For testing, we remove relational REs (containing a relational preposition such as ‘left of X’), because here we cannot be sure that the head noun of the target is fully informative; we also remove REs with more than one head noun from our list (i.e. these are mostly relational expressions as well such as ‘girl laughing at boy’). We pair each image region from the test set with its corresponding names from the remaining REs.\\nImage and Word Embeddings Following Schlangen et al. (2016), we derive representations of our visual inputs with a convolutional neural network, ‘GoogleNet’ (Szegedy et al., 2015), which was trained on the ImageNet corpus (Deng et al., 2009), and extract the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region. We add 7 features that encode information about the region relative to the image, thus representing each object as a vector of 1031 features. As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions).\\n**4 Three Models of Interfacing Visual and**\\nDistributional Information\\n**4.1 Direct Cross-Modal Mapping**\\nFollowing e.g. Lazaridou et al. (2014), referential meaning can be represented as a translation function that projects visual representations of objects to linguistic representations of words in a distributional vector space. Thus, in contrast to standard object recognition systems or the other models we will use here, cross-modal mapping does not treat words as individual labels or classifiers, but learns to directly predict continuous representations of words in a vector space, such as the space defined by the word2vec embeddings that we use in this work. This model will be called TRANSFER below.\\nDuring training, we pair each object with the distributional embedding of its name, and use standard Ridge regression for learning the transformation. Lazaridou et al. (2014) and Lazaridou et al. (2015a) test a range of technical tweaks and different algorithms for cross-modal mapping. For ease of comparison with other models, we stick with simple Ridge Regression in this work.\\nFor decoding, we map an object into the distributional space, and retrieve the nearest neighbors of the predicted vector using cosine similarity. In theory, the model should generalize easily to words that it has not observed in a pair with an object during training as it can map an object anywhere in the distributional space.\\n**4.2 Lexical Mapping Through Individual Word Classifiers**\\nAnother approach is to keep visual and distributional information separate, by training a separate visual classifier for each word w in the vocabu-\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nlary. Predictions can then be mapped into distributional space during application time via the vectors of the predicted words. Here, we use Schlangen et al. (2016)’s WAC model, building the training set for each word w as follows: all visual objects in a corpus that have been referred to as w are used as positive instances, the remaining objects as negative instances. Thus, the classifiers learn to predict referential appropriateness for individual words based on the visual features of the objects they refer to, in isolation of other words.\\nDuring decoding, we apply all word classifiers from the model’s vocabulary to the given object, and take the argmax over the individual word probabilities. The model can be used to predict names directly, without links into a distributional space.\\nIn order to extend the model’s vocabulary for zero-shot learning, we follow Norouzi et al. (2013) and associate the top n words with their corresponding distributional vector and compute the convex combination of these vectors. Then, in parallel to cross-modal mapping, we retrieve the nearest neighbors of the combined embedding from the distributional space. Thus, with this model, we use two different modes of decoding: one that projects into distributional space, one that only applies the available word classifiers.\\n**4.3 Word Prediction via Cross-Modal Similarity Mapping**\\nFinally, we implement an approach that combines ideas from cross-modal mapping with the WAC model: we train individual predictors for each word in the vocabulary, but, during training, we exploit lexical similarity relations encoded in a distributional space. Instead of treating a word as a binary classifier, we annotate its training instances with a fine-grained similarity signal according to their object names. When building the training set for such a word predictor w, instead of simply dividing objects into w and ¬w instances, we label each object with a real-valued similarity obtained from cosine similarity between w and v in a distributional vector space, where v is the word that was used to refer to the object. Thus, we task the model with jointly learning similarities and referential appropriateness, by training it with Ridge regression on a continuous output space. Object instances where v = w (i.e., the positive instances in the binary setup) have maximal similarity; the\\nremaining instances have a lower value which is more or less close to maximal similarity. This is the SIM-WAP model, recently proposed in (Anonymous).\\nImportantly, and going beyond (Anonymous), this model allows for an innovative treatment of words that only exist in a distributional space (without being paired with visual referents in the image corpus): as the predictors are trained on a continuous output space, no genuine positive instances of a word’s referent are needed. When training a predictor for such a word w, we use all available objects from our corpus and annotate them with the expected lexical similarity between w and the actual object names v, which for all objects will be below the maximal value that marks genuine positive instances. During decoding, this model does not need to project its predictions into a distributional space, but it simply applies all available predictors to the object, and takes the argmax over the predicted referential appropriateness scores.\\n**5 Experiment 1: Naming Objects**\\nThis Section reports on experiments in a standard setup of the object naming task where all object names are paired with visual instances of their referents during training. In a comparable task, i.e. object recognition with known object categories, cross-modal projection or transfer approaches have been reported to perform worse than standard object classification methods (Frome et al., 2013; Norouzi et al., 2013). This seems to suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the real world and that referential meaning should potentially be learned from visual object representation only.\\n**5.1 Model comparison**\\nSetup We use the train/test split of REFERIT data as in (Schlangen et al., 2016). We consider image regions with non-relational referring expressions that contain at least one of the 159 head nouns from the list of entry-level nouns (see section 3). This amounts to 6208 image regions for testing and 73K instances for training.\\nResults Table 1 shows accuracies in the object naming task for the TRANSFER, WAC and SIMWAP models according to their accuracies in the top n, including two variants of WAC where its top\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\n5 and top 10 predictions are project into the distributional space. Overall, the differences in accuracy between the models are small, but the various models that link their predictions to word representations in the distributional space all perform slightly worse than the plain WAC model, i.e. individual word classifiers trained on visual features only. This suggests that referential meanings for a word are learned less accurately when mapping from visual to distributional space, which replicates results reported in the literature on standard object recognition benchmarks.\\nhit @k(%) @1 @2 @5\\ntransfer 48.34 60.49 74.89 wac 49.34 61.86 75.35 wac, project top5 48.73 61.10 74.07 wac, project top10 48.68 61.23 74.31 sim-wap 48.13 60.60 75.40\\nTable 1: Accuracies in object naming\\n**5.2 Model combination**\\nIn order to get more insight into why the TRANSFER and SIM-WAP models produce slightly worse results than individual visual word classifiers, we now test to what extent the different models are complementary and combine them by aggregating over their naming predictions. If the models are complementary, their combination should lead to more confident and accurate naming decisions.\\nSetup We combine TRANSFER, SIM-WAP and WAC by aggregating the scores they predict for different object names for a given object. During testing, we apply all models to an image region and consider words ranked among the top 10. We first normalize the referential appropriateness scores in each top-10 list and then compute their sum. This aggregation scheme will give more weight to words that appear in the top 10 list of different models, and less weight to words that only get top-ranked by a single model. We test on the same data as in Section 5.1.\\nhit @k(%) 1 5 10\\nsim-wap + transfer 49.10 61.78 75.81 sim-wap + wac 51.10 63.45 77.92 transfer + wac 51.13 63.76 77.84 wac + transfer + sim-wap 52.19 64.71 78.40\\nTable 2: Object naming acc., combined models\\nResults Table 2 shows that all model combinations improve over the results of their isolated models in Table 1, suggesting that WAC, TRANSFER and SIM-WAP indeed do capture complementary aspects of referential word meaning. On their own, the distributionally informed models are less tuned to specific word occurrences than the visual word classifiers in the WAC model, but they can add useful information which leads to a clear overall improvement. We take this as a promising finding, supporting our initial hypothesis that knowledge on lexical distributional meaning should and can be exploited when learning how to use words for reference.\\nAv. cosine distance among top k gold - top k\\n5 10 5 10\\ntransfer 0.68 0.73 0.72 0.75 wac 0.82 0.80 0.82 0.84 sim-wap 0.68 0.74 0.72 0.75\\nTable 3: Cosine distances between word2vec embeddings of nouns generated in the top k\\n**5.3 Analysis**\\nFigure 2 illustrates objects from our test set where the combination of TRANSFER, SIM-WAP and WAC predicts an accurate name, whereas the models in isolation do not. These examples give some interesting insight into why the models capture different aspects of referential word use and meaning.\\nWord Similarities Many of the examples in Figure 2 suggest that the object names ranked among the top 3 by the TRANSFER and SIMWAP model are semantically similar to each other, whereas WAC generates object names on top that describe very different underlying object categories, such as seal / rock in Figure 2(a), animal / lamp in Figure 2(g) or chair / shirt in Figure 2(c). To quantify this general impression, Table 3 shows cosine distances among words in the top n generated by our models, using their word2vec embeddings. The average cosine distance between words in our vocabulary is 0.83. The transfer and sim-wap model rank words on top that are clearly more similar to each other than word pairs on average, whereas words ranked top by the wac model are more dissimilar. This parallels findings by Frome et al. (2013), discussed in Section 2. Additional evaluation metrics, such as success rates\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nin a human evaluation (cf. Zarrieß and Schlangen (2016)), would be an interesting direction for more detailed investigation here.\\nWord Use But even though the WAC classifiers lack knowledge on lexical similarities, they seem to able to detect relatively specific instances of word use such as hut in Figure 2(b), shirt in 2(c) or lamp in 2(h). Here, the combination with TRANSFER and SIM-WAP is helpful to give more weight to the object name that is taxonomically correct (sometimes pushing up words below the top-3 and hence not shown in Figure 2). In Figure 1(e), SIMWAP and TRANSFER give more weight to typical names for persons, whereas WAC top-ranks more unusual names, reflecting that the person is difficult to identify visually. Another observation is that the mapping models have difficulties dealing with object names in singular and plural. As these words have very similar representations in the distributional space, they are often predicted as likely variants among the top 10 by SIM-WAP and TRANSFER, whereas the WAC model seems to predict inappropriate plural words less often among the top 3. Such specific phenomena at the intersection of visual and semantic similarity have found very little attention in the literature. We will investigate them further in our Experiments on zeroshot naming in the following Section.\\n**6 Zero-Shot Naming**\\nZero-shot learning is an attractive prospect for REG from images, as it promises to overcome dependence on pairings of visual instances and natural names being available for all names, if visual/referential data can be generalised from other types of information. Previous work has looked at the feasibility of zero-shot learning as a function of semantic similarity or ontological closeness between unknown and known categories, and confirmed the intuition that the task is harder the less close unknown categories are to known ones (Frome et al., 2013; Norouzi et al., 2013).\\nOur experiments on object naming in Section 5 suggest that lexical similarities encoded in a distributional space might not always fully carry over to referential meaning. This could constitute an additional challenge for zero-shot learning, as distributional similarities might be misleading when the model has to fully rely on them for learning referential word meanings. Therefore, the following experiments investigate the performance of\\nour models in zero-shot naming as a function of the lexical relation between unknown and known object names, i.e. namely hypernyms and singular/plurals. Both relations are typically captured by distributional models of word meaning in terms of closeness in the vector space, but their visual and referential relation is clearly different.\\n**6.1 Vocabulary Splits and Testsets**\\nRandom As in previous work on zero-shot learning, we consider zero-shot naming for words of varying degrees of similarity in our vocabulary. We randomly split our 159 names from Experiment 1 into 10 subsets. We train the models on 90% of the nouns (and all their visual instances in the image corpus) and test on the set of image regions that are named with words which the model did not observe during training. Results reported in Table 4 on the random test set correspond to averaged scores from cross-validation over the 10 splits.\\nHypernyms We manually split the model’s vocabulary into set of hypernyms (see Appendix A) and the remaining nouns. We train the models on those 84K image regions that where not named with a hypernym, and test on 8895 image regions that were named with a hypernym in the corpus. We checked that for each of these hypernyms, the vocabulary contains at least one or two names that can be considered as hyponyms, i.e. the model sees objects during training that are instances of vehicle for example, but never encounters actual uses of that name. This test set is particularly interesting from an REG perspective, as objects named with very general terms by human speakers are often difficult to describe with more common, but more specific terms, as is illustrated by the uses of structure and thingy in Figure 1.\\nSingulars/Plurals We pick 68 words from our vocabulary that can be grouped into 34 singularplural noun pairs (see Appendix A). From each pair, we randomly include the singular or plural noun in the set of zero-shot nouns. Thus, we make sure that the model encounters singular and plural names during training, but it never encounters both variants of a name. This results in a more even training/test split, i.e. we train on 23K image regions and evaluate on 13825 instances.\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n(a) wac: seal, rock, water sim-wap: side, rock,rocks transfer: rocks, rock, water combination: rock\\n(c) wac: chair, shirt, guy sim-wap: woman, man, girl transfer: door, woman, window combination: shirt\\n(e) wac: chick, person, guy sim-wap: man, person, woman transfer: man, guy, girl combination: person\\n(g) wac: animal, lamp, table sim-wap: man, girl, person transfer: man, clouds, cloud combination: person\\n(b) wac: cactus, hut, mountain sim-wap: side, rock, mountain transfer: mountain, rocks, rock combination: hut\\n(d) wac: roof, house, building sim-wap: building, house, trees transfer: building, house, trees combination: house\\n(f) wac: bush, bushes, tree sim-wap: trees, tree, grass transfer: trees, tree, bushes combination: bushes\\n(h) wac: post, light, lamp sim-wap: tree, sky, pole transfer: tree, sky, trees combination: lamp\\nFigure 2: Examples from object naming experiment where model combination is accurate\\nZero-shot Model full vocab disjoint vocab names @1 @2 @5 @10 @1 @2\\nRandom\\ntransfer 0.05 2.38 16.57 35.71 41.49 62.34 wac, project top10 0.00 4.42 21.16 39.17 38.03 58.07 wac, project top5 0.00 4.39 21.63 40.01 37.46 57.36 sim-wap 3.71 13.13 36.49 54.44 42.28 64.26\\nHypernyms\\ntransfer 0.07 1.25 7.75 29.93 59.88 73.88 wac, project top10 0.00 3.01 15.55 36.99 50.51 66.33 wac, project top5 0.00 2.78 16.75 38.13 47.73 64.38 sim-wap 3.16 10.33 31.14 49.62 57.55 70.15\\nSingulars/Plurals transfer 0.01 22.84 44.30 72.85 34.56 51.79 wac, project top10 0.00 22.21 43.43 68.95 31.46 48.76 wac, project top5 0.00 22.18 43.93 69.33 31.46 48.88 sim-wap 15.39 34.73 56.62 77.32 37.24 54.02\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\n**6.2 Evaluation**\\nSome previous work on zero-shot image labeling assumes additional components that first identify whether an image should be labelled by a known or unknown word (Frome et al., 2013). We follow Lazaridou et al. (2014) and let the model decide whether to refer to an object by a known or unknown name. Related to that, distinct evaluation procedures have been used in the literature on zero-shot learning:\\nTesting on full vocabulary A realistic way to test zero-shot learning performance is to consider all words from a given vocabulary during testing, though the testset only contains instances of objects that have been named with a ‘zero-shot word’ (for which no visual instances were seen during training). Accuracies in this setup reflect how well the model is able to generalize, i.e. how often it decides to deviate from the words it was trained on, and (implicitly) predicts that the given object requires a “new” name. In case of the (i) hypernym and (ii) singular/plural test set, this accuracy also reflects to what extent the model is able to detect cases where (i) a more general or vague term is needed, where (ii) an unknown singular/plural counterpart of a known object type occurs.\\nTesting on disjoint vocabulary Alternatively, the model’s vocabulary can be restricted during testing to zero-shot words only, such that names encountered during training and testing are disjoint, see e.g. (Lampert et al., 2009, 2013). This setup factors out the generalization problem, and assesses to what extent a model is able to capture the referential meaning of a word that does not have instances in the training data.\\n**6.3 Results**\\nAs compared to Experiment 1 where models achieved similar performance, differences are more pronounced in the zero-shot setup, as shown in Table 4. In particular, we find that the SIMWAP model which induces individual predictors for words that have not been observed in the training data is clearly more successful than TRANSFER or WAC that project predictions into the distributional space. When tested on the full vocabulary, we find that TRANSFER and WAC very rarely generate names whose referents were excluded from training, which is in line with observations made by Lazaridou et al. (2015a). The SIM-WAP\\npredictors generalize much better, in particular on the singular/plural testset.\\nAn interesting exception is the good performance of the TRANSFER model on the hypernym test set, when evaluated with a disjoint vocabulary. This corroborates evidence from Experiment 1, namely that the transfer model captures taxonomic aspects of object names better than the other models. Projection via individual word classifiers, on the other hand, seems to generalize better than TRANSFER, at least when looking at accuracies @2 ... @10. Thus, combining several vectors predicted by a model of referential word meaning can provide additional information, as compared to mapping an object to a single vector in distributional space. More work is needed to establish how these approaches can be integrated more effectively.\\n   \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              **2 Related Work**\\nRecently, building a chatbot with data driven approaches (Ritter et al., 2011; Higashinaka et al., 2014) has drawn a lot of attention. Existing work along this line includes retrieval based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation based methods (Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2015, 2016; Xing\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\n.... .... ....\\nScore\\n1 2,M M Convolution Pooling\\n( )L\\n.... .... ....\\n1u\\n1nu \\nnu\\nr\\nWord Embedding GRU1\\nGRU2\\n....\\n1v\\n1nv \\nnv\\n1'nh \\nUtterance-Response Matching Matching Accumulation\\nSegment PairsWord Pairs\\nMatching Prediction\\n1'h\\n'nh\\nFigure 1: Architecture of SMN\\net al., 2016; Serban et al., 2016a). Our work belongs to retrieval based methods, and we study context based response selection.\\nEarly studies of retrieval based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b). Recently, researchers begin to pay attention to multi-turn conversation. For example, Lowe et al. (2015) match a response with the literal concatenation of context utterances. Yan et al. (2016) concatenate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture. Zhou et al. (2016) improve multi-turn response selection with a multi-view model including an utterance view and a word view. Our model is different in that it matches a response with each utterance at first and accumulates matching information instead of sentences by a GRU, thus useful information for matching can be sufficiently retained.\\n**3 Sequential Matching Network**\\n\\n**3.1 Problem Formalization**\\nSuppose that we have a data set D = {(yi, si, ri)}Ni=1, where si = {ui,1, . . . , ui,ni} represents a conversation context with {ui,k}nik=1 as utterances. ri is a response candidate and yi ∈ {0, 1} denotes a label. yi = 1 means ri is a proper response for si, otherwise yi = 0. Our goal is to learn a matching model g(·, ·) with D. For any context-response pair (s, r), g(s, r) measures the matching degree between s and r.\\n**3.2 Model Overview**\\nWe propose a sequential matching network (SMN) to model g(·, ·). Figure 1 gives the architecture.\\nSMN first decomposes context-response matching into several utterance-response pair matching and then all pair matching is accumulated as a context based matching through a recurrent neural network. SMN consists of three layers. The first layer matches a response candidate with each utterance in the context on a word level and a segment level, and important matching information from the two levels is distilled by convolution and pooling and encoded in a matching vector. The matching vectors are then fed into the second layer where they are accumulated in the hidden states of a recurrent neural network with GRU following the chronological order of the utterances in the context. The third layer calculates the final matching score with the hidden states of the second layer.\\nSMN enjoys several advantages over the existing models. First, a response candidate can meet each utterance in the context at the very beginning, thus matching information in every utteranceresponse pair can be sufficiently extracted and carried to the final matching score with minimal loss. Second, information extraction from each utterance is conducted on different levels of granularity and under sufficient supervision from the response, thus semantic structures that are useful to response selection in each utterance can be well identified and extracted. Third, matching and utterance relationships are coupled rather than separately modeled, thus utterance relationships (e.g., order), as a kind of knowledge, can supervise the formation of the matching score.\\nBy taking utterance relationships into account, SMN extends the “2D” matching that has proven effective in text pair matching for single-turn response selection to sequential “2D” matching for context based matching in response selection for multi-turn conversation. In the following sections,\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nwe will describe details of the three layers.\\n**3.3 Utterance-Response Matching**\\nGiven an utterance u in a context s and a response candidate r, the model looks up an embedding table and represents u and r as U = [eu,1, . . . , eu,nu ] and R = [er,1, . . . , er,nr ] respectively, where eu,i, er,i ∈ Rd are the embeddings of the i-th word of u and r respectively. U ∈ Rd×nu and R ∈ Rd×nr are then used to construct a word-word similarity matrix M1 ∈ Rnu×nr and a sequence-sequence similarity matrix M2 ∈ Rnu×nr which are two input channels of a convolutional neural network (CNN). The CNN distills important matching information from the matrices and encodes the information into a matching vector v.\\nSpecifically, ∀i, j, the (i, j)-th element of M1 is defined by\\ne1,i,j = e > u,i · er,j . (1)\\nM1 models the matching between u and r on a word level.\\nTo construct M2, we first employ a GRU to transform U and R to hidden vectors. Suppose that Hu = [hu,1, . . . , hu,nu ] are the hidden vectors of U, then ∀i, hu,i ∈ Rm is defined by\\nzi = σ(Wzeu,i +Uzhu,i−1) ri = σ(Wreu,i +Urhu,i−1)\\nh̃u,i = tanh(Wheu,i +Uh(ri hu,i−1)) hu,i = zi h̃u,i + (1− zi) hu,i−1, (2)\\nwhere hu,0 = 0, zi and ri are an update gate and a reset gate respectively, σ(·) is a sigmoid function, and Wz, Wh, Wr, Uz, Ur,Uh are parameters. Similarly, we have Hr = [hr,1, . . . , hr,nr ] as the hidden vectors of R. Then, ∀i, j, the (i, j)-th element of M2 is defined by\\ne2,i,j = h > u,iAhr,j , (3)\\nwhere A ∈ Rm×m is a linear transformation. ∀i, GRU models the sequential relationship and the dependency among words up to position i and encodes the text segment until the i-th word to a hidden vector. Therefore, M2 models the matching between u and r on a segment level. M1 and M2 are then processed by a CNN to form v. ∀f = 1, 2, CNN regards Mf as an input channel, and alternates convolution and max-pooling operations. Suppose that z(l,f) =\\n[ z (l,f) i,j ] I(l,f)×J(l,f) denotes the output of feature maps of type-f on layer-l, where z(0,f) = Mf , ∀f = 1, 2. On the convolution layer, we employ a 2D convolution operation with a window size r (l,f) w × r(l,f)h , and define z (l,f) i,j as\\nz (l,f) i,j = σ( Fl−1∑ f ′=0 r (l,f) w∑ s=0 r (l,f) h∑ t=0 W (l,f) s,t · z (l−1,f ′) i+s,j+t + b l,k), (4)\\nwhere σ(·) is a ReLU, W(l,f) ∈ Rr (l,f) w ×r (l,f) h and bl,k are parameters, and Fl−1 is the number of feature maps on the (l − 1)-th layer. A max pooling operation follows a convolution operation and can be formulated as\\nz (l,f) i,j = max\\np (l,f) w >s≥0 max p (l,f) h >t≥0 zi+s,j+t, (5)\\nwhere p(l,f)w and p (l,f) h are the width and the height of the 2D pooling respectively. The output of the final feature maps are concatenated and mapped to a low dimensional space with a linear transformation as the matching vector v ∈ Rq.\\nFrom Equation (1), (3), (4), and (5), we can see that by learning word embedding and parameters of GRU from training data, words or segments in an utterance that are useful to recognize the appropriateness of a response may have high similarity with some words or segments in the response and result in high value areas in the similarity matrices. These areas will be transformed and selected by convolution and pooling operations and carry the important information in the utterance to the matching vector. This is how our model identifies important information in context and leverage it in matching under the supervision of the response. We consider multiple channels because we want to capture important matching information on multiple levels of granularity of text.\\n**3.4 Matching Accumulation**\\nSuppose that [v1, . . . , vn] is the output of the first layer (corresponding to n pairs), at the second layer, a GRU takes [v1, . . . , vn] as an input and encodes the matching sequence into its hidden states Hm = [h ′ 1, . . . , h ′ n] ∈ Rq×n with a detailed parameterization similar to Equation (2). This layer has two functions: (1) it models the dependency and the temporal relationship of utterances in the context; (2) it leverages the temporal relationship to supervise the accumulation of the pair matching as a context based matching. Moreover, from\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nEquation (2), we can see that the reset gate (i.e., ri) and the update gate (i.e., zi) control how much information from the previous hidden state and the current input flows to the current hidden state, thus important matching vectors (corresponding to important utterances) can be accumulated while noise in the vectors can be filtered out.\\n**3.5 Matching Prediction and Learning**\\nWith [h′1, . . . , h ′ n], we define g(s, r) as\\ng(s, r) = softmax(W2L[h ′ 1, . . . , h ′ n] + b2), (6)\\nwhere W2 and b2 are parameters. We consider three parameterizations for L[h′1, . . . , h ′ n]: (1) only the last hidden state is used. Then L[h′1, . . . , h ′ n] = h ′ n. (2) the hidden states\\nare linearly combined. Then, L[h′1, . . . , h ′ n] =∑n\\ni=1wih ′ i, where wi ∈ R. (3) we follow (Yang et al., 2016) and employ an attention mechanism to combine the hidden states. Then, L[h′1, . . . , h ′ n] is defined as\\nti = tanh(W1,1hui,nu +W1,2h ′ i + b1), αi = exp(t>i ts)∑ i(exp(t > i ts)) ,\\nL[h′1, . . . , h ′ n] = n∑ i=1 αih ′ i, (7)\\nwhere W1,1 ∈ Rq×m,W1,2 ∈ Rq×q and b1 ∈ Rq are parameters. h′i and hui,nu are the i-th matching vector and the final hidden state of the i-th utterance respectively. ts ∈ Rq is a virtual context vector which is randomly initialized and jointly learned in training.\\nBoth (2) and (3) aim to learn weights for {h′1, . . . , h′n} from training data and highlight the effect of important matching vectors in the final matching. The difference is that weights in (2) are static, because the weights are totally determined by the positions of utterances, while weights in (3) are dynamically computed by the matching vectors and utterance vectors. We denote our model with the three parameterizations of L[h′1, . . . , h ′ n] as SMNlast, SMNstatic, and SMNdynamic, and empirically compare them in experiments.\\nWe learn g(·, ·) by minimizing cross entropy withD. Let Θ denote the parameters of SMN, then the objective function L(D,Θ) of learning can be formulated as\\n− N∑ i=1 [yilog(g(si, ri)) + (1− yi)log(1− g(si, ri))] . (8)\\n**4 Response Candidate Retrieval**\\nIn practice of a retrieval based chatbot, to apply the matching approach to response selection, one needs to retrieve a bunch of response candidates from an index beforehand. While candidate retrieval is not the focus of the paper, it is an important step in a real system. In this work, we exploit a heuristic method to obtain response candidates from the index. Given a message un with {u1, . . . , un−1} utterances in its previous turns, we extract top 5 keywords from {u1, . . . , un−1} based on their tf-idf scores1 and expand un with the keywords. Then we send the expanded message to the index and retrieve response candidates using the inline retrieval algorithm of the index. Finally, we use g(s, r) to re-rank the candidates and return the top one as a response to the context.\\n**5 Experiments**\\nWe tested our model on a public English data set and a Chinese data set we publish with this paper.\\n**5.1 Ubuntu Corpus**\\nThe English data set is the Ubuntu Corpus (Lowe et al., 2015) which contains multi-turn dialogues collected from chat logs of Ubuntu Forum. The data set consists of 1 million context-response pairs for training, 0.5 million pairs for validation, and 0.5 million pairs for test. Positive responses are true responses from human, and negative ones are randomly sampled. The ratio of the positive and the negative is 1:1 in training, and 1:9 in validation and test. We used the copy shared by Xu et al. (2016) 2 in which numbers, urls, and paths are replaced by special placeholders. We followed (Lowe et al., 2015) and employed recall at position k in n candidates (Rn@k) as evaluation metrics.\\n**5.2 Douban Conversation Corpus**\\nUbuntu Corpus is a domain specific data set, and response candidates are obtained from negative sampling without human judgment. To further verify the efficacy of our model, we created a new data set with open domain conversations, namely Douban Conversation Corpus. Response candidates in the test set of Douban Conversation Corpus are collected following the procedure of a re-\\n1Tf is word frequency in the context, while idf is calculated using the entire index.\\n2https://www.dropbox.com/s/ 2fdn26rj6h9bpvl/ubuntudata.zip?dl=0\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\ntrieval based chatbot and are labeled by human judges. Douban Conversation Corpus simulates the real scenario of a retrieval based chatbot, and we publish it to research communities to facilitate the research of multi-turn response selection.\\nSpecifically, we crawled 1.1 million dyadic dialogues (conversation between two persons) longer than 2 turns from Douban group3 which is a popular social networking service in China. From the data, we randomly sampled 0.5 million dialogues for creating a training set, 25 thousand dialouges for creating a validation set, and 1, 000 dialogues for creating a test set, and made sure that there is no overlap among the three sets. For each dialogue in training and validation, we took the last turn as a positive response for the previous turns as a context and randomly sampled another response from the 1.1 million data as a negative response. There are 1 million context-response pairs in the training set and 50 thousand pairs in the validation set.\\nTo create the test set, we first crawled 15 million post-reply pairs from Sina Weibo4 which is the largest microblogging service in China and indexed the pairs with Lucene5. We took the last turn of each Douban dyadic dialogue in the test set as a message, retrieved 10 response candidates from the index following the method in Section 4, and finally formed a test set with 10, 000 context-response pairs. We recruited three labelers to judge if a candidate is a proper response to the context. A proper response means the response can naturally reply to the message given the whole context. Each pair received three labels and the majority of the labels were taken as the final decision. Table 2 gives the statistics of the three sets. Note that the Fleiss’ kappa (Fleiss, 1971) of the labeling is 0.41, which indicates that the three labelers reached a relatively high agreement.\\nBesides Rn@ks, we also followed the convention of information retrieval and employed mean average precision (MAP) (Baeza-Yates et al., 1999), mean reciprocal rank (MRR) (Voorhees et al., 1999), and precision at position 1 (P@1) as evaluation metrics. We did not calculate R2@1 because in Douban corpus one context could have more than one correct responses, and we have to randomly sample one for R2@1, which may bring bias to evaluation. When using the labeled set,\\n3https://www.douban.com/group 4http://weibo.com/ 5https://lucenenet.apache.org/\\nwe removed conversations with all negative responses or all positive responses, as models make no difference on them. There are 6, 670 contextresponse pairs left in the test set.\\n**5.3 Baseline**\\nWe considered the following baselines:\\nBasic models: models in (Lowe et al., 2015) and (Kadlec et al., 2015) including TF-IDF, RNN, CNN, LSTM and BiLSTM.\\nMulti-view: the model proposed by Zhou et al. (2016) that utilizes a hierarchical recurrent neural network to model utterance relationships.\\nDeep learning to respond (DL2R): the model proposed by Yan et al. (2016) that reformulates the message with other utterances in the context.\\nAdvanced single-turn matching models: since BiLSTM does not represent the state-ofthe-art matching model, we concatenated the utterances in a context and matched the long text with a response candidate using more powerful models including MV-LSTM (Wan et al., 2016) (2D matching), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al., 2015) (two attention based models), and Multi-Channel which is described in Section 3.3. Multi-Channel is a simple version of our model without considering utterance relationships. We also appended the top 5 tf-idf words in context to the input message, and computed the score between the expanded message and a response with Multi-Channel, denoted as Multi-Channelexp.\\n**5.4 Parameter Tuning**\\nFor baseline models, if their results are available in the existing literatures (e.g., those on the Ubuntu corpus), we just copied the numbers, otherwise we implemented the models following the settings in the literatures. All models were implemented using Theano (Theano Development Team, 2016). Word embeddings were initialized by the results of word2vec (Mikolov et al., 2013) which ran on the training data, and the dimensionality of word vectors is 200. For Multi-Channel and layer one of\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nUbuntu Corpus Douban Conversation Corpus R2@1 R10@1 R10@2 R10@5 MAP MRR P@1 R10@1 R10@2 R10@5\\nTF-IDF 0.659 0.410 0.545 0.708 0.331 0.359 0.179 0.095 0.172 0.405 RNN 0.768 0.403 0.547 0.819 0.390 0.422 0.208 0.011 0.223 0.589 CNN 0.848 0.549 0.684 0.896 0.417 0.440 0.226 0.012 0.252 0.647 LSTM 0.901 0.638 0.784 0.949 0.485 0.527 0.320 0.187 0.343 0.720 BiLSTM 0.895 0.630 0.780 0.944 0.479 0.514 0.313 0.184 0.330 0.716 Multi-View 0.908 0.662 0.801 0.951 0.505 0.543 0.342 0.202 0.350 0.729 DL2R 0.899 0.626 0.783 0.944 0.488 0.527 0.330 0.193 0.342 0.705 MV-LSTM 0.906 0.653 0.804 0.946 0.498 0.538 0.348 0.202 0.351 0.710 Match-LSTM 0.904 0.653 0.799 0.944 0.500 0.537 0.345 0.202 0.348 0.720 Attentive-LSTM 0.903 0.633 0.789 0.943 0.495 0.523 0.331 0.192 0.328 0.718 Multi-Channel 0.904 0.656 0.809 0.942 0.506 0.543 0.349 0.203 0.351 0.709 Multi-Channelexp 0.714 0.368 0.497 0.745 0.476 0.515 0.317 0.179 0.335 0.691 SMNlast 0.923 0.723 0.842 0.956 0.526 0.571 0.392 0.236 0.387 0.729 SMNstatic 0.927 0.725 0.838 0.962 0.523 0.572 0.387 0.228 0.387 0.734 SMNdynamic 0.926 0.726 0.847 0.961 0.529 0.569 0.395 0.233 0.396 0.724\\nTable 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statistically significant compared with the best baseline.\\nour model, we set the dimensionality of the hidden states of GRU as 200. We tuned the window size of convolution and pooling in {(2, 2), (3, 3)(4, 4)} and chose (3, 3) finally. The number of feature maps is 8. In layer two, we set the dimensionality of matching vectors and the hidden states of GRU as 50. The parameters were updated by stochastic gradient descent with Adam algorithm (Kingma and Ba, 2014) on a single Tesla K80 GPU. The initial learning rate is 0.001, and the parameters of Adam, β1 and β2 are 0.9 and 0.999 respectively. We employed early-stopping as a regularization strategy. Models were trained in minibatches with a batch size 200, and maximum utterance length is 50. We set the maximum context length (i.e., number of utterances) as 10, because performance of models does not get improved on contexts longer than 10 (details are shown in the supplementary material). We padded zeros if the number of utterances in a context is less than 10, otherwise we kept the last 10 utterances.\\n**5.5 Evaluation Results**\\nTable 3 shows the evaluation results on the two data sets. Our models outperform baselines greatly in terms of all metrics on both data sets, and the improvements are statistically significant (t-test with p-value ≤ 0.01, except R10@5 on Douban Corpus). Even the state-of-the-art singleturn matching models perform much worse than our models. The results demonstrate that one cannot neglects utterance relationships and simply perform multi-turn response selection by concatenating utterances together. Our models achieve significant improvements over Multi-View, which justified our “matching first” strategy. DL2R is\\nworse than our models, indicating that utterance reformulation with heuristic rules is not a good method to utilize context information. Rn@ks are low on Douban corpus as there are multiple correct candidates for a context (e.g., if there are 3 correct responses, then the maximumR10@1 is 0.33). SMNdynamic is only slightly better than SMNstatic and SMNlast. The reason might be that GRU can select useful signals from the matching sequence and accumulate them in the final state with its gate mechanism, thus the efficacy of attention mechanism is not obvious for the task.\\n**5.6 Further Analysis**\\nVisualization: we visualize the similarity matrices and the gates of GRU in layer two using an example from the Ubuntu corpus to further clarify how our model identifies important information in the context and how it selects important matching vectors with the gate mechanism of GRU as described in Section 3.3 and Section 3.4. The example is {u1: how can unzip many rar ( number for example ) files at once; u2: sure you can do that in bash; u3: okay how? u4: are the files all in the same directory? u5: yes they all are; r: then the command glebihan should extract them all from/to that directory}. It is from the test set and our model successfully ranked the correct response to the top position. Due to space limitation, we only visualized M1, M2 and the update gate (i.e. z) in Figure 2. Other pieces of our model are shown in the supplementary material. We can see that in u1 important words including “unzip”, “rar”, “files” are recognized and carried to matching by “command”, “extract”, and “directory” in r, while u3 is almost useless and thus little infor-\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nUbuntu Corpus Douban Conversation Corpus R2@1 R10@1 R10@2 R10@5 MAP MRR P@1 R10@1 R10@2 R10@5\\nReplaceM 0.905 0.661 0.799 0.950 0.503 0.541 0.343 0.201 0.364 0.729 ReplaceA 0.918 0.716 0.832 0.954 0.522 0.565 0.376 0.220 0.385 0.727 Only M1 0.919 0.704 0.832 0.955 0.518 0.562 0.370 0.228 0.371 0.737 Only M2 0.921 0.715 0.836 0.956 0.521 0.565 0.382 0.232 0.380 0.734 SMNlast 0.923 0.723 0.842 0.956 0.526 0.571 0.392 0.236 0.387 0.729\\nTable 4: Evaluation results of model ablation.\\nth en th e\\nco m\\nm an\\nd gl eb ih an sh ou ld ex tra ct th em a ll fro m /toth at di re ct or y\\nhow can\\nunzip many\\nrar ( _number_ for\\nexample )\\nfiles at once\\n0.00\\n0.15\\n0.30\\n0.45\\n0.60\\n0.75\\n0.90\\n1.05\\n1.20\\n1.35\\n1.50\\nv a lu\\ne\\n(a) M1 of u1 and r\\nth en th e\\nco m\\nm an\\nd\\ngl eb\\nih an\\nsh ou\\nld ex tr ac\\nt th em a ll\\nfro m\\n/toth at\\ndi re\\nct or\\ny\\nokay how 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 v a lu e\\n(b) M1 of u3 and r\\n0 10 20 30 40\\nu_1\\nu_2\\nu_3\\nu_4\\nu_5\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nv a lu\\ne\\n(c) Update gate\\nFigure 2: Model visualization. Darker areas mean larger value.\\nmation is extracted from it. u1 is crucial to response selection and nearly all information from u1 and r flows to the hidden state of GRU, while other utterances are less informative and the corresponding gates are almost “closed” to keep the information from u1 and r until the final state.\\nModel ablation: we investigate the effect of different parts of SMN by removing them one by one from SMNlast, shown in Table 4. First, replacing the multi-channel “2D” matching with a neural tensor network (NTN) (Socher et al., 2013) (denoted as ReplaceM ) makes the performance drop dramatically. This is because NTN only matches a pair by an utterance vector and a response vector and loses important information in the pair. Together with the visualization, we can conclude that “2D” matching plays a key role in the “matching first” strategy as it captures the important matching information in each pair with minimal loss. Second, the performance slightly drops when replacing the GRU for matching accumulation with a multi-layer perceptron (denoted as ReplaceA). This indicates that utterance relationships are useful. Finally, we left only one channel in matching and found that M2 is a little more powerful than M1 and we achieve the best results with both of them (except on R10@5 on Douban Corpus).\\nContext length: we study how our model (SMNlast) performs across the length of contexts. Figure 3 shows the comparison on MAP in different length intervals on the Douban corpus. Our model consistently performs better than the baselines, and when contexts become longer, the gap becomes larger. The results demonstrate that our model can well capture the dependencies, espe-\\ncially long dependencies, among utterances in contexts. We give the comparisons on other metrics in our supplementary material.\\n(2,5] (5,10] (10,) context length\\n40\\n45\\n50\\n55 60 M A P\\nLSTM MV-LSTM Multi-View SMN\\nFigure 3: Comparison across context length Retrieval v.s. Generation: we compared SMN with a state-of-the-art response generation model VHERD (Serban et al., 2016b) which was trained using D on the Douban corpus. We conducted a side-by-side human comparison on the top one responses of the two models for each context in the test set. The result is that SMN wins on 238 examples, loses on 207 examples, and is comparable with VHRED on the remaining 555 examples. This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice.\\n   \n",
            "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
            "132  **2 Inflectional Morphology and Paradigm Completion**\\nMany languages exhibit inflectional morphology, i.e., the form of an individual lexical entry mutates to show properties such as person, number or case. The citation form of a lexical entry is referred to as the lemma and the collection of its possible inflections as its paradigm. Tab. 1 shows an example of a partial paradigm; we display several forms for the Spanish verbal lemma soñar. We may index the entries of a paradigm by a morphological tag, e.g., the 2SgPresInd form sueñas in Tab. 1. In generation, the speaker must select an entry of the paradigm given the form’s context. In general, the presence of rich inflectional morphology is problematic for NLP systems as it greatly increases the token-type ratio and, thus, word form sparsity.\\nAn important task in inflectional morphology is paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Cotterell et al., 2015; Faruqui et al., 2016). Its goal is to map a lemma to all individual inflections, e.g., (soñar, 1SgPresInd) 7→ sueño. There are good solutions for paradigm completion when a large amount of annotated training data is available (Cotterell et al., 2016a).1 In this work, we address the lowresource setting, an up to now unsolved challenge.\\n**2.1 Transferring Inflectional Morphology**\\nIn comparison to other NLP annotations, e.g., partof-speech (POS) and named entities, morphological inflection does not lend itself easily to transfer.\\n1The SIGMORPHON 2016 shared task (Cotterell et al., 2016a) on morphological reinflection, a harder generalization of paradigm completion, found that ≥ 98% accuracy can be achieved in many languages with neural sequence-to-sequence models, improving the state of the art by 10%.\\nWe can define a universal set of POS tags (Petrov et al., 2012) or of entity types (e.g., coarse-grained types like person and location or fine-grained types (Yaghoobzadeh and Schütze, 2015)), but inflection is much more language-specific. It is infeasible to transfer morphological knowledge from Chinese to Portuguese as Chinese does not use inflected word forms. Transferring named entity recognition, however, among Chinese and European languages works well (Wang and Manning, 2014a). But even transferring inflectional paradigms from morphologically rich Arabic to Portuguese seems difficult as the inflections often mark dissimilar subcategories. In contrast, transferring morphological knowledge from Spanish to Portuguese, two languages with similar conjugations and 89% lexical similarity, appears promising. Thus, we conjecture that transfer of inflectional morphology is only viable among related languages.\\n**2.2 Formalization of the Task**\\nWe now offer a formal treatment of the crosslingual paradigm completion task and develop our notation. Let Σ` be a discrete alphabet for language ` and let T` be a set of morphological tags for `. Given a lemma w` in `, the morphological paradigm (inflectional table) π can be formalized as a set of pairs\\nπ(w`) = {( fk[w`], tk )} k∈T (w`)\\n(1)\\nwhere fk[w`] ∈ Σ+` is an inflected form, tk ∈ T` is its morphological tag and T (w`) is the set of slots in the paradigm; e.g., a Spanish paradigm is:\\nπ(SOÑAR)= {( sueño, 1SgPresInd ) , . . . , ( soñaran, 3PlPastSbj )}\\nParadigm completion consists of predicting the entire paradigm π(w`) given the lemma w`.\\nIn cross-lingual paradigm completion, we consider a high-resource source language `s (lots of training data available) and a low-resource target language `t (little training data available). We denote the source training examples as Ds (with |Ds| = ns) and the target training examples as Dt (with |Dt| = nt). The goal of cross-lingual paradigm completion is to populate paradigms in the low-resource target language with the help of data from the high-resource source language, using only few in-domain examples.\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\n**3 Cross-Lingual Transfer as Multi-Task Learning**\\nWe describe our probability model for morphological transfer using terminology from multi-task learning (Caruana, 1997; Collobert et al., 2011). We consider two tasks, training a paradigm completor (i) for a high-resource language and (ii) for a low-resource language. We want to train jointly so we reap the benefits of having related languages. Thus, we define the log-likelihood as\\nL(θ)= ∑\\n(k,w`t )∈Dt\\nlog pθ (fk[w`t ] | w`t , tk) (2)\\n+ ∑\\n(k,w`s )∈Ds\\nlog pθ(fk[w`s ] | w`s , tk)\\nwhere we tie parameters θ for the two languages together to allow the transfer of morphological knowledge between languages. Each probability distribution pθ defines a distribution over all possible realizations of an inflected form, i.e., a distribution over Σ∗. For example, consider the related Romance languages Spanish and French; focusing on one term from each of the summands in Eq. (2) (the past participle of the translation of to visit in each language), we arrive at\\nLvisit(θ) = log pθ(visitado | visitar, PastPart) + log pθ(visité | visiter, PastPart) (3)\\nOur cross-lingual setting forces both transductions to share part of the parameter vector θ, to represent morphological regularities between the two languages in a common embedding space and, thus, to enable morphological transfer. This is no different from monolingual multi-task settings, e.g., jointly training a parser and tagger for transfer of syntax.\\nBased on recent advances in neural transducers, we parameterize each distribution as an encoderdecoder RNN, as in (Kann and Schütze, 2016b). In their setup, the RNN encodes the input and predicts the forms in a single language. In contrast, we force the network to predict two languages.\\n**3.1 Encoder-Decoder RNN**\\nWe parameterize the distribution pθ as an encoderdecoder gated RNN with attention (Bahdanau et al., 2015), the state-of-the-art solution for the monolingual case (Kann and Schütze, 2016b). A bidirectional gated RNN encodes the input sequence (Cho et al., 2014) – the concatenation of (i) the language\\n! h1\\n! h2\\n! h3\\n! hN\\nh1\\nh2\\nh3\\nhN\\ns o ñ r\\ns u e s1 s2 s3 sN\\ny1= y2= y3=M\\n…\\nFigure 1: Encoder-decoder RNN for paradigm completion. The lemma soñar is mapped to a target form (e.g., sueña). For brevity, language and target tags are omitted from the input. Thickness of red arrows symbolizes the degree to which the model attends to the corresponding hidden state of the encoder.\\ntag, (ii) the morphological tag of the form to be generated and (iii) the characters of the input word – represented by embeddings. The input to the decoder consists of concatenations of −→ hi and ←− hi , the forward and backward hidden states of the encoder. The decoder, a unidirectional RNN, uses attention: it computes a weight for each hi. Each weight reflects the importance given to that input position. Using the attention weights αij , the probability of the output sequence given the input sequence is:\\np(y | x1, . . . , x|X|) = |Y |∏\\nt=1\\ng(yt−1, st, ct) (4)\\nwhere y = (y1, . . . , y|Y |) is the output sequence (a sequence of |Y | characters), x = (x1, . . . x|X|) is the input sequence (a sequence of |X| characters), g is a non-linear function, st is the hidden state of the decoder and ct is the sum of the encoder states hi, weighted by attention weights αi(st−1) which depend on the decoder state:\\nct =\\n|X|∑\\ni=1\\nαi(st−1)hi (5)\\nFig. 1 shows the encoder-decoder. See Bahdanau et al. (2015) for further details.\\n**3.2 Input Format**\\nEach source form is represented as a sequence of characters; each character is represented as an embedding. In the same way, each source tag is represented as a sequence of subtags, and each subtag is represented as an embedding. More formally,\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nwe define the alphabet Σ = ∪`∈LΣ` as the set of characters in the languages in L, with L being the set of languages in the given experiment. Next, we define S as the set of subtags that occur as part of the set of morphological tags T = ∪`∈LT`; e.g., if 1SgPresInd ∈ T , then 1, Sg, Pres, Ind ∈ S . Note that the set of subtags S is defined as attributes from the UNIMORPH schema (Sylak-Glassman, 2016) and, thus, is universal across languages; the schema is derived from research in linguistic typology.2 The format of the input to our system is S+Σ+. The output format is Σ+. Both input and output are padded with distinguished BOW and EOW symbols.\\nWhat we have described is the representation of Kann and Schütze (2016b). In addition, we preprend a symbol λ ∈ L to the input string (e.g., λ = Es, also represented by an embedding), so the RNN can handle multiple languages simultaneously and generalize over them.\\n**4 Languages and Language Families**\\nTo verify the applicability of our method to a wide range of languages, we perform experiments on example languages from several different families.\\nRomance languages, a subfamily of IndoEuropean, are widely spoken, e.g., in Europe and Latin America. Derived from the common ancestor Vulgar Latin (Harris and Vincent, 2003), they share large parts of their lexicon and inflectional morphology; we expect knowledge among them to be easily transferable.\\nWe experiment on Catalan, French, Italian, Portuguese and Spanish. Tab. 2 shows that Spanish – which takes the role of the low-resource language in our experiments – is closely related with the other four, with Portuguese being most similar. We hypothesize that the transferability of morphological knowledge between source and target corresponds to the degree of lexical similarity; thus, we expect Portuguese and Catalan to be more beneficial for Spanish than Italian and French.\\nThe Indo-European Slavic language family has its origin in eastern-central Europe (Corbett and Comrie, 2003). We experiment on Bulgarian, Macedonian, Russian and Ukrainian (Cyrillic script) and on Czech, Polish and Slovene (Latin script). Macedonian and Ukranian are low-resource\\n2Note that while the subtag set is universal, which subtags a language actually uses is language-specific; e.g., Spanish does not mark animacy as Russian does. We contrast this with the universal POS set (Petrov et al., 2012), where it is reasonable to expect that we see all 17 tags in every language.\\nlanguages, so we assign them the low-resource role. For Romance and for Uralic, we experiment with groups containing three or four source languages. To arrive at a comparable experimental setup for Slavic, we run two experiments, each with three source and one target language: (i) from Russian, Bulgarian and Czech to Macedonian; and (ii) from Russian, Polish and Slovene to Ukrainian.\\nWe hope that the paradigm completor learns similar embeddings for, say, the characters “e” in Polish and “ ” in Ukrainian. Thus, the use of two scripts in Slavic allows us to explore transfer across different alphabets.\\nWe further consider a non-Indo-European language family, the Uralic languages. We experiment on the three most commonly spoken languages – Finnish, Estonian and Hungarian (Abondolo, 2015) – as well as Northern Sami, a language used in Northern Scandinavia. While Finnish and Estonian are closely related (both are members of the Finnic subfamily), Hungarian is a more distant cousin. Estonian and Northern Sami are lowresource languages, so we assign them the lowresource role, resulting in two groups of experiments: (i) Finnish, Hungarian and Estonian to Northern Sami; (ii) Finnish, Hungarian and Northern Sami to Estonian.\\nArabic (baseline) is a Semitic language (part of the Afro-Asiatic family (Hetzron, 2013)) that is spoken in North Africa, the Arabian Peninsula and other parts of the Middle East. It is unrelated to all other languages used in this work. Both in terms of form (new words are mainly built using a templatic system) and categories (it has tags such as construct state), Arabic is very different. Thus, we do not expect it to support morphological knowledge transfer and we use it as a baseline for all target languages.\\n**5 Experiments**\\nWe run three experiments on 21 distinct pairings of languages to show the feasibility of morphological transfer and analyze our method. We first discuss details common to all experiments.\\nWe keep hyperparameters during all experiments (and for all languages) fixed to the following values. Encoder and decoder RNNs each have 100\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\n50·20 50·21 50·22 50·23 50·24 50·25 50·26 50·27 Number of Samples\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8 1.0 A cc ur ac y\\nLanguages Pt Ca It Fr Ar Es\\nFigure 2: Learning curves showing the accuracy on Spanish test when training on language λ ∈ {PT, CA, IT, FR, AR, ES}. Except for λ=ES, each model is trained on 12,000 samples from λ and “Number of Samples” (x-axis) of Spanish.\\nhidden units and the size of all subtag, character and language embeddings is 300. For training we use ADADELTA (Zeiler, 2012) with minibatch size 20. All models are trained for 300 epochs. Following Le et al. (2015), we initialize all weights in the encoder, decoder and the embeddings except for the GRU weights in the decoder to the identity matrix. Biases are initialized to zero.\\nEvaluation metrics: (i) 1-best accuracy: the percentage of predictions that match the true answer exactly; (ii) average edit distance between prediction and true answer. The two metrics differ in that accuracy gives no partial credit and incorrect answers may be drastically different from the annotated form without incurring additional penalty. In contrast, edit distance gives partial credit for forms that are closer to the true answer.\\n**5.1 Exp. 1: Transfer Learning for Paradigm Completion**\\nIn this experiment, we investigate to what extent our model transfers morphological knowledge from a high-resource source language to a low-resource target language. We experimentally answer three questions. (i) Is transfer learning possible for morphology? (ii) How much annotated data do we need in the low-resource target language? (iii) How closely related must the two languages be to achieve good results?\\nData. Based on complete inflection tables from unimorph.org (Kirov et al., 2016), we create datasets as follows. Each training set consists of 12,000 samples in the high-resource source language and nt∈{50, 200} samples in the lowresource target language. We create target lan-\\nguage dev and test sets of sizes 1600 and 10,000, respectively.3 For Romance and Arabic, we create learning curves for nt∈{100, 400, 800, 1600, 3200, 6400, 12000}. Lemmata and inflections are randomly selected from all available paradigms.\\nResults and Discussion. Tab. 3 shows the effectiveness of transfer learning. There are two baselines. (i) “0”: no transfer, i.e., we consider only in-domain data; (ii) “AR”: Arabic, which is unrelated to all target languages.\\nWith the exception of the 200 sample case of ET→SME, cross-lingual transfer is always better than the two baselines; the maximum improvement is 0.58 (0.58 vs. 0.00) in accuracy for the 50 sample case of CA→ES. More closely related source languages improve performance more than distant ones. French, the Romance language least similar to Spanish, performs worst for →ES. For the target language Macedonian, Bulgarian provides most benefit. This can again be explained by similarity: Bulgarian is closer to Macedonian than the other languages in this group. The best result for Ukrainian is RU→UK. Unlike Polish and Slowenian, Russian is the only language in this group that uses the same script as Ukrainian, showing the importance of the alphabet for transfer. Still, the results also demonstrate that transfer works across alphabets (although not as well); this suggests that similar embeddings for similar characters have been learned. Finnish is the language that is closest to Estonian and it again performs best as a source language for Estonian. For Northern Sami, transfer works least well, probably because the distance between sources and target is largest in this case. The distance of the Sami languages from the Finnic (Estonian, Finnish) and Ugric (Hungarian) languages is much larger than the distances within Romance and within Slavic.4 However, even for Northern Sami, adding an additional language is still always beneficial compared to the monolingual baseline.\\nLearning curves for Romance and Arabic further support our finding that language similarity is important. In Fig. 2, knowledge is transferred to Spanish, and a baseline – a model trained only on Spanish data – shows the accuracy obtained without any transfer learning. Here, Catalan and Italian help the most, followed by Portuguese, French and,\\n3For Estonian, we use 7094 (not 12,000) train and 5000 (not 10,000) test samples as more data is unavailable.\\n4We have enlisted the expert for Uralic at our university and are in the process of analyzing SME results in more detail.\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nRomance Slavic I Slavic II Uralic I Uralic II source 0 AR PT CA IT FR 0 AR RU BG CS 0 AR RU PL SL 0 AR FI HU ET 0 AR FI HU SME target →ES →MK →UK →SME →ET\\n5 0 acc 0.00 0.04 0.48 0.58 0.46 0.29 0.00 0.00 0.23 0.47 0.13 0.01 0.01 0.47 0.16 0.07 0.00 0.00 0.03 0.01 0.01 0.02 0.01 0.35 0.21 0.17\\nED 5.42 4.06 0.85 0.80 1.15 1.82 5.71 5.59 1.61 0.87 2.32 5.23 4.80 0.77 2.14 3.12 8.47 8.64 5.41 6.53 7.03 4.50 4.51 1.55 2.19 2.60 2 0 0 acc 0.38 0.54 0.62 0.78 0.74 0.60 0.21 0.40 0.62 0.77 0.57 0.16 0.21 0.64 0.55 0.50 0.05 0.09 0.20 0.18 0.06 0.34 0.53 0.74 0.71 0.66 ED 1.37 0.87 0.57 0.78 0.44 0.82 1.93 1.12 0.68 0.36 0.72 2.09 1.60 0.49 0.73 0.82 5.43 4.93 3.20 3.37 5.01 1.47 0.98 0.41 0.48 0.62\\nTable 3: Accuracy (acc) and edit distance (ED) of cross-lingual transfer learning for paradigm completion. The target language is indicated by “→”, e.g., it is Spanish for “→ES”. Sources are indicated in the row “source”; “0” is the monolingual case. Except for Estonian, we train on ns = 12,000 source samples and nt ∈ {50, 200} target samples (as indicated by the row). There are two baselines in the table. (i) “0”: no transfer, i.e., we consider only in-domain data; (ii) “AR”: the Semitic language Arabic is unrelated to all target languages and functions as a dummy language that is unlikely to provide relevant information. All languages are denoted using the official codes (SME=Northern Sami).\\nfinally, Arabic. This corresponds to the order of lexical similarity with Spanish, except for the performance of Portuguese (cf. Tab. 2). A possible explanation is the potentially confusing overlap of lemmata between the two languages – cf. discussion in the next subsection. That the transfer learning setup improves performance for the unrelated language Arabic as source is at first surprising. But adding new samples to a small training set helps prevent overfitting (e.g., rote memorization) even if the source is a morphologically unrelated language; effectively acting as a regularizer.5 This will also be discussed below.\\nError Analysis for Romance. Even for only 50 Spanish instances, many inflections are correctly produced in transfer. For, e.g., (criar, 3PlFutSbj) 7→ criaren, model outputs are: fr: criaren, ca: criaren, es: crntaron, it: criaren, ar: ecriren, pt: criaren (all correct except for the two baselines). Many errors involve accents, e.g., (contrastar, 2PlFutInd) 7→ contrastaréis; model outputs are: fr: contrastareis, ca: contrastareis, es: conterarı́an, it: contrastareis, ar: contastarı́as, pt: contrastareis. Some inflections all systems get wrong, mainly because of erroneously applying the inflectional rules of the source to the target. Finally, the output of the model trained on Portuguese contains a class of errors that are unlike those of other systems. Example: (contraatacar, 1SgCond) 7→ contraatacarı́a with those solutions: fr: contratacarı́am, ca: contraatacarı́a, es: concarnar, it: contratacé, ar: cuntatarı́a and pt: contra-atacarı́a. The Portuguese model inserts “-” because Portuguese train data contains contraatacar and “-” appears in its inflected form.6\\n5Following (Kann and Schütze, 2016b) we did not use standard regularizers. To verify that the effect of Arabic is a regularization effect, we ran a small monolingual experiment on ES (200 setting) with dropout 0.5 (Srivastava et al., 2014). The resulting accuracy is 0.57, very similar to the comparable Arabic number of 0.54 in the table.\\n6To investigate this in more detail we retrain the Portuguese model with 50 Spanish samples, but exclude all lemmata\\n0 PT CA IT FR AR →ES\\non e sh ot acc 0.00 0.44 0.39 0.23 0.13 0.00 ED 6.26 1.01 1.27 1.83 2.87 7.00\\nze ro sh ot acc 0.00 0.14 0.08 0.01 0.02 0.00 ED 7.18 1.95 1.99 3.12 4.27 7.50\\nTable 4: Results for one-shot and zero-shot transfer learning. Formatting is the same as for Tab. 3. We still use ns = 12000 source samples. In the one-shot (resp. zero-shot) case, we observe exactly one form (resp. zero forms) for each tag in the target language at training time.\\nAn example for the generally improved performance across languages for 200 Spanish training samples is (contrastar, 2PlIndFut) 7→ contrastaréis: all models now produce the correct form.\\n**5.2 Exp. 2: Zero-Shot/One-Shot Transfer**\\nIn §5.1, we investigated the relationship between indomain (target) training set size and performance. Here, we look at the extreme case of training set sizes 1 (one-shot) and 0 (zero-shot) for a tag. We train our model on a single sample for half of the tags appearing in the low-resource language, i.e., if T` is the set of morphological tags for the target language, train set size is |T`|/2. As before, we add 12,000 source samples.\\nWe report one-shot accuracy (resp. zero-shot accuracy), i.e., the accuracy for samples with a tag that has been seen once (resp. never) during training. Note that the model has seen the individual subtags each tag is composed of.7\\nData. Our experimental setup is similar to §5.1: we use the same dev, test and high-resource train sets as before. However, the low-resource data is created in the way specified above. To remove a potentially confounding variable, we impose the condition that no two training samples belong to\\nthat appear in Spanish train/dev/test, resulting in only 3695 training samples. Accuracy on test increases by 0.09 despite the reduced size of the training set.\\n7It is very unlikely that due to random selection a subtag will not be in train; this case did not occur in our experiments.\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nthe same lemma. Results and Discussion. Tab. 4 shows that the Spanish and Arabic systems do not learn anything useful for either half of the tags. This is not surprising as there is not enough Spanish data for the system to generalize well and Arabic does not contribute exploitable information. The systems trained on French and Italian, in contrast, get a nonzero accuracy for the zero-shot case as well as 0.13 and 0.23, respectively, in the one-shot case. This shows that a single training example is sometimes sufficient for successful generation although generalization to tags never observed is rarely possible. Catalan and Portuguese show the best performance in both settings; this is intuitive since they are the languages closest to the target (cf. Tab. 2). In fact, adding Portuguese to the training data yields an absolute increase in accuracy of 0.44 (0.44 vs. 0.00) for one-shot and 0.14 (0.14 vs. 0.00) for zero-shot with corresponding improvements in edit distance.\\nOverall, this experiment shows that with transfer learning from a closely related language the performance of zero-shot morphological generation improves over the monolingual approach, and, in the one-shot setting, it is possible to generate the right form nearly half the time.\\n**5.3 Exp. 3: True Transfer vs. Other Effects**\\nWe would like to separate the effects of regularization that we saw for Arabic from true transfer.\\nTo this end, we generate a random cipher (i.e., a function γ : Σ ∪ S 7→ Σ ∪ S) and apply it to all word forms and morphological tags of the high-resource train set; target language data are not changed. Ciphering makes it harder to learn true “linguistic” transfer of morphology. Consider the simplest case of transfer: an identical mapping in two languages, e.g., (visitar, 1SgPresInd) 7→ visito in both Portuguese and Spanish. If we transform Portuguese using the cipher γ(iostv...) = kltqa..., then visito becomes aktkql in Portuguese and its tag becomes similarly unrecognizable as being identical to the Spanish tag 1SgPresInd. Our intuition is that ciphering will disrupt transfer of morphology.8 On the other hand, the regularization effect we observed with Arabic should still be effective.\\nData. We use the Portuguese-Spanish and 8Note that ciphered input is much harder than transfer between two alphabets (Latin/Cyrillic) because it creates ambiguous input. In the example, Spanish “i” is totally different from Portuguese “i” (which is really “k”), but the model must use the same representation.\\nArabic-Spanish data from Experiment 1. We generate a random cipher and apply it to morphological tags and word forms for Portuguese and Arabic. The language tags are kept unchanged. Spanish is also not changed. For comparability with Tab. 3, we use the same dev and test sets as before.\\nResults and Discussion. Tab. 5 shows that performance of PT→ES drops a lot: from 0.48 to 0.09 for 50 samples and from 0.62 to 0.54 for 200 samples. This is because there are no overt similarities between the two languages left after applying the cipher, e.g., the two previously identical forms visito are now different.\\nThe impact of ciphering on AR→ES varies: slightly improved in one case (0.54 vs. 0.56), slightly worse in three cases. We also apply the cipher to the tags and Arabic and Spanish share subtags, e.g., Sg. Just the knowledge that something is a subtag is helpful because subtags must not be generated as part of the output. We can explain the tendency of ciphering to decrease performance on AR→ES by the “masking” of common subtags.\\nFor 200 samples and ciphering, there is no clear difference in performance between Portuguese and Arabic. However, for 50 samples and ciphering, Portuguese (0.09) seems to perform better than Arabic (0.02) in accuracy. Portuguese uses suffixation for inflection whereas Arabic is templatic and inflectional changes are not limited to the end of the word. This difference is not affected by ciphering. Perhaps even ciphered Portugese lets the model learn better that the beginnings of words just need to be copied. For 200 samples, the Spanish dataset may be large enough, so that ciphered Portuguese no longer helps in this regard.\\nComparing no transfer with transfer from a ciphered language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place.\\n**6 Related Work**\\nCross-lingual transfer learning has been used for many tasks: automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012), entity recog-\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\n0→ES PT→ES AR→ES orig ciph orig ciph\\n5 0 acc 0.00 0.48 0.09 0.04 0.02\\nED 5.42 0.85 3.25 4.06 4.62 2 0 0 acc 0.38 0.62 0.54 0.54 0.56\\nED 1.37 0.57 0.95 0.87 0.93\\nTable 5: Results for ciphering. “0→ES” and “orig” are original results, copied from Tab. 3; “ciph” is the result after the cipher has been applied.\\nnition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Padó and Lapata, 2005). The drawback is that machine translation errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010).\\nIn the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages.\\nWork on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-Markov conditional random field (Sarawagi and Cohen, 2004). Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2015; Kann and Schütze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art.\\nEncoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves\\nand Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation (MT): (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k ∈ {1, 2} and m = k. Other work has addressed cases with k > 2 or m > 2; this would be an interesting avenue of future research for paradigm completion. (iii) Whereas training RNNs in MT is hard, we only experienced one difficult issue in our experiments (due to the low-resource setting): regularization. (iv) Some work is word- or subword-based, our work is character-based. The same way that similar word embeddings are learned for the inputs cow and vache (French for “cow”) in MT, we expect similar embeddings to be learned for similar Cyrillic/Latin characters. (v) Similar to work in MT, we show that zero-shot (and, by extension, one-shot) learning is possible.\\n(Ha et al., 2016) (which was developed in parallel to our transfer model although we did not prepublish our paper on arxiv) is most similar to our work. Whereas Ha et al. (2016) address MT, we focus on the task of paradigm completion in low-resource settings and establish the state of the art for this problem.\\n   \n",
            "133                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       **2 Related Work**\\nText-based sentiment analysis systems can be broadly categorized into knowledge-based and statistics-based systems (Cambria, 2016). While the use of knowledge bases was initially more popular for the identification of emotions and polarity in text, sentiment analysis researchers have recently been using statistics-based approaches, with a special focus on supervised statistical methods (Pang et al., 2002; Socher et al., 2013).\\nIn 1970, Ekman (Ekman, 1974) carried out extensive studies on facial expressions which showed that universal facial expressions are able to provide sufficient clues to detect emotions. Recent studies on speech-based emotion analysis (Datcu and Rothkrantz, 2008) have focused on identifying relevant acoustic features, such as fundamental frequency (pitch), intensity of utterance, bandwidth, and duration.\\nAs for fusing audio and visual modalities for emotion recognition, two of the early works were done by De Silva et al. (De Silva et al., 1997) and Chen et al. (Chen et al., 1998). Both works showed that a bimodal system yielded a higher accuracy than any unimodal system. More recent research on audio-visual fusion for emotion recognition has been conducted at either feature level (Kessous et al., 2010) or decision level (Schuller, 2011).\\nWhile there are many research papers on audiovisual fusion for emotion recognition, only a few have been devoted to multimodal emotion or sentiment analysis using textual clues along with visual and audio modalities. Wollmer et al. (Wollmer et al., 2013) and Rozgic et al. (Rozgic et al., 2012a,b) fused information from audio, visual, and textual modalities to extract emotion and sentiment. Metallinou et al. (Metallinou et al., 2008) and Eyben et al. (Eyben et al., 2010a) fused audio and textual modalities for emotion recognition.\\nBoth approaches relied on a feature-level fusion. Wu et al. (Wu and Liang, 2011) fused audio and textual clues at decision level.\\n**3 Method**\\nIn this work, we propose a LSTM network that takes as input all utterances in a video and extracts contextual unimodal and multimodal features by modeling the dependencies among the input utterances. Below, we propose an overview of the method -\\n1. Context-Independent Unimodal UtteranceLevel Feature Extraction\\nFirst, the unimodal features are extracted without considering the contextual information of the utterances (Section 3.1). Table 1 presents the feature extraction methods used for each modality.\\n2. Contextual Unimodal and Multimodal Classification\\nThe context-independent unimodal features (from Step 1) are then fed into a LSTM network (termed contextual LSTM) that allows consecutive utterances in a video to share semantic information in the feature extraction process (which provides context-dependent unimodal and multimodal classification of the utterances). We experimentally show that this proposed framework improves the performance of utterance-level sentiment classification over traditional frameworks.\\nVideos, comprising of its constituent utterances, serve as the input. We represent the dataset as U:\\nU = ⎡⎢⎢⎢⎢⎢⎣ u1,1 u1,2 u1,3 ... u1,L1 u2,1 u2,2 u2,3 ... u2,L2 . . . ... . uM,1 uM,2 uM,3 ... uM,LM ⎤⎥⎥⎥⎥⎥⎦ .\\nHere, ui,j denotes the jth utterance of the ith video and L = [L1, L2, ..., LM ] represents the number of utterances per video in the dataset set.\\n**3.1 Extracting Context-Independent Unimodal Features**\\nInitially, the unimodal features are extracted from each utterance separately, i.e., we do not consider the contextual relation and dependency among the utterances (Table 1). Below, we explain the textual, audio, and visual feature extraction methods.\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\n**3.1.1 text-CNN: Textual Features Extraction**\\nFor feature extraction from textual data, we use a convolutional neural network (CNN).\\nThe idea behind convolution is to take the dot product of a vector of k weights, wk, known as kernel vector, with each k-gram in the sentence s(t) to obtain another sequence of features c(t) = (c1(t), c2(t), . . . , cL(t)):\\ncj = wTk ⋅ xi∶i+k−1. We then apply a max pooling operation over the feature map and take the maximum value ĉ(t) = max{c(t)} as the feature corresponding to this particular kernel vector. We use varying kernel vectors and window sizes to obtain multiple features.\\nThe process of extracting textual features is as follows -\\nFirst, we represent each sentence as the concatenation of vectors of the constituent words. These vectors are the publicly available 300-dimensional word2vec vectors trained on 100 billion words from Google News (Mikolov et al., 2013). The convolution kernels are thus applied to these word vectors instead of individual words. Each sentence is wrapped to a window of 50 words which serves as the input to the CNN.\\nThe CNN has two convolutional layers - the first layer having a kernel size of 3 and 4, with 50 feature maps each and a kernel size 2 with 100 feature maps for the second. The convolution layers are interleaved with pooling layers of dimension 2. We use ReLU as the activation function. The convolution of the CNN over the sentence learns abstract representations of the phrases equipped with implicit semantic information, which with each successive layer spans over increasing number of words and ultimately the entire sentence.\\nModality Model\\nText text-CNN: Deep Convolutional NeuralNetwork with word embeddings\\nVideo 3d-CNN: 3-dimensional CNNs employed onutterances of the videos Audio openSMILE: Extracts low level audiodescriptors from the audio modality\\nTable 1: Methods for extracting context independent baseline features from different modalities.\\n**3.1.2 Audio Feature Extraction**\\nAudio features are extracted in 30 Hz frame-rate; we use a sliding window of 100 ms. To compute the features, we use the open-source software\\nopenSMILE (Eyben et al., 2010b) which automatically extracts pitch and voice intensity. Voice normalization is performed and voice intensity is thresholded to identify samples with and without voice. Z-standardization is used to perform voice normalization.\\nThe features extracted by openSMILE consist of several low-level descriptors (LLD) and their statistical functionals. Some of the functionals are amplitude mean, arithmetic mean, root quadratic mean, etc. Taking into account all functionals of each LLD, we obtained 6373 features.\\n**3.1.3 Visual Feature Extraction**\\nWe use 3D-CNN to obtain visual features from the video. We hypothesize that 3D-CNN will not only be able to learn relevant features from each frame, but will also be able to learn the changes among given number of consecutive frames.\\nIn the past, 3D-CNN has been successfully applied to object classification on 3D data (Ji et al., 2013). Its ability to achieve state-of-the-art results motivated us to use it.\\nLet vid ∈ Rc×f×h×w be a video, where c = number of channels in an image (in our case c = 3, since we consider only RGB images), f = number of frames, h = height of the frames, and w = width of the frames. Again, we consider the 3D convolutional filter filt ∈ Rfm×c×fl×fh×fw, where fm = number of feature maps, c = number of channels, fd = number of frames (in other words depth of the filter), fh = height of the filter, and fw = width of the filter. Similar to 2D-CNN, filt slides across video vid and generates output convout ∈ Rfm×c×(f−fd+1)×(h−fh+1)×(w−fw+1). Next, we apply max pooling to convout to select only relevant features. The pooling will be applied only to the last three dimensions of the array convout.\\nIn our experiments, we obtained best results with 32 feature maps (fm) with the filter-size of 5 × 5 × 5 (or fd × fh × fw). In other words, the dimension of the filter is 32 × 3 × 5 × 5 × 5 (or fm × c × fd × fh × fw). Subsequently, we apply max pooling on the output of convolution operation, with window-size being 3×3×3. This is followed by a dense layer of size 300 and softmax. The activations of this dense layer are finally used as the video features for each utterance.\\n**3.2 Context-Dependent Feature Extraction**\\nWe hypothesize that, within a video, there is a high probability of utterance relatedness with respect\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nto their sentimental and emotional clues. Since most videos tend to be about a single topic, the utterances within each video are correlated, e.g., due to the development of the speaker’s idea, coreferences, etc. This calls for a model which takes into account such inter-dependencies and the effect these might have on the current utterance. To capture this flow of informational triggers across utterances, we use a LSTM-based recurrent network scheme (Gers, 2001).\\n**3.2.1 Long Short-Term Memory**\\nLSTM is a kind of recurrent neural network (RNN), an extension of conventional feed-forward neural network. Specifically, LSTM cells are capable of modeling long-range dependencies, which other traditional RNNs fail to do given the vanishing gradient issue. Each LSTM cell consists of an input gate i, an output gate o, and a forget gate f , which enables it to remember the error during the error propagation. Current research (Zhou et al., 2016) indicates the benefit of using such networks to incorporate contextual information in the classification process.\\nIn our case, the LSTM network serves the purpose of context-dependent feature extraction by modeling relations among utterances. We term our architecture ‘contextual LSTM’. We propose several architectural variants of it later in the paper.\\nLSTM\\nsc-L ST M\\nUtterance 1 Utterance 2 Utterance !\\nContext Sensitive Utterance features\\nDense Layer\\nSoftmax Output\\nLSTM LSTM\\nContext Independent Utterance features\\nFigure 1: Contextual LSTM network: input features are passed through an unidirectional LSTM layer, followed by a dense layer and then a softmax layer. Categorical cross entropy loss is taken for training. The dense layer activations serve as the output features.\\n**3.2.2 Contextual LSTM Architecture**\\nLet unimodal features have dimension k, each utterance is thus represented by a feature vector xi,t ∈ Rk, where t represents the tth utterance of the video i. For a video, we collect the vectors for all the utterances in it, to get Xi = [xi,1,xi,2, ...,xi,Li] ∈ RLi×k, where Li represents the number of utterances in the video. This matrix Xi serves as the input to the LSTM. Figure 1 demonstrates the functioning of this LSTM module.\\nIn the procedure getLstmFeatures(Xi) of Algorithm 1, each of these utterance xi,t is passed through a LSTM cell using the equations mentioned in line 32 to 37. The output of the LSTM cell hi,t is then fed into a dense layer and finally into a softmax layer (line 38 to 39). The activations of the dense layer zi,t are used as the contextdependent features of contextual LSTM.\\n**3.2.3 Training**\\nThe training of the LSTM network is performed using categorical cross entropy on each utterance’s softmax output per video, i.e.,\\nloss = 1 N\\nN\\n∑ n=1\\nC\\n∑ c=1 yn,c log2( ˆyn,c),\\nwhere N = total number of utterances in a video, yn,c = original output of class c, and ˆyn,c = predicted output.\\nA dropout layer between the LSTM cell and dense layer is introduced to check overfitting. As the videos do not have same the number of utterances, padding is introduced to serve as neutral utterances. To avoid the proliferation of noise within the network, masking is done on these padded utterances to eliminate their effect in the network. Parameter tuning is done on the train set by splitting it into train and validation components with 80/20% split. RMSprop has been used as the optimizer which is known to resolve Adagrad’s radically diminishing learning rates (Duchi et al., 2011). After feeding the train set to the network, the test set is passed through it to generate their context-dependent features.\\nDifferent Network Architectures We consider the following variants of the contextual LSTM architecture in our experiments -\\nsc-LSTM This variant of the contextual LSTM architecture consists of unidirectional\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nLSTM cells. As this is the simple variant of the contextual LSTM, we termed it as simple contextual LSTM (sc-LSTM)\\nh-LSTM We also test on an architecture where the dense layer after the LSTM cell is omitted. Thus, the output of the LSTM cell hi,t provides our context-dependent features and the softmax layer provides the classification. We call this architecture hidden LSTM (h-LSTM).\\nbc-LSTM Bi-directional LSTMs are two unidirectional LSTMs stacked together having opposite directions. Thus, an utterance can get information from other utterances occurring before and after itself in the video. We replaced the regular LSTM with a bi-directional LSTM and named the resulting architecture as bi-directional contextual LSTM (bc-LSTM). The training process of this architecture is similar to sc-LSTM.\\nuni-SVM In this setting, we first obtain the unimodal features as explained in Section 3.1, concatenate them and then send to a SVM for the final classification. It should be noted that using a gated recurrent unit (GRU) instead of LSTM did not improve the performance.\\n**3.3 Fusion of Modalities**\\nWe accomplish multimodal fusion in two different ways as explained below -\\n**3.3.1 Non-hierarchical Framework**\\nIn non-hierarchical framework, we concatenate context-independent unimodal features (from Section 3.1) and feed that into the contextual LSTM networks, i.e., sc-LSTM, bc-LSTM, and h-LSTM.\\n**3.3.2 Hierarchical Framework**\\nContextual unimodal features, taken as input, can further improve performance of the multimodal fusion framework explained in Section 3.3.1. To accomplish this, we propose a hierarchical deep network which comprises of two levels –\\nLevel-1: context-independent unimodal features (from 3.1) are fed to the proposed LSTM network (Section 3.2.2) to get context-sensitive unimodal feature representations for each utterance. Individual LSTM networks are used for each modality.\\nLevel-2: consists of a contextual LSTM network similar to Level-1 but independent in training and computation. Output from each LSTM network in Level-1 are concatenated and fed into\\nthis LSTM network, thus providing an inherent fusion scheme - the prime objective of this level (Fig 2). The performance of the second level banks on the quality of the features from the previous level, with better features aiding the fusion process. Algorithm 1 describes the overall computation for utterance classification. For the hierarchical framework, we train Level 1 and Level 2 successively but separately.\\nWeight Bias Wi,Wf ,Wc,Wo ∈ Rd×k bi, bf , bc, bo ∈ Rd Pi, Pf , Pc, PoVo ∈ Rd×d bz ∈ Rm\\nWz ∈ Rm×d bsft ∈ Rc Wsft ∈ Rc×m\\nTable 2: Summary of notations used in Algorithm 1. Note: d - dimension of hidden unit. k - dimension of input vectors to LSTM layer . c - number of classes.\\n**4 Experimental Results**\\n\\n**4.1 Dataset details**\\nMost of the research in multimodal sentiment analysis is performed on datasets with speaker overlap in train and test splits.\\nBecause each individual has a unique way of expressing emotions and sentiments, finding generic, person-independent features for sentimental analysis is very tricky. In real-world applications, the\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nAlgorithm 1 Proposed Architecture 1: procedure TRAINARCHITECTURE( U, V) 2: Train context-independent models with U 3: for i:[1,M] do ▷ extract baseline features 4: for j:[1,Li] do 5: xi,j ← TextFeatures(ui,j) 6: x ′\\ni,j ← V ideoFeatures(ui,j) 7: x”i,j ← AudioFeatures(ui,j) 8: Unimodal: 9: Train LSTM at Level-1 with X,X ′ andX”.\\n10: for i:[1,M] do ▷ unimodal features 11: Zi ← getLSTMFeatures(Xi) 12: Z ′ i ← getLSTMFeatures(X ′\\ni) 13: Z”i ← getLSTMFeatures(X”i ) 14: Multimodal: 15: for i:[1,M] do 16: for j:[1,Li] do 17: if Non-hierarchical fusion then 18: x∗i,j ← (xi,j ∣∣x ′\\ni,j ∣∣x”i,j) ▷ concatenation\\n19: else 20: if Hierarchical fusion then 21: x∗i,j ← (zi,j ∣∣z ′\\ni,j ∣∣z”i,j) ▷ concatenation 22: Train LSTM at Level-2 with X∗. 23: for i:[1,M] do ▷ multimodal features 24: Z∗i ← getLSTMFeatures(X∗i ) 25: testArchitecture( V) 26: return Z∗\\n27: procedure TESTARCHITECTURE( V) 28: Similar to training phase. V is passed through the\\nlearnt models to get the features and classification outputs. Table 2 shows the trainable parameters.\\n29: procedure GETLSTMFEATURES(Xi) ▷ for ith video 30: Zi ← φ 31: for t:[1,Li] do ▷ Table 2 provides notation 32: it ← σ(Wixi,t + Pi.ht−1 + bi) 33: C̃t ← tanh(Wcxi,t + Pcht−1 + bc) 34: ft ← σ(Wfxt + Pfht−1 + bf) 35: Ct ← it ∗ C̃t + ft ∗Ct−1 36: ot ← σ(Woxt + Poht−1 + VoCt + bo) 37: ht ← ot ∗ tanh(Ct) ▷ output of lstm cell 38: zt ← ReLU(Wzht + bz) ▷ dense layer 39: prediction← softmax(Wsftzt + bsft) 40: Zi ← Zi ∪ zt 41: return Zi\\nmodel should be robust to person variance but it is very difficult to come up with a generalized model from the behavior of a limited number of individuals To this end, we perform person-independent experiments to emulate unseen conditions. Our train/test splits of the datasets are completely disjoint with respect to speakers.\\nWhile testing, our models have to classify emotions and sentiments from utterances by speakers they have never seen before.\\nIEMOCAP: The IEMOCAP contains the acts of 10 speakers in a two way conversation segmented into utterances. The database contains the following categorical labels: anger, happiness, sadness, neutral, excitement, frustration, fear, surprise, and other, but we take only the first four so as to compare with the state of the art (Rozgic et al., 2012b) and other authors. Videos by the first 8 speakers are considered in the train set. The train/test split details are provided in table 3.\\nMOSI: The MOSI dataset is a dataset rich in sentimental expressions where 93 persons review topics in English. It contains positive and negative classes as its sentiment labels. The train/validation set comprises of the first 62 individuals in the dataset.\\nMOUD: This dataset contains product review videos provided by around 55 persons. The reviews are in Spanish (we use Google Translate API1 to get the english transcripts). The utterances are labeled to be either positive, negative or neutral. However, we drop the neutral label to maintain consistency with previous work. The first 59 videos are considered in the train/val set.\\nTable 3 provides information regarding train/test split of all the datasets. In these splits it is ensured that 1) No two utterances from the train and test splits belong to the same video. 2) The train/test splits have no speaker overlap. This provides the speaker-independent setting.\\nTable 3 also provides cross dataset split details where the complete datasets of MOSI and MOUD are used for training and testing respectively. The proposed model being used on reviews from different languages allows us to analyze its robustness and generalizability.\\nIt should be noted that the datasets’ individual configuration and splits are same throughout all the experiments (i.e., context-independent unimodal feature extraction, LSTM-based context-\\n1http://translate.google.com\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\ndependent unimodal and multimodal feature extraction and classification).\\n**4.2 Performance of Different Models and Comparisons**\\nIn this section, we present unimodal and multimodal sentiment analysis performance of different LSTM network variants as explained in Section 3.2.3 and comparison with the state of the art.\\nHierarchical vs Non-hierarchical Fusion Framework - As expected, trained contextual unimodal features help the hierarchical fusion framework to outperform the non-hierarchical framework. Table 4 demonstrates this by comparing both hierarchical and non-hierarchical framework using the bc-LSTM network. Due to this fact, we provide all further analysis and results using the hierarchical framework. Nonhierarchical model outperforms the performance of the baseline uni-SVM. This further leads us to conclude that it is the context-sensitive learning paradigm which plays the key role in improving performance over the baseline.\\nComparison among Network Variants - It is to be noted that both sc-LSTM and bc-LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets. Since, bcLSTM has access to both the preceding and following information of the utterance sequence, it performs consistently better on all the datasets over sc-LSTM.\\nThe usefulness of the dense layer in improving the performance is prominent from the experimental results as shown in Table 4. The performance improvement is in the range of 0.3% to 1.5% on MOSI and MOUD datasets. On the IEMOCAP dataset, the performance improvement of bc-LSTM and sc-LSTM over h-LSTM is in the range of 1% to 5%.\\nComparison with the Baseline and state of the art - Every LSTM network variant has outperformed the baseline uni-SVM on all the datasets by the margin of 2% to 5%(see Table 4). These results prove our initial hypothesis that modeling the contextual dependencies among utterances, which uni-SVM cannot do, improves the classification. The higher performance improvement on the IEMOCAP dataset indicates the necessity of modeling long-range dependencies among the utterances as continuous emotion recognition\\nis a multiclass sequential problem where a person doesnt frequently change emotions (Wöllmer et al., 2008).\\nWe have implemented and compared with the current state-of-the-art approach proposed by Poria et al. (Poria et al., 2015). In their method, they extracted features from each modality and fed to a multiple kernel learning (MKL) classifier. However, they did not conduct the experiment in speaker-independent manner and also did not consider the contextual relation among the utterances. Experimental results in Table 5 shows that the proposed method has outperformed Poria et al. (Poria et al., 2015) by a significant margin. For the emotion recognition task, we have compared our method with the current state of the art (Rozgic et al., 2012b), who extracted features in a similar fashion to (Poria et al., 2015) did. However, for fusion they used SVM trees.\\n**4.3 Importance of the Modalities**\\nAs expected, in all kinds of experiments, bimodal and trimodal models have outperformed unimodal models. Overall, audio modality has performed better than visual on all the datasets. On MOSI and IEMOCAP datasets, textual classifier achieves the best performance over other unimodal classifiers. On IEMOCAP dataset, the unimodal and multimodal classifiers obtained poor performance to classify neutral utterances. Textual modality, combined with non-textual modes boosts the performance in IEMOCAP by a large margin. However, the margin is less in the other datasets.\\nOn the MOUD dataset, textual modality performs worse than audio modality due to the noise introduced in translating Spanish utterances to English. Using Spanish word vectors2 in text-CNN results in an improvement of 10% . Nonetheless, we report results using these translated utterances as opposed to utterances trained on Spanish word vectors, in order to make fair comparison with (Poria et al., 2015).\\n**4.4 Generalization of the Models**\\nTo test the generalizability of the models, we have trained our framework on complete MOSI dataset and tested on MOUD dataset (Table 6). The performance was poor for audio and textual modality as the MOUD dataset is in Spanish while the model is trained on MOSI dataset which is in En-\\n2http://crscardellino.me/SBWCE\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nModality MOSI MOUD IEMOCAP hierarchical (%)\\nno n-\\nhi er\\n(% ) hierarchical (%)\\nno n-\\nhi er\\n(% ) hierarchical (%)\\nno n-\\nhi er\\n(% )\\nun i-\\nSV M\\nhL\\nST M\\nsc -L\\nST M\\nbc -L\\nST M\\nun i-\\nSV M\\nhL\\nST M\\nsc -L\\nST M\\nbc -L\\nST M\\nun i-\\nSV M\\nhL\\nST M\\nsc -L\\nST M\\nbc -L\\nST M\\nT 75.5 77.4 77.6 78.1 49.5 50.1 51.3 52.1 65.5 68.9 71.4 73.6 V 53.1 55.2 55.6 55.8 46.3 48.0 48.2 48.5 47.0 52.0 52.6 53.2 A 58.5 59.6 59.9 60.3 51.5 56.3 57.5 59.9 52.9 54.4 55.2 57.1\\nT + V 76.7 78.9 79.9 80.2 78.5 50.2 50.6 51.3 52.2 50.9 68.5 70.3 72.3 75.4 73.2 T + A 75.8 78.3 78.8 79.3 78.2 53.1 56.9 57.4 60.4 55.5 70.1 74.1 75.2 75.6 74.5 V + A 58.6 61.5 61.8 62.1 60.3 62.8 62.9 64.4 65.3 64.2 67.6 67.8 68.2 68.9 67.3 T + V + A 77.9 78.1 78.6 80.3 78.1 66.1 66.4 67.3 68.1 67.0 72.5 73.3 74.2 76.1 73.5\\nTable 4: Comparison of models mentioned in Section 3.2.3. The table reports the accuracy of classification. Note: non-hier ← Non-hierarchical bc-lstm. For remaining fusion hierarchical fusion framework is used (Section 3.3.2)\\nModality Sentiment (%) Emotion on IEMOCAP (%)MOSI MOUD angry happy sad neutral T 78.12 52.17 76.07 78.97 76.23 67.44 V 55.80 48.58 53.15 58.15 55.49 51.26 A 60.31 59.99 58.37 60.45 61.35 52.31\\nT + V 80.22 52.23 77.24 78.99 78.35 68.15 T + A 79.33 60.39 77.15 79.10 78.10 69.14 V + A 62.17 65.36 68.21 71.97 70.35 62.37 A + V + T 80.30 68.11 77.98 79.31 78.30 69.92 State-of 73.551 63.251 73.10 2 72.402 61.902 58.102-the-art 1by (Poria et al., 2015),2by (Rozgic et al., 2012b)\\nTable 5: Accuracy % on textual (T), visual (V), audio (A) modality and comparison with the state of the art. For fusion, hierarchical fusion framework was used (Section 3.3.2)\\nModality MOSI→MOUDuni-SVM h-LSTM sc-LSTM bc-LSTM T 46.5% 46.5% 46.6% 46.9% V 43.3% 45.5% 48.3% 49.6% A 42.9% 46.0% 46.4% 47.2%\\nT + V 49.8% 49.8% 49.8% 49.8% T + A 50.4% 50.9% 51.1% 51.3% V + A 46.0% 47.1% 49.3% 49.6% T + V + A 51.1% 52.2% 52.5% 52.7%\\nTable 6: Cross Dataset comparison. The table reports the accuracy of classification.\\nglish language. However, notably visual modality performs better than other two modalities in this experiment which signifies that in cross-lingual scenarios facial expressions carry more generalized, robust information than audio and textual modalities. We could not carry out the similar experiment for emotion recognition as no other utterance-level dataset apart from the IEMOCAP was available at the time of our experiments.\\n**4.5 Qualitative Analysis**\\nIn some cases the predictions of the proposed method are wrong given the difficulty in recognizing the face and noisy audio signal in the utterances. Also, cases where the sentiment is very weak and non contextual, the proposed approach shows some bias towards its surrounding utter-\\nances which further leads to wrong predictions. This can be solved by developing a context aware attention mechanism. In order to have a better understanding on roles of modalities for overall classification, we also have done some qualitative analysis. For example, this utterance - ”who doesn’t have any presence or greatness at all.”, was classified as positive by the audio classifier (“doesn’t” was spoken normally by the speaker, but “presence and greatness at all” was spoken with enthusiasm). However, textual modality caught the negation induced by “doesn’t” and classified correctly. In another utterance “amazing special effects” as there was no jest of enthusiasm in speaker’s voice and face audio-visual classifier failed to identify the positivity of this utterance. On the other textual classifier correctly detected the polarity as positive.\\nOn the other hand, textual classifier classified this sentence - “that like to see comic book characters treated responsibly” as positive, possibly because of the presence of positive phrases such as “like to see”, “responsibly”. However, the high pitch of anger in the person’s voice and the frowning face helps identify this to be a negative utterance.\\n   \n",
            "134                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 **2 Related Work**\\n\\n**2.1 Automatic Keyphrase Extraction**\\nKeyphrase provides a succinct and accurate way of describing a subject or a subtopic in a document. A number of extraction algorithms have been proposed, and typically the process of extracting can be broken down into two steps.\\nThe first step is to generate a list of phrase candidates with heuristic methods. As these candidates are prepared for further filtering, a considerable amount of candidates are produced in this step to increase the possibility that most of the correct keyphrases are kept. The primary ways of extracting candidates include retaining word sequences that match certain part-of-speech tag patterns (e.g., nouns, adjectives) (Liu et al., 2011;\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\nWang et al., 2016; Le et al., 2016), and extracting important n-grams or noun phrases (Hulth, 2003; Medelyan et al., 2008).\\nThe second step is to score each candidate phrase regarding its likelihood of being a keyphrase in the given document. The top-ranked candidates are returned as keyphrases. Both supervised and unsupervised machine learning methods are widely employed here. For supervised methods, this task is solved as a binary classification problem, and various of learning methods and features have been explored (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009b; Lopez and Romary, 2010; Gollapalli and Caragea, 2014). As for the unsupervised approaches, primary ideas include finding the central nodes in text graph (Mihalcea and Tarau, 2004; Grineva et al., 2009), detecting representative phrases from topical clusters (Liu et al., 2009, 2010) and so on.\\nAside from the commonly adopted two steps process, another two previous studies realized the keyphrase extraction in entirely different ways. Tomokiyo and Hurst (2003) applied two language models to measure the phraseness and informativeness of phrases. Liu et al. (2011) share the most similar idea to our work. They used a word alignment model, which learns the translation from the documents to the keyphrases. This approach alleviates the problem of vocabulary gap between source and target to a certain degree. However, this translation model can hardly deal with semantic meaning. Additionally, they trained the model with the target of title/summary to enlarge the number of training samples, which may diverge away from the real objective of generating keyphrases.\\n**2.2 Encoder-Decoder Model**\\nThe RNN Encoder-Decoder model (also referred as Sequence-to-Sequence Learning) is an end-toend approach. It was firstly introduced by Cho et al. (2014) and Sutskever et al. (2014) to solve translation problems. As it provides a powerful tool for modeling variable-length sequence in an end-to-end fashion, it fits many Natural Language Processing tasks and soon achieves great successes (Rush et al., 2015; Vinyals et al., 2015; Serban et al., 2016).\\nDifferent strategies have been explored to improve the performance of Encoder-Decoder model. The attention mechanism (Bahdanau et al.,\\n2014) is a soft alignment approach that allows the model to automatically locate the relevant input component. In order to make use of the important information in the source text, some studies sought ways to copy certain parts of content from the source text and paste them into target text (Allamanis et al., 2016; Gu et al., 2016; Zeng et al., 2016). There exists a discrepancy between the optimizing objective during training and the metrics during evaluation. A few studies attempted to eliminate this discrepancy by incorporating new training algorithm (Marc’Aurelio Ranzato et al., 2016) or modifying optimizing objective(Shen et al., 2016).\\n**3 Methodology**\\nThis section will introduce in detail our proposed deep keyphrase generation. Firstly, the task of keyphrase generation is defined, followed by the overview of how we apply the RNN EncoderDecoder model. Details of the framework as well as the copy mechanism will be introduced in Section 3.3 and 3.4.\\n**3.1 Problem Definition**\\nGiven a keyphrase dataset consisting of N data samples, the i-th data sample (x(i),p(i)) contains one source text x(i), and Mi target keyphrases p(i) = (p(i,1),p(i,2), . . . ,p(i,Mi)). Both the source text x(i) and keyphrase p(i,j) are sequences of words:\\nx(i) = x (i) 1 , x (i) 2 , . . . , x (i) L xi\\np(i,j) = y (i,j) 1 , y (i,j) 2 , . . . , y (i,j) L p(i,j)\\nLx(i) and Lp(i,j)denotes the length of word sequence of x(i) and p(i,j) respectively.\\nNow for each data sample, there are one source text sequence and multiple target phrase sequences. To apply the RNN Encoder-Decoder model, the data need to be converted into textkeyphrase pairs which contain only one source sequence and one target sequence. One simple way is to split the data sample (x(i),p(i)) intoMi pairs: (x(i),p(i,1)), (x(i),p(i,2)), . . . , (x(i),p(i,Mi)). Then the Encoder-Decoder model is ready to be applied to learn the mapping from source sequence to target sequence. For the purpose of simplicity, (x,y) is used to denote each data pair in the rest of this section, where x is the word sequence of a source text and y is the word sequence of its keyphrase.\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\n**3.2 Encoder-Decoder Model**\\nThe basic idea of our keyphrase generation model is to compress the content of source text into a hidden representation with encoder, and generate corresponding keyphrases by the decoder based on the representation . Both the encoder and decoder are implemented with recurrent neural networks (RNN).\\nThe encoder RNN converts the variable-length input sequence x = (x1, x2, ..., xT ) into a set of hidden representation h = (h1, h2, . . . , hT ), by iterating the following equations along the time t:\\nht = f (xt,ht−1) (1)\\nwhere f is a non-linear function. And we get the context vector c, acting as the representation of the whole input x, through a non-linear function q.\\nc = q(h1, h2, ..., hT ) (2)\\nThe decoder is another RNN, decompresses the context vector and generates a variable-length sequence y = (y1, y2, ..., yT ′) word by word, through a conditional language model:\\nst = g(yt−1, st−1, c) p(yt|y1,...,t−1,x) = g(yt−1, st, c) (3)\\nwhere st is the hidden state of decoder RNN at time t. The non-linear function g is a softmax classifier which outputs the probabilities of all the words in the vocabulary. yt is the predicted word at time t, by taking the word with largest probability after g(·).\\nThe encoder and decoder networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence. After training, we use the beam search to generate phrases and a max heap is maintained to get the predictions with highest probabilities.\\n**3.3 Details of Encoder and Decoder**\\nA bidirectional Gated Recurrent Unit (GRU) is applied as our encoder to replace the simple recurrent neural network. Previous studies (Bahdanau et al., 2014; Cho et al., 2014) indicate it can provide a general better performance language modeling than simple RNN and a simpler structure than Long Short-term Memory Networks(Hochreiter and Schmidhuber, 1997). So the above non-linear function f is replaced by the GRU function (see in (Cho et al., 2014)).\\nAnother forward GRU is utilized as the decoder. In addition, an attention mechanism is adopted to improve the performance. The attention mechanism was firstly introduced by Bahdanau et al. (2014) to make the model focus on the important parts in input dynamically. The context vector c is computed as a weighted sum of hidden representation h = (h1, . . . , hT ):\\nci = T∑ j=1 αijhj αij = exp(a(si−1, hj))∑T k=1 exp(a(si−1, hk))\\n(4)\\nwhere a(si−1, hj) is a soft alignment function that measures the similarity between si−1 and hj , namely to which degree the inputs around position j and the output at position i match.\\n**3.4 Copy Mechanism**\\nTo ensure the quality of learned representation and reduce the size of vocabulary, typically the RNN model only considers a certain number of frequent words (e.g. 30,000 words in (Cho et al., 2014)), but a large amount of long-tail words are simply ignored. Therefore the RNN is not able to predict any keyphrase which contains out-of-vocabulary words. Actually, the important phrases can also be identified by their syntactic and location features, even though we don’t know the meanings. The copy mechanism is one feasible solution that enables RNN to predict unknown words based on contextual features.\\nBy incorporating the copy mechanism, the probability of predicting each new word yt would consist of two parts. The first term is the probability of generating the term (see Equation 3) and the second one is the probability of copying it from source text:\\np(yt|y1,...,t−1,x) = pg(yt|y1,...,t−1,x) + pc(yt|y1,...,t−1,x) (5)\\nSimilar to attention mechanism, the copy mechanism pinpoints the appearance of yt−1 in source text, and use its location information (ρtτ ) to compute the weighted hidden representation (ζ(yt−1)) of input x.\\nζ(yt−1) = TS∑ τ=1 ρtτhτ (6)\\nρtτ = { 1 K p(xτ , c|st−1) xτ = yt−1 0 Otherwise\\n(7)\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nwhere K is the normalization term. Subsequently, we obtain the probabilities of copying pc(yt|y1,...,t−1) following the Equation 3, in which the target words are replaced by the words in source text, and the vector of yt−1 is replaced by [e(yt−1), ζ(yt−1)]\\nT , where the e(yt−1) is the embedding of last predicted word.\\n**4 Experiment Settings**\\nThis section starts with discussing how we design our evaluation experiments, followed by the description of training and testing datasets. Then, we introduce evaluation metrics and baselines.\\n**4.1 Training Dataset**\\nThere are several publicly-available datasets for evaluating keyphrase generation. The largest one came from Krapivin et al. (2008), which contained 2,304 scientific publications. However, this amount of data is unable to train a robust recurrent neural network model. In fact, there are millions of scientific papers available online, each of which contains the keyphrases assigned by authors. Therefore, we collected a large amount of high-quality scientific metadata in Computer Science domain from various online digital libraries, including ACM Digital Library, ScienceDirect, Wiley, Web of Science, and so on. In total, we obtained 567,830 articles after removing duplicates and overlaps, which is 200 times larger than the one of Krapivin et al. (2008). Note that our model is only trained on 527,830 articles since 40,000 publications are held out for training baselines and for building a test dataset.\\n**4.2 Testing Datasets**\\nFor evaluating the proposed model more comprehensively, four widely-adopted scientific publication datasets are used. In addition, since these datasets only contain few hundreds or thousands of publications, we contribute a new testing dataset KP20k with a much larger number of scientific articles. We take the title and abstract as the source text. Each dataset is described in details in the below text.\\n– Inspec (Hulth, 2003): This dataset provides 2,000 paper abstracts. We adopt the 500 testing papers and the corresponding uncontrolled keyphrases for evaluation, and the remaining 1,500 papers are used for training the supervised baseline models.\\n– Krapivin (Krapivin et al., 2008): This dataset provides 2,304 papers with full-text and author-assigned keyphrases. However, the author did not mention how to split testing data, so we simply select the first 400 papers in alphabetical order as the testing data, and the remaining papers are used to train the supervised baselines.\\n– NUS (Nguyen and Kan, 2007): We use the author-assigned keyphrases and treat all 211 papers as the testing data. Since the NUS dataset did not specifically mention the ways of splitting training and testing data, the results of the supervised baseline models are obtained through a five-fold cross validation.\\n– SemEval-2010 (Kim et al., 2010): 288 articles are collected from ACM Digital Library. 100 articles are used for testing and the rest are for training supervised baselines.\\n– KP20k: We build a new testing dataset that contains the title, abstract and keyphrases of 20,000 scientific articles in Computer Science. They are randomly selected from our obtained 567,830 articles. For the supervised baselines, another 20,000 articles are randomly selected for training. Therefore, our proposed model is trained on 527,830 that holds out these 40,000 articles.\\n**4.3 Implementation Details**\\nIn total, there are 2,780,316 〈text, keyphrase〉 pairs for training, in which text refers to the concatenation of the title and abstract of a publication, and keyphrase indicates an author-assigned keyword. The text pre-processing steps including tokenization, lowercasing and replacing all digits with symbol 〈digit〉 are applied. Two encoder-decoder models are trained, one with only attention mechanism (RNN) and one with both attention and copy mechanism enabled (CopyRNN). For both models, we choose the top 50,000 frequently-occurred words as our vocabulary, the dimension of embedding is set to 150, and the dimension of hidden layers is set to 300. Models are optimized using Adam (Kingma and Ba, 2014) with initial learning rate = 10−4, gradient clipping = 0.1 and dropout rate = 0.5. The max depth of beam search is set to 6, and the beam size is set to 200. In the generation of keyphrases, we find that the model tends to assign higher probabilities for shorter keyphrases,\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nwhereas most of keyphrases contain more than two words. To resolve this problem, we apply a simple heuristic by preserving only the first singleword phrase (with the highest generating probability) and removing the rest.\\n**4.4 Baseline Models**\\nFour unsupervised algorithms (Tf-Idf, TextRank (Mihalcea and Tarau, 2004), SingleRank (Wan and Xiao, 2008), ExpandRank (Wan and Xiao, 2008)) and two supervised algorithms (KEA (Witten et al., 1999) and Maui (Medelyan et al., 2009a)) are adopted as baselines. We set up the four unsupervised methods following the optimal settings in (Hasan and Ng, 2010) and the two supervised methods following the default setting as specified in their papers.\\n**4.5 Evaluation Metric**\\nThree evaluation metrics, the macro-averaged precision, recall and F-measure (F1) are employed for measuring the algorithm performance. Follow the standard definition, precision is defined as the number of correctly-predicted keyphrases over the number of all predicted keyphrases, recall is computed by the number of correctly-predicted keyphrases over the total data records. Note that, when determining the match of two keyphrases, we use Porter Stemmer for pre-processing.\\n**5 Results and Analysis**\\nWe conduct an empirical study on three different tasks to evaluate our model.\\n**5.1 Predicting Present Keyphrases**\\nThis is the same as the keyphrase extraction task in prior studies, in which we would like to analyze how well our proposed model perform on the commonly-defined task. To make a fair comparison, we only consider the present keyphrases for evaluation in this task. Table 2 provides the performances of the six baseline models, as well as our proposed models (i.e., RNN and CopyRNN). For each method, the table lists its F-measure at top 5 and top 10 predictions on the five datasets. The best scores are highlighted in bold and the underlines indicate the second best performances.\\nThe results show that the four unsupervised models (Tf-idf, TextTank, SingleRank and ExpandRank) perform robust across different datasets. The ExpandRank fails to return any result on the KP20k dataset due to its high time com-\\nplexity. The measures on NUS and SemEval here are higher than the ones reported in (Hasan and Ng, 2010) and (Kim et al., 2010), probably because we utilized the paper abstract instead of the full-text for training, which may filter out some noisy information. The performances of the two supervised models (i.e., Maui and KEA) are unstable on some datasets, but Maui achieves the best performances on three datasets among all the baseline models.\\nAs for our proposed keyphrase prediction approaches, the RNN model with the attention mechanism does not perform as well as we expected. It might be because the RNN model only concerns on finding the hidden semantics behind the text, which may tend to generate keyphrases or words that are too general and may not necessarily referring to the source text. In addition, it fails to recall keyphrases that contain out-of-vocabulary words (since the RNN model only takes the top 50,000 words in vocabulary). This indicates that a pure generative model may not fit the extraction task, and we need to further link back to the language usage in the source text. Indeed, the copyRNN model, by considering more contextual information, significantly outperforms not only the RNN model but also all baselines, exceeding the best baselines by more than 20% on average. This result demonstrates the importance of source text for extraction task. Besides, nearly 2% of all the correct predictions contain out-of-vocabulary words.\\nThe example in Figure 1(a) shows the result of predicted present keyphrases by RNN and CopyRNN for an article about video search. We see that both models can generate phrases that related to the topic of information retrieval and video. However most of RNN predictions are high-level terminologies, which are too general to be selected as keyphrases. The CopyRNN, on the other hand, predicts more detailed phrases like “video metadata” and “integrated ranking”. An interesting bad case is, “rich content” is coordinate with a keyphrase “video metadata”, and the CopyRNN puts it into prediction mistakenly.\\n**5.2 Predicting Absent Keyphrases**\\nAs stated, one important motivation for this work is that we are interested in the proposed model’s capability for predicting absent keyphrases based on the “understanding” of content. It is worth noting that such prediction is a very challenging task, and, to the best of our knowledge, no ex-\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nMethod Inspec Krapivin NUS SemEval KP20k\\nF1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10\\nTf-Idf 0.221 0.313 0.129 0.160 0.136 0.184 0.128 0.194 0.102 0.126 TextRank 0.223 0.281 0.189 0.162 0.195 0.196 0.176 0.187 0.175 0.147\\nSingleRank 0.214 0.306 0.189 0.162 0.140 0.173 0.135 0.176 0.096 0.119 ExpandRank 0.210 0.304 0.081 0.126 0.132 0.164 0.139 0.170 N/A N/A\\nMaui 0.040 0.042 0.249 0.216 0.249 0.268 0.044 0.039 0.270 0.230 KEA 0.098 0.126 0.110 0.152 0.069 0.084 0.025 0.026 0.171 0.154\\nRNN 0.085 0.064 0.135 0.088 0.169 0.127 0.157 0.124 0.179 0.189 CopyRNN 0.278 0.342 0.311 0.266 0.334 0.326 0.293 0.304 0.333 0.262\\nTable 2: The performance of predicting present keyphrase of various models on five benchmark datasets\\nisting methods can handle this task. Therefore, we only provide the RNN and copyRNN performances in the discussion of the results of this task. Here, we evaluate the performance with the recall of top 10 and top 50 results, to see how many absent keyphrases can be correctly predicted. We use the absent keyphrases in the testing datasets for evaluation.\\nDataset RNN CopyRNN R@10 R@50 R@10 R@50\\nInspec 0.031 0.061 0.047 0.100 Krapivin 0.095 0.156 0.113 0.202\\nNUS 0.050 0.089 0.058 0.116 SemEval 0.041 0.060 0.043 0.067 KP20k 0.083 0.144 0.125 0.211\\nTable 3: Absent keyphrases prediction performance of RNN and CopyRNN on five datasets\\nTable 3 present the recalls of the top 10/50 predicted keyphrases for our RNN and CopyRNN models, in which we observe that the CopyRNN can, on average, recall around 8% (15%) of keyphrases at top 10 (50) predictions. This indicates that, to some extent, both models can capture the hidden semantics behind the textual content and make reasonable predictions. In addition, with the advantage of features from the source text, the CopyRNN model also outperforms the RNN model in this condition, though not improve as much as the present keyphrase extraction task. An example is shown in Figure 1(b), in which we see that two absent keyphrases “video retrieval” and “video indexing” are correctly recalled by both models. The interesting thing is, the term “indexing” does not appear in the text, but the models\\nmay detect the information “index videos” in the first sentence and paraphrase it to the target phrase. And the CopyRNN successfully predicts another two keyphrases by capturing the detailed information from the text (highlighted text segments).\\n**5.3 Transfer to News Articles**\\nThe RNN and CopyRNN are supervised models, and they are trained on data in specific domain and writing style. However, with sufficient training on a large-scale dataset, we expect the models to be able to learn universal language features that are effective in other corpus as well. Thus in this task, we will test our model on another type of text, to see whether the model would work when being transferred to a different environment.\\nWe utilize the popular news article dataset DUC-2001 (Wan and Xiao, 2008) for analysis. The dataset consists of 308 news articles and 2,488 manually annotated keyphrases. The result is shown in Table 4, from which we could see that the CopyRNN could extract a portion of correct keyphrases from a unfamiliar text. Compare to the results reported in (Hasan and Ng, 2010), the performance of CopyRNN is better than TextRank (Mihalcea and Tarau, 2004) and KeyCluster (Liu et al., 2009), but lags behind the other\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nthree baselines. As transfered to corpus in a complete strange type and domain, the model encounters more unknown words and has to rely more on the syntactic features in the text. In this experiment, the CopyRNN recalls 766 keyphrases. 14.3% of them contain out-of-vocabulary words and many names of persons and places are correctly predicted.\\n**6 Discussion**\\nOur experimental results demonstrate that the CopyRNN model not only performs well on predicting present keyphrases but also has the ability of generating topical relevant keyphrases that are absent in the text. In a broader sense, this model attempts to map a long text (i.e., paper abstract) with representative short text chunks (i.e., keyphrases), which can potentially be applied to improve information retrieval performance by generating high-quality index terms, as well as assisting user browsing by summarizing long documents into short readable phrases.\\nSo far we have examined our model on scientific publications and news articles, demonstrating that our model has the ability to capture universal language patterns and extract key information from unfamiliar texts. We believe that the models have a greater potential to be generalized to other domains and types, like books, online reviews etc., if it is trained on larger data corpus. Also, we directly apply our model, which is trained on publication dataset, into generating keyphrases for news articles without any adaptive training. We believe that with proper training on news data, the\\nmodel would make further improvement. Additionally, this work mainly studies the problem of discovering core content from textual materials. Here, the encoder-decoder framework is applied to model language; however, such framework can also be extended to locate the core information on other data resources such as to summarize content from images and videos.\\n   \n",
            "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           **2 Problem Description and Prior Work**\\n\\n**2.1 Formulation and Standard Softmax**\\nMost of current NMT models use one-hot representations to represent the words in the output vocabulary – each word w is represented by a unique sparse vector eid(w) ∈ RV , in which only one element at the position corresponding to the word ID id(w) ∈ {x ∈ N | 1 ≤ x ≤ V } is 1, while others are 0. V represents the vocabulary size of the target language. NMT models optimize network parameters by treating the one-hot representation eid(w) as the true probability distribution, and minimizing the cross entropy between it and the softmax probability v:\\nLH(v, id(w)) := H(eid(w),v), (1) = log sum expu− uid(w), (2)\\nv := expu/ sum expu, (3)\\nu := Whuh+ βu, (4)\\nwhere sumx represents the sum of all elements in x, xi represents the i-th element of x, Whu ∈\\nRV×H and βu ∈ RV are trainable parameters and H is the total size of hidden layers directly connected to the output layer.\\nAccording to Equation (4), this model clearly requires time/space computation in proportion to O(HV ), and the actual load of the computation of the output layer is directly affected by the size of vocabulary V , which is typically set around tens of thousands (Sutskever et al., 2014).\\n**2.2 Prior Work on Suppressing Complexity of NMT Models**\\nSeveral previous works have proposed methods to reduce computation in the output layer. The hierarchical softmax (Morin and Bengio, 2005) predicts each word based on binary decision and reduces computation time to O(H log V ). However, this method still requires O(HV ) space for the parameters, and requires calculation much more complicated than the standard softmax, particularly at test time. The differentiated softmax (Chen et al., 2016) divides words into clusters, and predicts words using separate part of the hidden layer for each word clusters. This method make the conversion matrix of the output layer sparser than a fully-connected softmax, and can reduce time/space computation amount by ignoring zero part of the matrix. However, this method restricts the usage of hidden layer, and the size of the matrix is still in proportion to V .\\nSampling-based approximations (Mnih and Teh, 2012; Mikolov et al., 2013) to the denominator of the softmax have also been proposed to reduce calculation at training. However, these methods are basically not able to be applied at test time, still require heavy computation like the standard softmax.\\nOther methods using characters (Ling et al., 2015) or subwords (Sennrich et al., 2016; Chitnis and DeNero, 2015) can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency.\\n**3 Binary Code Prediction Models**\\n\\n**3.1 Representing Words using Bit Arrays**\\nFigure 2(a) shows the conventional softmax prediction, and Figure 2(b) shows the binary code prediction model proposed in this study. Unlike the conventional softmax, the proposed method predicts each output word indirectly using dense\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\nFigure 2: Designs of output layers.\\nbit arrays that correspond to each word. Let b(w) := [b1(w), b2(w), · · · , bB(w)] ∈ {0, 1}B be the target bit array obtained for word w, where each bi(w) ∈ {0, 1} is an independent binary function given w, and B is the number of bits in whole array. For convenience, we introduce some constraints on b. First, a wordw is mapped to only one bit array b(w). Second, all unique words can be discriminated by b, i.e., all bit arrays satisfy that:\\nid(w) 6= id(w′)⇒ b(w) 6= b(w′). (5)\\nThird, multiple bit arrays can be mapped to the same word as described in Section 3.5. By considering second constraint, we can also constrain B ≥ dlog2 V e, because b should have at least V unique representations to distinguish each word. The output layer of the network independently predicts B probability values q := [q1(h), q2(h), · · · , qB(h)] ∈ [0, 1]B using the current hidden values h by logistic regressions:\\nq(h) = σ(Whqh+ βq), (6) σ(x) := 1/(1 + exp(−x)), (7)\\nwhere Whq ∈ RB×H and βq ∈ RB are trainable parameters. When we assume that each qi is the probability that “the i-th bit becomes 1,” the joint probability of generating word w can be represented as:\\nPr(w|h) := B∏ i=1 ( biqi + b̄iq̄i ) , (8)\\nAlgorithm 1 Mapping words to bit arrays. Require: w ∈ V Ensure: b ∈ {0, 1}B = Bit array representing w\\nx :=  0, if w = UNK 1, if w = BOS 2, if w = EOS 2 + rank(w), otherwise bi := bx/2i−1c mod 2 b← [b1, b2, · · · , bB ]\\nwhere x̄ := 1 − x. We can easily obtain the maximum-probability bit array from q by simply assuming bi = 1 if qi ≥ 1/2, and bi = 0 otherwise. However, this calculation may generate invalid bit arrays which do not correspond to actual words according to the mapping between words and bit arrays. For now, we simply assume that w = UNK (unknown) when such bit arrays are obtained, and discuss alternatives later in Section 3.5.\\nThe constraints described here are very general requirements for bit arrays, which still allows us to choose between a wide variety of mapping functions. However, designing the most appropriate mapping method for NMT models is not a trivial problem. In this study, we use a simple mapping method described in Algorithm 1, which was empirically effective in preliminary experiments.1 Here, V is the set of V target words including 3 extra markers: UNK, BOS (begin-of-sentence), and EOS (end-of-sentence), and rank(w) ∈ N>0 is the rank of the word according to their frequencies in the training corpus. Algorithm 1 is one of the minimal mapping methods (i.e., satisfying B = dlog2 V e), and generated bit arrays have the characteristics that their higher bits roughly represents the frequency of corresponding words (e.g., if w is frequently appeared in the training corpus, higher bits in b(w) tend to become 0).\\n**3.2 Loss Functions**\\nFor learning correct binary representations, we can use any loss function that satisfies a constraint that:\\nLB(q, b) { = 0, if q = b, ≥ 0, otherwise. (9)\\nFor example, the squared-distance:\\nLB(q, b) := B∑ i=1 (qi − bi)2, (10)\\n1Other methods examined included random codes, Huffman codes (Huffman, 1952) and Brown clustering (Brown et al., 1992) with zero-padding to adjust code lengths, and some original allocation methods based on the word2vec embeddings (Mikolov et al., 2013).\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nor the cross-entropy:\\nLB(q, b) := − B∑ i=1 ( bi log qi + b̄i log q̄i ) , (11)\\nare candidates for the loss function. We also examined both loss functions in the preliminary experiments, and in this paper, we only used the squared-distance function (Equation (10)), because this function achieved higher translation accuracies than Equation (11).\\n**3.3 Efficiency of the Binary Code Prediction**\\nThe computational complexity for the parameters Whq and βq is O(HB). This is equal to O(H log V ) when using a minimal mapping method like that shown in Algorithm 1, and is significantly smaller than O(HV ) when using standard softmax prediction. For example, if we chose V = 65536 = 216 and use Algorithm 1’s mapping method, then B = 16 and total amount of computation in the output layer could be suppressed to 1/4096 of its original size.\\nOn a different note, the binary code prediction model proposed in this study shares some ideas with the hierarchical softmax (Morin and Bengio, 2005) approach. Actually, when we used a binarytree based mapping function for b, our model can be interpreted as the hierarchical softmax with two strong constraints for guaranteeing independence between all bits: all nodes in the same level of the hierarchy share their parameters, and all levels of the hierarchy are predicted independently of each other. By these constraints, all bits in b can be calculated in parallel. This is particularly important because it makes the model conducive to being calculated on GPGPUs.\\nHowever, the binary code prediction model also introduces problems of robustness due to these strong constraints. As the experimental results show, the simplest prediction model which directly maps words into bit arrays seriously decreases translation quality. In Sections 3.4 and 3.5, we introduce two additional techniques to prevent reductions of translation quality and improve robustness of the binary code prediction model.\\n**3.4 Hybrid Softmax/Binary Model**\\nAccording to the Zipf’s law (Zipf, 1949), the distribution of word appearances in an actual corpus is biased to a small subset of the vocabulary. As a result, the proposed model mostly learns\\ncharacteristics for frequent words and cannot obtain enough opportunities to learn for rare words. To alleviate this problem, we introduce a hybrid model using both softmax prediction and binary code prediction as shown in Figure 2(c). In this model, the output layer calculates a standard softmax for the N − 1 most frequent words and an OTHER marker which indicates all rare words. When the softmax layer predicts OTHER, then the binary code layer is used to predict the representation of rare words. In this case, the actual probability of generating a particular word is separated into two equations according to the frequency of words:\\nPr(w|h) := { v′id(w), if id(w) < N,\\nv′N · π(w,h), otherwise, (12)\\nv′ := expu′/ sum expu′, (13) u′ := Whu′h+ βu′ , (14)\\nπ(w,h) := B∏ i=1 ( biqi + b̄iq̄i ) , (15)\\nwhere Whu′ ∈ RN×H and βu′ ∈ RN are trainable parameters, and id(w) assumes that the value corresponds to the rank of frequency of each word. We also define the loss function for the hybrid model using both softmax and binary code losses:\\nL := { lH(id(w)), if id(w) < N, lH(N) + lB, otherwise, (16) lH(i) := λHLH(v ′, i), (17)\\nlB := λBLB(q, b), (18)\\nwhere λH and λB are hyper-parameters to determine strength of both softmax/binary code losses. These also can be adjusted according to the training data, but in this study, we only used λH = λB = 1 for simplicity.\\nThe computational complexity of the hybrid model is O(H(N + log V )), which is larger than the original binary code modelO(H log V ). However,N can be chosen asN V because the softmax prediction is only required for a few frequent words. As a result, we can control the actual computation for the hybrid model to be much smaller than the standard softmax complexity O(HV ),\\nThe idea of separated prediction of frequent words and rare words comes from the differentiated softmax (Chen et al., 2016) approach. However, our output layer can be configured as a fullyconnected network, unlike the differentiated soft-\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nFigure 3: Example of the classification problem using redundant bit array mapping.\\nFigure 4: Training and generation processes with error-correcting code.\\nmax, because the actual size of the output layer is still small after applying the hybrid model.\\n**3.5 Applying Error-correcting Codes**\\nThe 2 methods proposed in previous sections impose constraints for all bits in q, and the value of each bit must be estimated correctly for the correct word to be chosen. As a result, these models may generate incorrect words due to even a single bit error. This problem is the result of dense mapping between words and bit arrays, and can be avoided by creating redundancy in the bit array. Figure 3 shows a simple example of how this idea works when discriminating 2 words using 3 bits. In this case, the actual words are obtained by estimating the nearest centroid bit array according to the Hamming distance between each centroid and the predicted bit array. This approach can predict correct words as long as the predicted bit arrays are in the set of neighbors for the correct centroid (gray regions in the Figure 3), i.e., up to a 1-bit error in the predicted bits can be corrected. This ability to be robust to errors is a central idea behind error-correcting codes (Shannon, 1948). In general, an error-correcting code has the ability to correct up to b(d−1)/2c bit errors when all centroids differ d bits from each other (Golay, 1949). d is known as the free distance determined\\nby the design of error-correcting codes. Errorcorrecting codes have been examined in some previous work on multi-class classification tasks, and have reported advantages from the raw classification (Dietterich and Bakiri, 1995; Klautau et al., 2003; Liu, 2006; Kouzani and Nasireding, 2009; Kouzani, 2010; Ferng and Lin, 2011, 2013). In this study, we applied an error-correcting algorithm to the bit array obtained from Algorithm 1 to improve robustness of the output layer in an NMT system. A challenge in this study is trying a large classification (#classes > 10,000) with error-correction, unlike previous studies focused on solving comparatively small tasks (#classes < 100). And this study also tries to solve a generation task unlike previous studies. As shown in the experiments, we found that this approach is highly effective in these tasks.\\nFigure 4 (a) and (b) illustrate the training and generation processes for the model with errorcorrecting codes. In the training, we first convert the original bit arrays b(w) to a center bit array b′ in the space of error-correcting code: b′(b) := [b′1(b), b ′ 2(b), · · · , b′B′(b)] ∈ {0, 1}B ′ , where B′(B) ≥ B is the number of bits in the error-correcting code. The NMT model learns its parameters based on the loss between predicted probabilities q and b′. Note that typical errorcorrecting codes satisfy O(B′/B) = O(1), and this characteristic efficiently suppresses the increase of actual computation cost in the output layer due to the application of the error-correcting code. In the generation of actual words, the decoding method of the error-correcting code converts the redundant predicted bits q into a dense representation q̃ := [q̃1(q), q̃2(q), · · · , q̃B(q)], and uses q̃ as the bits to restore the word, as is done in the method described in the previous sections.\\nIt should be noted that the method for performing error correction directly affects the quality of the whole NMT model. For example, the mapping shown in Figure 3 has only 3 bits and it is clear that these bits represent exactly the same information as each other. In this case, all bits can be estimated using exactly the same parameters, and we can not expect that we will benefit significantly from applying this redundant representation. Therefore, we need to choose an error correction method in which the characteristics of original bits should be distributed in various positions of the resulting bit arrays so that errors in bits are\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nAlgorithm 2 Encoding into a convolutional code. Require: b ∈ {0, 1}B Ensure: b′ ∈ {0, 1}2(B+6) = Redundant bit array\\nx[t] := { bt, if 1 ≤ t ≤ B 0, otherwise y1t := x[t− 6 .. t] · [1001111] mod 2 y2t := x[t− 6 .. t] · [1101101] mod 2 b′ ← [y11 , y21 , y12 , y22 , · · · , y1B+6, y2B+6]\\nAlgorithm 3 Decoding from a convolutional code. Require: q ∈ (0, 1)2(B+6) Ensure: q̃ ∈ {0, 1}B = Restored original bit array g(q, b) := b log q + (1− b) log(1− q)\\nφ0[s | s ∈ {0, 1}6]← {\\n0, if s = [000000] −∞, otherwise\\nfor t = 1→ B + 6 do for scur ∈ {0, 1}6 do\\nsprev(x) := [x] ◦ scur[1 .. 5] o1(x) := ([x] ◦ scur) · [1001111] mod 2 o2(x) := ([x] ◦ scur) · [1101101] mod 2 g′(x) := g(q2t−1, o1(x)) + g(q2t, o2(x)) φ′(x) := φt−1[s\\nprev(x)] + g′(x) x̂← arg maxx∈{0,1} φ′(x) rt[s\\ncur]← sprev(x̂) φt[s\\ncur]← φ′(x̂) end for\\nend for s′ ← [000000] for t = B → 1 do\\ns′ ← rt+6[s′] q̃t ← s′1\\nend for q̃ ← [q̃1, q̃2, · · · , q̃B ]\\nnot highly correlated with each-other. In addition, it is desirable that the decoding method of the applied error-correcting code can directly utilize the probabilities of each bit, because q generated by the network will be a continuous probabilities between zero and one.\\nIn this study, we applied convolutional codes (Viterbi, 1967) to convert between original and redundant bits. Convolutional codes perform a set of bit-wise convolutions between original bits and weight bits (which are hyper-parameters). They are well-suited to our setting here because they distribute the information of original bits in different places in the resulting bits, work robustly for random bit errors, and can be decoded using bit probabilities directly.\\nAlgorithm 2 describes the particular convolutional code that we applied in this study, with two convolution weights [1001111] and [1101101] as fixed hyper-parameters.2 Where x[i .. j] :=\\n2We also examined many configurations of convolutional codes which have different robustness and computation costs, and finally chose this one.\\n[xi, · · · , xj ] and x · y := ∑\\ni xiyi. On the other hand, there are various algorithms to decode convolutional codes with the same format which are based on different criteria. In this study, we use the decoding method described in Algorithm 3, where x ◦ y represents the concatenation of vectors x and y. This method is based on the Viterbi algorithm (Viterbi, 1967) and estimates original bits by directly using probability of redundant bits. Although Algorithm 3 looks complicated, this algorithm can be performed efficiently on CPUs at test time, and is not necessary at training time when we are simply performing calculation of Equation (6). Algorithm 2 increases the number of bits from B intoB′ = 2(B+6), but does not restrict the actual value of B.\\n**4 Experiments**\\n\\n**4.1 Experimental Settings**\\nWe examined the performance of the proposed methods on two English-Japanese bidirectional translation tasks which have different translation difficulties: ASPEC (Nakazawa et al., 2016) and BTEC (Takezawa, 1999). Table 1 describes details of two corpora. To prepare inputs for training, we used tokenizer.perl in Moses (Koehn et al., 2007) and KyTea (Neubig et al., 2011) for English/Japanese tokenizations respectively, applied lowercase.perl from Moses, and replaced out-of-vocabulary words such that rank(w) > V − 3 into the UNK marker.\\nWe implemented each NMT model using C++ in the DyNet framework (Neubig et al., 2017) and trained/tested on 1 GPU (GeForce GTX TITAN X). Each test is also performed on CPUs to compare its processing time. We used the concat local attention model proposed in Luong et al. (2015) constructed using a 1-layer LSTM (input/forget/output gates and non-peepholes) (Gers et al., 2000) encoder/decoder with 30% dropout (Srivastava et al., 2014) for the input/output vectors of the LSTMs. Only output layers and loss functions are replaced, and other network architectures are identical for the conventional/proposed models. We used the Adam optimizer (Kingma and Ba, 2014) with fixed hyper-parameters α = 0.001, β1 = 0.9β2 = 0.999, ε = 10\\n−8, and minibatches with 64 sentences sorted according to their sequence lengths. For evaluating the quality of each model, we calculated BLEU (Papineni et al., 2002) every 1000 mini-batches. Table 2 lists all\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nTable 1: Details of the corpus.\\nName ASPEC BTEC\\nLanguages En↔ Ja\\n#sentences Train 2.00 M 465. k Dev 1,790 510 Test 1,812 508 Vocabulary size V 65536 25000\\nTable 2: Evaluated models.\\nName Summary Softmax Softmax prediction (Figure 2(a)) Binary Figure 2(b) w/ raw bit array Hybrid-N Figure 2(c) w/ softmax size N Binary-EC Binary w/ error-correction Hybrid-N-EC Hybrid-N w/ error-correction\\nmodels we examined in experiments.\\n**4.2 Results and Discussion**\\nTable 3 shows the BLEU on the test set, number of bits B (or B′) for the binary code, actual size of the output layer #out, number of parameters in the output layer #W,β, as well as the ratio of #W,β or amount of whole parameters compared with Softmax, and averaged processing time at training (per mini-batch on GPUs) and test (per sentence on GPUs/CPUs), respectively. Figure 5(a) and 5(b) shows training curves up to 180,000 epochs about some English→Japanese settings. To relax instabilities of translation qualities while training (as shown in Figure 5(a) and 5(b)), each BLEU in Table 3 is calculated by averaging actual test BLEU of 5 consecutive results around the epoch that has the highest dev BLEU.\\nFirst, we can see that each proposed method largely suppresses the actual size of the output layer from ten to one thousand times compared with the standard softmax. By looking at the total number of parameters, we can see that the proposed models require only 70% of the actual memory, and the proposed model reduces the total number of parameters for the output layers to a practically negligible level. Note that most of remaining parameters are used for the embedding lookup at the input layer in both encoder/decoder. These still occupy O(EV ) memory, where E represents the size of each embedding layer and usually O(E/H) = O(1). These are not targets to be reduced in this study because these values rarely are accessed at test time because we only need to access them for input words, and do not need them to always be in the physical memory. It might be\\npossible to apply a similar binary representation as that of output layers to the input layers as well, then express the word embedding by multiplying this binary vector by a word embedding matrix. This is one potential avenue of future work.\\nTaking a look at the BLEU for the simple Binary method, we can see that it is far lower than other models for all tasks. This is expected, as described in Section 3, because using raw bit arrays causes many one-off estimation errors at the output layer due to the lack of robustness of the output representation. In contrast, Hybrid-N and Binary-EC models clearly improve BLEU from Binary, and they approach that of Softmax. This demonstrates that these two methods effectively improve the robustness of binary code prediction models. Especially, Binary-EC generally achieves higher quality than Hybrid-512 despite the fact that it suppress the number of parameters by about 1/10. These results show that introducing redundancy to target bit arrays is more effective than incremental prediction. In addition, the Hybrid-NEC model achieves the highest BLEU in all proposed methods, and in particular, comparative or higher BLEU than Softmax in BTEC. This behav-\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nTable 3: Comparison of BLEU, size of output layers, number of parameters and processing time.\\nCorpus Method BLEU % B #out #W,β\\nRatio of #params Time (En→Ja) [ms] EnJa JaEn #W,β All Train Test: GPU / CPU\\nASPEC Softmax 31.13 21.14 — 65536 33.6 M 1/1 1 1026. 121.6 / 2539. Binary 13.78 6.953 16 16 8.21 k 1/4.10 k 0.698 711.2 73.08 / 122.3 Hybrid-512 22.81 13.95 16 528 271. k 1/124. 0.700 843.6 81.28 / 127.5 Hybrid-2048 27.73 16.92 16 2064 1.06 M 1/31.8 0.707 837.1 82.28 / 159.3 Binary-EC 25.95 18.02 44 44 22.6 k 1/1.49 k 0.698 712.0 78.75 / 164.0 Hybrid-512-EC 29.07 18.66 44 556 285. k 1/118. 0.700 850.3 80.30 / 180.2 Hybrid-2048-EC 30.05 19.66 44 2092 1.07 M 1/31.4 0.707 851.6 77.83 / 201.3\\nBTEC Softmax 47.72 45.22 — 25000 12.8 M 1/1 1 325.0 34.35 / 323.3 Binary 31.83 31.90 15 15 7.70 k 1/1.67 k 0.738 250.7 27.98 / 54.62 Hybrid-512 44.23 43.50 15 527 270. k 1/47.4 0.743 300.7 28.83 / 66.13 Hybrid-2048 46.13 45.76 15 2063 1.06 M 1/12.1 0.759 307.7 28.25 / 67.40 Binary-EC 44.48 41.21 42 42 21.5 k 1/595. 0.738 255.6 28.02 / 69.76 Hybrid-512-EC 47.20 46.52 42 554 284. k 1/45.1 0.744 307.8 28.44 / 56.98 Hybrid-2048-EC 48.17 46.58 42 2090 1.07 M 1/12.0 0.760 311.0 28.47 / 69.44\\nFigure 6: BLEU changes in the Hybrid-N methods according to the softmax size (En→Ja).\\nior clearly demonstrates that these two methods are orthogonal, and combining them together can be effective. We hypothesize that the lower quality of Softmax in BTEC is caused by an over-fitting due to the large number of parameters required in the softmax prediction.\\nProposed methods also improve actual computation time in both training and test. In particular, proposed methods can be performed significantly faster than Softmax at testing on CPUs by x5 to x20, which are directly affected by the complexity of output layers. In addition, we can also see that applying error-correcting code is also efficient at the point of the decoding speed.\\nFigure 6 shows the trade-off between the translation quality and the size of softmax layers in the hybrid prediction model (Figure 2(c)) without error-correction. According to the model definition in Section 3.4, the softmax prediction and\\nraw binary code prediction can be assumed to be the upper/lower-bound of the hybrid prediction model. The curves in Figure 6 move between Softmax and Binary models, and this behavior intuitively explains the characteristics of the hybrid prediction. In addition, we can see that the BLEU score in BTEC quickly improves, and saturates at N = 1024 in contrast to the ASPEC model, which is still improving at N = 2048. We presume that the shape of curves in Figure 6 is also affected by the difficulty of the corpus, i.e., when we train the hybrid model for easy datasets (e.g., BTEC is easier than ASPEC), it is enough to use a small softmax layer (e.g. N ≤ 1024).\\n   \n",
            "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 **2 WordNet-Grounded Context-Sensitive Token Embeddings**\\nIn this section, we focus on defining our contextsensitive token embeddings. We first describe our grounding of word types using WordNet concepts. Then, we describe our model of contextsensitive token-level embeddings as a weighted sum of WordNet concept embeddings.\\n**2.1 WordNet Grounding**\\nWe use WordNet to map each word type to a set of synsets, including possible generalizations or abstractions. Among the labeled relations defined in WordNet between different synsets, we focus on the hypernymy relation to help model generaliza-\\ntion and selectional preferences between words, which is especially important for predicting PP attachments (Resnik, 1993). To ground a word type, we identify the the set of (direct and indirect) hypernyms of the WordNet senses of that word. A simplified grounding of the word ‘pool’ is illustrated in Figure 1. This grounding is key to our model of token embeddings, to be described in the following subsections.\\n**2.2 Context-Sensitive Token Embeddings**\\nOur goal is to define a context-sensitive model of token embeddings which can be used as a dropin replacement for traditional type-level word embeddings.\\nNotation. Let Senses(w) be the list of synsets defined as possible word senses of a given word type w in WordNet, and Hypernyms(s) be the list of hypernyms for a synset s.1 For example, according to Figure 1:\\nSenses(pool) = [pond.n.01, pool.n.09], and\\nHypernyms(pond.n.01) = [pond.n.01, lake.n.01,\\nbody of water.n.01, entity.n.01] (1)\\nEach WordNet synset s is associated with a set of parameters vs ∈ Rn which represent its embedding. This parameterization is similar to that of Rothe and Schütze (2015).\\nEmbedding model. Given a sequence of tokens t and their corresponding word types w, let ui ∈ Rn be the embedding of the word token ti at position i. Unlike most embedding models, the token embeddings ui are not parameters. Rather, ui is computed as the expected value of concept embeddings used to ground the word type wi corresponding to the token ti: ui = ∑\\ns∈Senses(wi) ∑ s′∈Hypernyms(s) p(s, s′ | t,w, i) vs′\\n(2)\\nsuch that∑ s∈Senses(wi) ∑ s′∈Hypernyms(s) p(s′, s | t,w, i) = 1\\n(3)\\n1For notational convenience, we assume that s ∈ Hypernyms(s).\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\nFigure 2: Steps for computing the contextsensitive token embedding for the word ‘pool’, as described in §2.2.\\nThe distribution which governs the expectation over synset embeddings factorizes into two components:\\np(s, s′ | t,w, i) ∝λwi exp−λwi rank(s,wi)× MLP([vs′ ; context(i, t)]) (4)\\nThe first component, λwi exp −λwi rank(s,wi), is a sense prior which reflects the prominence of each word sense for a given word type. Note that this is defined for each word type (wi), and is shared across all tokens which have the same word type. The parameterization of the sense prior is similar to an exponential distribution since WordNet senses are organized in descending order of their frequency. The scalar parameter (λwi) controls the decay of the probability mass.\\nThe second component, MLP([vs′ ; context(i, t)]) is what makes the token representations context-sensitive. It scores each concept in the WordNet grounding of wi by feeding the concatenation of the concept embedding and a dense vector that summarizes the textual context into a multilayer perceptron (MLP) with two tanh layers followed by a softmax layer. This component is inspired by the soft attention often used in neural machine translation (Bahdanau et al., 2014).2 The definition of the\\n2Although soft attention mechanism is typically used to\\ncontext function is dependent on the encoder used to encode the context. We describe a specific instantiation of this function in §3.\\nTo summarize, Figure 2 illustrates how to compute the embedding of a word token ti = ‘pool’ in a given context:\\n1. compute a summary of the context context(i, t),\\n2. enumerate related concepts for ti,\\n3. compute p(s, s′ | t,w, i) for each pair (s, s′), and\\n4. compute ui = E[vs′ ].\\nIn the following section, we describe our model for predicting PP attachments, including our definition for context.\\n**3 PP Attachment**\\nDisambiguating PP attachments is an important and challenging NLP problem, and is a good fit for evaluating our WordNet-grounded contextsensitive embeddings since modeling hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993).\\nFigure 3, reproduced from Belinkov et al. (2014), illustrates an example of the PP attachment prediction problem. The accuracy of a\\nexplicitly represent the importance of each item in a sequence, it can also be applied to non-sequential items.\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\ncompetitive English dependency parser at predicting the head word of an ambiguous prepositional phrase is 88.5%, significantly lower than the overall unlabeled attachment accuracy of the same parser (94.2%).3\\nThis section formally defines the problem of PP attachment disambiguation, describes our baseline model, then shows how to integrate the token-level embeddings in the model.\\n**3.1 Problem definition**\\nWe follow Belinkov et al. (2014)’s definition of the PP attachment problem. Given a preposition p and its direct dependent d in the prepositional phrase (PP), our goal is to predict the correct head word for the PP among an ordered list of candidate head words h. Each example in the train, validation, and test sets consists of an input tuple 〈h, p, d〉 and an output index k to identify the correct head among the candidates in h.\\n**3.2 Model definition**\\nBoth our proposed and baseline models for PP attachment use bidirectional RNN with LSTM cells (bi-LSTM) to encode the sequence t = 〈h1, . . . , hK , p, d〉.\\nWe score each candidate head by feeding the concatenation of the output bi-LSTM vectors for the head hk, the preposition p and the direct dependent d through an MLP, with a fully connected tanh layer to obtain a non-linear projection of the concatenation, followed by a fully-connected softmax layer:\\np(hkis head) = MLPattach([lstm out(hk);\\nlstm out(p);\\nlstm out(d)]) (5)\\nTo train the model, we use cross-entropy loss at the output layer for each candidate head in the training set. At test time, we predict the candidate head with the highest probability according to the model in Eq. 5, i.e.,\\nk̂ = argmax k p(hkis head = 1). (6)\\nThis model is inspired by the Head-Prep-ChildTernary model of Belinkov et al. (2014). The main difference is that we replace the input features for each token with the output bi-RNN vectors.\\n3See Table 2 in §4 for detailed results.\\nWe now describe the difference between the proposed model and the baseline. Generally, let lstm in(ti) and lstm out(ti) represent the input and output vectors of the bi-LSTM for each token ti ∈ t in the sequence.\\nBaseline model. In the baseline model, we use type-level word embeddings to represent the input vector lstm in(ti) for a token ti in the sequence. The word embedding parameters are initialized with pre-trained vectors, then tuned along with the parameters of the bi-LSTM and MLPattach. We call this model LSTM-PP.\\nProposed model. In the proposed model, we use token level word embedding as described in §2 as the input to the bi-LSTM, i.e., lstm in(ti) = ui. The context used for the attention component is simply the hidden state from the previous timestep. However, since we use a bi-LSTM, the model essentially has two RNNs, and accordingly we have two context vectors, and associated attentions. That is, contextf (i, t) = lstm in(ti−1) for the forward RNN and contextb(i, t) = lstm in(ti+1) for the backward RNN. The synset embedding parameters are initialized with pretrained vectors and tuned along with the sense decay (λw) and MLP parameters from Eq. 4, the parameters of the bi-LSTM and those of MLPattach. We call this model OntoLSTM-PP.\\n**4 Experiments**\\nDataset and evaluation. We used the English PP attachment dataset created and made available by Belinkov et al. (2014). The training and test splits contain 33,359 and 1951 labeled examples respectively. As explained in §3.1, the input for each example is 1) an ordered list of candidate head words, 2) the preposition, and 3) the direct dependent of the preposition. The head words are either nouns or verbs and the dependent is always a noun. All examples in this dataset have at least two candidate head words. As discussed in Belinkov et al. (2014), this dataset is a more realistic PP attachment task than the RRR dataset (Ratnaparkhi et al., 1994). The RRR dataset is a binary classification task with exactly two head word candidates in all examples. The context for each example in the RRR dataset is also limited which defeats the purpose of our context-sensitive embeddings.\\nModel specifications and hyperparameters. For efficient implementation, we use mini-batch\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n470\\n471\\n472\\n473\\n474\\n475\\n476\\n477\\n478\\n479\\n480\\n481\\n482\\n483\\n484\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nupdates with the same number of senses and hypernyms for all examples, padding zeros and truncating senses and hypernyms as needed. For each word type, we use a maximum of S senses and H indirect hypernyms from WordNet. In our initial experiments on a held-out development set (10% of the training data), we found that values greater than S = 3 and H = 5 did not improve performance. We also used the development set to tune the number of layers in MLPattach separately for the OntoLSTM-PP and LSTM-PP, and the number of layers in the attention MLP in OntoLSTM-PP. When a synset has multiple hypernym paths, we use the shortest one. Finally, words types which do not appear in WordNet are assumed to have one unique sense per word type with no hypernyms. Since the POS tag for each word is included in the dataset, we exclude WordNet synsets which are incompatible with the POS tag. The synset embedding parameters are initilized using the synset vectors obtained by running AutoExtend (Rothe and Schütze, 2015) on 100-dimensional GloVe (Pennington et al., 2014) vectors for WordNet 3.1. Representation for the OOV word types in LSTMPP and OOV synset types in OntoLSTM-PP were randomly drawn from a uniform 100-d distribution. Initial sense prior parameters (λw) were also drawn from a uniform 1-d distribution.\\nBaselines. In our experiments, we compare our proposed model, OntoLSTM-PP with three baselines – LSTM-PP initialized with GloVe embedding, LSTM-PP initialized with GloVe vectors retrofitted to WordNet using the approach of Faruqui et al. (2015), and finally the best performing standalone PP attachment system from Belinkov et al. (2014), referred to as “HPCD (full)” in the paper. “HPCD (full)” is a neural network model that learns to compose the vector representations of each of the candidate heads with those of the preposition and the dependent, and predict attachments. The input representations are enriched using syntactic context information, POS, WordNet and VerbNet (Kipper et al., 2008) information and the distance of the head word from the PP is explicitly encoded in composition architecture. In contrast, we do not use syntactic context, VerbNet and distance information, and do not explicitly encode POS information.\\n**4.1 PP Attachment Results**\\nTable 1 shows that our proposed token level embedding scheme “OntoLSTM-PP” outperforms the better variant of our baseline “LSTM-PP” (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%. “OntoLSTM-PP’ also outperforms “HPCD (full)”, the previous best result on this dataset.\\nInitializing the word embeddings with GloVeretro (which uses WordNet as described in Faruqui et al. (2015)) instead of “textitGloVe” amounts to a small improvement, compared to the improvements obtained using “OntoLSTM-PP”. This result illustrates that our approach of dynamically choosing a context sensitive distribution over synsets is a more effective way of making use of WordNet.\\nEffect on dependency parsing. Following Belinkov et al. (2014), we used RBG parser (Lei et al., 2014), and modified it by adding a binary feature indicating the PP attachment predictions from our model.\\nWe compare four ways to compute the additional binary features: 1) the predictions of the best standalone system “HPCD (full)” in Belinkov et al. (2014), 2) the predictions of our baseline model “LSTM-PP”, 3) the predictions of our improved model “OntoLSTM-PP”, and 4) the gold labels “Oracle PP”.\\nTable 2 shows the effect of using the PP attachment predictions as features within a dependency parser. We note there is a relatively small difference in unlabeled attachment accuracy for all dependencies (not only PP attachments), even when gold PP attachments are used as additional features to the parser. However, when gold PP attachment are used, we note a large potential improvement of 10.46 points (between the PPA accuracy for “RBG” and “RBG + Oracle PP”), which confirms that adding PP predictions as features is\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nSystem Full UAS PPA Acc. RBG 94.17 88.51 RBG + HPCD (full) 94.19 89.59 RBG + LSTM-PP 94.14 86.35 RBG + OntoLSTM-PP 94.30 90.11 RBG + Oracle PP 94.60 98.97\\nTable 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.\\nan effective approach. Our proposed model “RBG + OntoLSTM-PP” recovers 15% of this potential improvement, while “RBG + HPCD (full)” recovers 10%, which illustrates that PP attachment remains a difficult problem with plenty of room for improvements even when using a dedicated model to predict PP attachments and using its predictions in a dependency parser.\\nWe also note that, although we use the same predictions of the “HPCD (full)” model in Belinkov et al. (2014),4 we report different results than Belinkov et al. (2014). For example, the unlabeled attachment score (UAS) of the baselines “RBG” and “RBG + HPCD (full)” are 94.17 and 94.19, respectively, in Table 2, compared to 93.96 and 94.05, respectively, in Belinkov et al. (2014). This is due to the use of different versions of the RBG parser.5\\n**4.2 Analysis**\\nIn this subsection, we analyze different aspects of our model in order to develop a better understanding of its behavior.\\nEffect of context sensitivity and sense priors. We now show some results that indicate the relative strengths of two components of our contextsensitive token embedding model. The second row in Table 3 shows the test accuracy of a system trained without sense priors (that is, making p(s|wi) from Eq. 2 a uniform distribution), and the third row shows the effect of making the token representations context-insensitive by giving a similar attention score to all related concepts, essentially making them type level representations,\\n4The authors kindly provided their predictions for 1942 test examples (out of 1951 examples in the full test set). In Table 2, we use the same subset of 1942 test examples and will include a link to the subset in the final draft.\\n5We use the latest commit (SHA: e07f74dd2ba47348fd548935155ded38eea20809) on the GitHub repository of the RGB parser.\\nbut still grounded in WordNet. As it can be seen, removing context sensitivity has an adverse effect on the results. This illustrates the importance of the sense priors and the attention mechanism.\\nIt is interesting that, even without sense priors and attention, the results with WordNet grounding is still higher than that of the two LSTM-PP systems in Table 1. This result illustrates the regularization behavior of sharing concept embeddings across multiple words, which is especially important for rare words.\\nEffect of training data size. Since “OntoLSTM-PP” uses external information, the gap between the model and “LSTM-PP” is expected to be more pronounced when the training data sizes are smaller. To test this hypothesis, we trained the two models with different amounts of training data and measured their accuracies on the test set. The plot is shown in Figure 4. As expected, the gap tends to be larger at lower data sizes. Surprisingly, even with 2000 sentences in the training data set, OntoLSTM-PP outperforms LSTM-PP trained with the full data set. When both the models are trained with the fill dataset, LSTM-PP reaches a training accuracy of 95.3%, whereas OntoLSTM-PP reaches 93.5%. The fact that LSTM-PP is overfitting the training data more, indicates the regularization capability of OntoLSTM-PP.\\nQualitative analysis. To better understand the effect of WordNet grounding, we took a sample of sentences from the test set whose PP attachments were correctly predicted by OntoLSTM-PP but not by LSTM-PP. A common pattern observed was\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nFigure 5: Two examples from the test set where OntoLSTM-PP predicts the head correctly and LSTM-PP does not, along with weights by OntoLSTM-PP to synsets that contribute to token representations of infrequent word types. The prepositions are shown in bold, LSTM-PP’s predictions in red and OntoLSTMPP’s predictions in green. Words that are not candidate heads or dependents are shown in brackets.\\nModel PPA Acc. full 89.7 - sense priors 88.4 - attention 87.5\\nTable 3: Effect of removing sense priors and context sensitivity (attention) from the model\\nthat those sentences contained words not seen frequently in the training data. Figure 5 shows two such cases. In both cases, the weights assigned by OntoLSTM-PP to infrequent words are also shown. The word types soapsuds and buoyancy do not occur in the training data, but OntoLSTMPP was able to leverage the parameters learned for the synsets that contributed to their token representations. Another important observation is that the word type buoyancy has four senses in WordNet (we consider the first three), none of which is the metaphorical sense that is applicable to markets as shown in the example here. Selecting a combination of relevant hypernyms from various senses may have helped OntoLSTM-PP make the right prediction. This shows the value of using hypernymy information from WordNet. Moreover, this indicates the strength of the hybrid nature of the model, that lets it augment ontological information with distributional information.\\nParameter space We note that the vocabulary sizes in OntoLSTM-PP and LSTM-PP are comparable as the synset types are shared across word types. In our experiments with the full PP attachment dataset, we learned embeddings for 18k synset types with OntoLSTM-PP and 11k word types with LSTM-PP. Since the biggest contribution to the parameter space comes from the embedding layer, the complexities of both the models are comparable.\\nImplementation and code availability. The models are implemented using Keras (Chollet, 2015), and the functionality is available in the form of Keras layers to make it easier for other researchers to use the proposed embeddingn model.\\nFuture work. This approach may be extended to other NLP tasks that can benefit from using encoders that can access WordNet information. WordNet also has some drawbacks, and may not always have sufficient coverage given the task at hand. As we have shown in §4.2, our model can deal with missing WordNet information by augmenting it with distributional information. Moreover, the methods described in this paper can be extended to other kinds of structured knowledge sources like Freebase which may be more suitable for tasks like question answering.\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\n**5 Related Work**\\nThis work is related to various lines of research within the NLP community: dealing with synonymy and homonymy in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distributional and knowledge based semantics; and selectional preferences and their relation with syntactic and semantic relations.\\nThe need for going beyond a single vector per word-type has been well established for a while, and many efforts were focused on building multi-prototype vector space models of meaning (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Jauhar et al., 2015; Neelakantan et al., 2015, etc.). However, the target of all these approaches is obtaining multi-sense word vector spaces, either by incorporating sense tagged information or other kinds of external context. The number of vectors learned is still fixed, based on the preset number of senses. In contrast, our focus is on learning a context dependent distribution over those concept representations. Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task.\\nRelated to the idea of concept embeddings is Rothe and Schütze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings. In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept embeddings.\\nThere is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology.\\nFaruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest.\\nResnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al. (2013) applied models of selectional preferences automatically learned from WordNet and distributional information, to the problem of semantic role labeling. Resnik (1993); Brill and Resnik (1994); Agirre (2008) and others have used WordNet information towards improving prepositional phrase attachment predictions.\\n   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Conclusion  \\\n",
            "0                                                                                                                                                                                                                                                                                                                                                                                                                            Our contributions are twofold. First, we developed a neural language model trained on a phonetic transliteration of poetic form and content. Although example output looked promising, this model was limited by its inability to generalise to novel forms of verse. We then proposed a more robust model trained on unformed poetic text, whose output form is constrained at sample time. This approach offers greater control over the style of the generated poetry than the earlier method, and facilitates themes and poetic devices. An indistinguishability test, where participants were asked to classify a randomly selected set of human “nonsense verse” and machine-generated poetry, showed generated poetry to be indistinguishable from that written by humans. In addition, the poems that were deemed most ‘humanlike’, most aesthetic and most emotive, respectively, were all machine-generated. In future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (Luong et al., 2013), which are common in poetry.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We introduced phrasal recurrent neural network, an RNN model with all potential candidate phrases considered. Our model does not require any human labeled data to construct the structures. It outperforms the state-of-the-art LSTM language models. Our model does not require any external resources such as human labeled data or word align model to construct the phrases. It outperforms both state-of-the-art PBSMT and RNNsearch model.\\nWe make two main contributions:\\n• Instead of packing all information in distributed representation and internal hidden status, which are computing-friendly, we try to represent natural structure in an explicit way, which are human-friendly.\\n• Instead of stacking deeper and deeper layers of RNNs, we explore the possibility of network construction in another dimension: making RNN sequences parallel.   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In this paper, we have proposed an adversarial multi-task learning framework, in which the taskspecific and task-invariant features are learned non-redundantly, therefore capturing the sharedprivate separation of different tasks. We have demonstrated the effectiveness of our approach by applying our model to 16 different text classification tasks. We also perform extensive qualitative analysis, deriving insights and indirectly explaining the quantitative improvements in the overall performance.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899   \n",
            "3    In this paper, we have investigated models of referential word meaning, using different ways of combining visual information about a word’s referent and distributional knowledge about its lexical similarities. Previous cross-modal mapping models essentially force semantically similar objects to be mapped into the same area in the semantic space regardless of their actual visual similarity. We found that cross-modal mapping produces semantically appropriate and mutually highly similar object names in its top-n list, but does not preserve differences in referential word use (e.g. appropriatness of person vs. woman) especially within the same semantic field. We have shown that it is beneficial for performance in standard and zeroshot object naming to treat words as individual predictors that capture referential appropriateness and are only indirectly linked to a distributional space, either through lexical mapping during application or through cross-modal similarity mapping during training. As we have tested these approaches on a rather small vocabulary, which may limit generality of conclusions, future work will be devoted to scaling up these findings to larger test sets, as e.g. recently collected through conversational agents (Das et al., 2016) that circumvent the need for human-human interaction data. Also from an REG perspective, various extensions of this approach are possible, such as the inclusion of contextual information during object naming and its combination with attribute selection.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899   \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We present a new context based model for multiturn response selection in retrieval-based chatbots. Experiment results on public data sets show that the model can significantly outperform the stateof-the-art methods. Besides, we publish the first human labeled multi-turn response selection data set to research communities. In the future, we are going to study how to model logical consistency of responses and improve candidate retrieval (see supplementary material).\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899   \n",
            "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ...   \n",
            "132                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We presented a cross-lingual transfer learning method for paradigm completion, based on an RNN encoder-decoder model. Our experiments showed that information from a high-resource language can be leveraged for paradigm completion in a related low-resource language. Our analysis showed that the degree to which the source language data helps for a certain target language depends on their relatedness. Our method led to significant improvements in settings with limited training data – up to 58% absolute improvement in accuracy – and, thus, enables the use of state-of-the-art models for paradigm completion in low-resource languages.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899   \n",
            "133                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Contextual relationship among the utterances is mostly ignored in the literature. In this paper, we developed a LSTM-based network to extract contextual features from the utterances of a video for multimodal sentiment analysis. The proposed method has outperformed the state of the art and showed significant performance improvement over the baseline. As a part of the future work, we plan to propose LSTM attention model to determine importance of the utterances and contribution of modalities in the sentiment classification.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899   \n",
            "134                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In this paper, we propose an RNN-based generative model for predicting keyphrase in scientific text. To the best of our knowledge, this is the first application of the encoder-decoder model to keyphrase prediction task. Our model summarizes phrases based the deep semantic meaning of the text and it is able to handle rarely-occurred phrases by incorporating a copy mechanism. Comprehensive empirical studies demonstrate the effectiveness of our proposed model for generating both present and absent keyphrases for different types of text. Our future work may include the following two directions.\\n– In this work, we only evaluated the performance of proposed model by conducting offline experiments. In the future, we are interested in comparing the model with human annotators and evaluating the quality of predicted phrases by human judges.\\n– Our current model does not fully consider the correlation among target keyphrases. It would also be interesting to explore the multiple-output optimization on our model.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899   \n",
            "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In this study, we proposed neural machine translation models which indirectly predict output words via binary codes, and two model improvements: a hybrid prediction model using both softmax and binary codes, and introducing error-correcting codes to introduce robustness of binary code prediction. Experiments show that the proposed model can achieve comparative translation qualities to standard softmax prediction, while significantly suppressing the amount of parameters in the output layer, and improving calculation speeds while training and especially testing.\\nOne interesting avenue of future work is to automatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here. In Algorithms 2 and 3 we use convolutions that were determined heuristically, and it is likely that learning these along with the model could result in improved accuracy or better compression capability.\\n9\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n876\\n877\\n878\\n879\\n880\\n881\\n882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899   \n",
            "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In this paper, we proposed a grounding of lexical items which acknowledges the semantic ambiguity of word types using WordNet and a method to learn a context-sensitive distribution over their representations. We also showed how to integrate the proposed representation with recurrent neural networks for disambiguating prepositional phrase attachments, showing that the proposed WordNetgrounded context-sensitive token embeddings outperforms standard type-level embeddings for predicting PP attachments. We provided a detailed qualitative and quantitative analysis of the proposed model.   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Referencias  \n",
            "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [{'title': 'Word length frequency and distribution in english: Observations, theory, and implications for the construction of verse lines', 'author': ['Hideaki Aoyama', 'John Constable.'], 'venue': 'arXiv preprint cmp-lg/9808004 .', 'citeRegEx': 'Aoyama and Constable.,? 1998', 'shortCiteRegEx': 'Aoyama and Constable.', 'year': 1998}, {'title': 'Neural probabilistic language models', 'author': ['Yoshua Bengio', 'Holger Schwenk', 'Jean-Sébastien Senécal', 'Fréderic Morin', 'Jean-Luc Gauvain.'], 'venue': 'Innovations in Machine Learning, Springer, pages 137–186.', 'citeRegEx': 'Bengio et al\\.,? 2006', 'shortCiteRegEx': 'Bengio et al\\.', 'year': 2006}, {'title': 'Issues in building general letter to sound rules', 'author': ['Alan W Black', 'Kevin Lenzo', 'Vincent Pagel'], 'venue': None, 'citeRegEx': 'Black et al\\.,? \\Q1998\\E', 'shortCiteRegEx': 'Black et al\\.', 'year': 1998}, {'title': 'Learning phrase representations using rnn encoder-decoder for statistical machine translation', 'author': ['Kyunghyun Cho', 'Bart Van Merriënboer', 'Caglar Gulcehre', 'Dzmitry Bahdanau', 'Fethi Bougares', 'Holger Schwenk', 'Yoshua Bengio.'], 'venue': 'Proceedings of', 'citeRegEx': 'Cho et al\\.,? 2014', 'shortCiteRegEx': 'Cho et al\\.', 'year': 2014}, {'title': 'Full face poetry generation', 'author': ['Simon Colton', 'Jacob Goodwin', 'Tony Veale.'], 'venue': 'Proceedings of the Third International Conference on Computational Creativity. pages 95–102.', 'citeRegEx': 'Colton et al\\.,? 2012', 'shortCiteRegEx': 'Colton et al\\.', 'year': 2012}, {'title': 'An exact a* method for deciphering letter-substitution ciphers', 'author': ['Eric Corlett', 'Gerald Penn.'], 'venue': 'Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, pages 1040–', 'citeRegEx': 'Corlett and Penn.,? 2010', 'shortCiteRegEx': 'Corlett and Penn.', 'year': 2010}, {'title': 'A formula for predicting readability: Instructions', 'author': ['Edgar Dale', 'Jeanne S Chall.'], 'venue': 'Educational research bulletin pages 37–54.', 'citeRegEx': 'Dale and Chall.,? 1948', 'shortCiteRegEx': 'Dale and Chall.', 'year': 1948}, {'title': 'Wasp: Evaluation of different strategies for the automatic generation of spanish verse', 'author': ['Pablo Gervás.'], 'venue': 'Proceedings of the AISB-00 Symposium on Creative & Cultural Aspects of AI. pages 93–100.', 'citeRegEx': 'Gervás.,? 2000', 'shortCiteRegEx': 'Gervás.', 'year': 2000}, {'title': 'Automatic analysis of rhythmic poetry with applications to generation and translation', 'author': ['Erica Greene', 'Tugba Bodrumlu', 'Kevin Knight.'], 'venue': 'Proceedings of the 2010 conference on empirical methods in natural language processing. Association for', 'citeRegEx': 'Greene et al\\.,? 2010', 'shortCiteRegEx': 'Greene et al\\.', 'year': 2010}, {'title': 'Long short-term memory', 'author': ['Sepp Hochreiter', 'Jürgen Schmidhuber.'], 'venue': 'Neural computation 9(8):1735–1780.', 'citeRegEx': 'Hochreiter and Schmidhuber.,? 1997', 'shortCiteRegEx': 'Hochreiter and Schmidhuber.', 'year': 1997}, {'title': 'Character-aware neural language models', 'author': ['Yoon Kim', 'Yacine Jernite', 'David Sontag', 'Alexander M Rush.'], 'venue': 'arXiv preprint arXiv:1508.06615 .', 'citeRegEx': 'Kim et al\\.,? 2015', 'shortCiteRegEx': 'Kim et al\\.', 'year': 2015}, {'title': 'Unsupervised analysis for decipherment problems', 'author': ['Kevin Knight', 'Anish Nair', 'Nishit Rathod', 'Kenji Yamada.'], 'venue': 'Proceedings of the COLING/ACL on Main conference poster sessions. Association for Computational Linguistics, pages 499–', 'citeRegEx': 'Knight et al\\.,? 2006', 'shortCiteRegEx': 'Knight et al\\.', 'year': 2006}, {'title': 'N-gram similarity and distance', 'author': ['Grzegorz Kondrak.'], 'venue': 'String processing and information retrieval. Springer, pages 115–126.', 'citeRegEx': 'Kondrak.,? 2005', 'shortCiteRegEx': 'Kondrak.', 'year': 2005}, {'title': 'Better word representations with recursive neural networks for morphology', 'author': ['Thang Luong', 'Richard Socher', 'Christopher D Manning.'], 'venue': 'CoNLL. pages 104–113.', 'citeRegEx': 'Luong et al\\.,? 2013', 'shortCiteRegEx': 'Luong et al\\.', 'year': 2013}, {'title': 'Towards a computational model of poetry generation', 'author': ['Hisar Manurung', 'Graeme Ritchie', 'Henry Thompson.'], 'venue': 'Technical report, The University of Edinburgh.', 'citeRegEx': 'Manurung et al\\.,? 2000', 'shortCiteRegEx': 'Manurung et al\\.', 'year': 2000}, {'title': 'Efficient estimation of word representations in vector space', 'author': ['Tomas Mikolov', 'Kai Chen', 'Greg Corrado', 'Jeffrey Dean.'], 'venue': 'arXiv preprint arXiv:1301.3781 .', 'citeRegEx': 'Mikolov et al\\.,? 2013', 'shortCiteRegEx': 'Mikolov et al\\.', 'year': 2013}, {'title': 'Recurrent neural network based language model', 'author': ['Tomas Mikolov', 'Martin Karafiát', 'Lukas Burget', 'Jan Cernockỳ', 'Sanjeev Khudanpur.'], 'venue': 'INTERSPEECH. volume 2, page 3.', 'citeRegEx': 'Mikolov et al\\.,? 2010', 'shortCiteRegEx': 'Mikolov et al\\.', 'year': 2010}, {'title': 'Poetry generation system with an emotional personality', 'author': ['Joanna Misztal', 'Bipin Indurkhya.'], 'venue': 'Proceedings of the Fourth International Conference on Computational Creativity.', 'citeRegEx': 'Misztal and Indurkhya.,? 2014', 'shortCiteRegEx': 'Misztal and Indurkhya.', 'year': 2014}, {'title': 'Performance tradeoffs in dynamic time warping algorithms for isolated word recognition', 'author': ['Cory Myers', 'Lawrence R Rabiner', 'Aaron E Rosenberg.'], 'venue': 'Acoustics, Speech and Signal Processing, IEEE Transactions on 28(6):623–635.', 'citeRegEx': 'Myers et al\\.,? 1980', 'shortCiteRegEx': 'Myers et al\\.', 'year': 1980}, {'title': 'Gaiku: Generating haiku with word associations norms', 'author': ['Yael Netzer', 'David Gabay', 'Yoav Goldberg', 'Michael Elhadad.'], 'venue': 'Proceedings of the Workshop on Computational Approaches to Linguistic Creativity. Association for Computational Lin-', 'citeRegEx': 'Netzer et al\\.,? 2009', 'shortCiteRegEx': 'Netzer et al\\.', 'year': 2009}, {'title': 'Learning phoneme mappings for transliteration without parallel data', 'author': ['Sujith Ravi', 'Kevin Knight.'], 'venue': 'Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Compu-', 'citeRegEx': 'Ravi and Knight.,? 2009', 'shortCiteRegEx': 'Ravi and Knight.', 'year': 2009}, {'title': 'Training neural network language models on very large corpora', 'author': ['Holger Schwenk', 'Jean-Luc Gauvain.'], 'venue': 'Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. Association for', 'citeRegEx': 'Schwenk and Gauvain.,? 2005', 'shortCiteRegEx': 'Schwenk and Gauvain.', 'year': 2005}, {'title': 'Generating text with recurrent neural networks', 'author': ['Ilya Sutskever', 'James Martens', 'Geoffrey E Hinton.'], 'venue': 'Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 1017–1024.', 'citeRegEx': 'Sutskever et al\\.,? 2011', 'shortCiteRegEx': 'Sutskever et al\\.', 'year': 2011}, {'title': 'Sequence to sequence learning with neural networks', 'author': ['Ilya Sutskever', 'Oriol Vinyals', 'Quoc V Le.'], 'venue': 'Advances in neural information processing systems. pages 3104–3112.', 'citeRegEx': 'Sutskever et al\\.,? 2014', 'shortCiteRegEx': 'Sutskever et al\\.', 'year': 2014}, {'title': 'Harnessing constraint programming for poetry composition', 'author': ['Jukka M Toivanen', 'Matti Järvisalo', 'Hannu Toivonen'], 'venue': 'In Proceedings of the Fourth International Conference on Computational Creativity', 'citeRegEx': 'Toivanen et al\\.,? \\Q2013\\E', 'shortCiteRegEx': 'Toivanen et al\\.', 'year': 2013}, {'title': 'Freetts 1.2: A speech synthesizer written entirely in the java programming language', 'author': ['Willie Walker', 'Paul Lamere', 'Philip Kwok'], 'venue': None, 'citeRegEx': 'Walker et al\\.,? \\Q2010\\E', 'shortCiteRegEx': 'Walker et al\\.', 'year': 2010}, {'title': 'The carnegie mellon pronouncing dictionary [cmudict', 'author': ['R Weide.'], 'venue': '0.6].', 'citeRegEx': 'Weide.,? 2005', 'shortCiteRegEx': 'Weide.', 'year': 2005}, {'title': 'Backpropagation through time: what it does and how to do it', 'author': ['Paul J Werbos.'], 'venue': 'Proceedings of the IEEE 78(10):1550–1560.', 'citeRegEx': 'Werbos.,? 1990', 'shortCiteRegEx': 'Werbos.', 'year': 1990}, {'title': 'Generating chinese classical poems with rnn encoderdecoder', 'author': ['Xiaoyuan Yi', 'Ruoyu Li', 'andMaosong Sun'], 'venue': None, 'citeRegEx': 'Yi et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Yi et al\\.', 'year': 2016}, {'title': 'Chinese poetry generation with recurrent neural networks', 'author': ['Xingxing Zhang', 'Mirella Lapata.'], 'venue': 'EMNLP. pages 670–680.', 'citeRegEx': 'Zhang and Lapata.,? 2014', 'shortCiteRegEx': 'Zhang and Lapata.', 'year': 2014}]  \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [{'title': 'Neural Machine Translation by Jointly Learning to Align and Translate', 'author': ['Dzmitry Bahdanau', 'Kyunghyun Cho', 'Yoshua Bengio.'], 'venue': 'CoRR. volume abs/1409.0. http://arxiv.org/abs/1409.0473.', 'citeRegEx': 'Bahdanau et al\\.,? 2014', 'shortCiteRegEx': 'Bahdanau et al\\.', 'year': 2014}, {'title': 'The Mathematics of Statistical Machine Translation : Parameter Estimation', 'author': ['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Robert L Mercer.'], 'venue': 'Computational Linguistics 10598.', 'citeRegEx': 'Brown et al\\.,? 1993', 'shortCiteRegEx': 'Brown et al\\.', 'year': 1993}, {'title': 'Generative Incremental Dependency Parsing with Neural Networks', 'author': ['Jan Buys', 'Phil Blunsom.'], 'venue': 'Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Con-', 'citeRegEx': 'Buys and Blunsom.,? 2015', 'shortCiteRegEx': 'Buys and Blunsom.', 'year': 2015}, {'title': 'A Structured Language Model', 'author': ['Ciprian Chelba.'], 'venue': 'Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Madrid, Spain, pages 498–500.', 'citeRegEx': 'Chelba.,? 1997', 'shortCiteRegEx': 'Chelba.', 'year': 1997}, {'title': 'Structured language modeling', 'author': ['Ciprian Chelba', 'Frederick Jelinek'], 'venue': None, 'citeRegEx': 'Chelba and Jelinek.,? \\Q2000\\E', 'shortCiteRegEx': 'Chelba and Jelinek.', 'year': 2000}, {'title': 'A Hierarchical PhraseBased Model for Statistical Machine Translation', 'author': ['David Chiang.'], 'venue': 'Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05). Association for Computational Lin-', 'citeRegEx': 'Chiang.,? 2005', 'shortCiteRegEx': 'Chiang.', 'year': 2005}, {'title': 'Syntactic structures', 'author': ['Noam Chomsky.'], 'venue': 'LondonThe Hague-Paris.', 'citeRegEx': 'Chomsky.,? 1957', 'shortCiteRegEx': 'Chomsky.', 'year': 1957}, {'title': 'A Character-level Decoder without Explicit Segmentation for Neural Machine Translation', 'author': ['Junyoung Chung', 'Kyunghyun Cho', 'Yoshua Bengio'], 'venue': None, 'citeRegEx': 'Chung et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Chung et al\\.', 'year': 2016}, {'title': 'Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling', 'author': ['Junyoung Chung', 'Caglar Gulcehre', 'Kyunghyun Cho', 'Yoshua Bengio.'], 'venue': 'arXiv pages 1–9.', 'citeRegEx': 'Chung et al\\.,? 2014', 'shortCiteRegEx': 'Chung et al\\.', 'year': 2014}, {'title': 'Character-based Neural Machine Translation', 'author': ['Marta R Costa-Jussà', 'José A R Fonollosa.'], 'venue': 'CoRR abs/1603.0. http://arxiv.org/abs/1603.00810.', 'citeRegEx': 'Costa.Jussà and Fonollosa.,? 2016', 'shortCiteRegEx': 'Costa.Jussà and Fonollosa.', 'year': 2016}, {'title': 'Handwritten Digit Recognition with a Back-Propagation Network', 'author': ['Y Le Cun', 'Bernhard E Boser', 'John S Denker', 'R E Howard', 'W Habbard', 'L D Jackel', 'Dale B Henderson.'], 'venue': 'Advances in Neural Information Processing Systems pages 396–404.', 'citeRegEx': 'Cun et al\\.,? 1990', 'shortCiteRegEx': 'Cun et al\\.', 'year': 1990}, {'title': 'Recurrent Neural Network Grammars', 'author': ['Chris Dyer', 'Adhiguna Kuncoro', 'Miguel Ballesteros', 'Noah A Smith.'], 'venue': 'CoRR abs/1602.0. http://arxiv.org/abs/1602.07776.', 'citeRegEx': 'Dyer et al\\.,? 2016', 'shortCiteRegEx': 'Dyer et al\\.', 'year': 2016}, {'title': 'Finding structure in time', 'author': ['Jeffrey L. Elman.'], 'venue': 'Cognitive Science 14(2):179–211. https://doi.org/10.1016/0364-0213(90)90002-E.', 'citeRegEx': 'Elman.,? 1990', 'shortCiteRegEx': 'Elman.', 'year': 1990}, {'title': 'A Neural Syntactic Language Model', 'author': ['Ahmad Emami', 'Frederick Jelinek.'], 'venue': 'Machine Learning 60(1):195–227. https://doi.org/10.1007/s10994005-0916-y.', 'citeRegEx': 'Emami and Jelinek.,? 2005', 'shortCiteRegEx': 'Emami and Jelinek.', 'year': 2005}, {'title': 'Tree-to-Sequence Attentional Neural Machine Translation', 'author': ['Akiko Eriguchi', 'Kazuma Hashimoto', 'Yoshimasa Tsuruoka.'], 'venue': 'CoRR abs/1603.0. http://arxiv.org/abs/1603.06075.', 'citeRegEx': 'Eriguchi et al\\.,? 2016', 'shortCiteRegEx': 'Eriguchi et al\\.', 'year': 2016}, {'title': 'Scalable inference and training of context-rich syntactic translation models', 'author': ['Michel Galley', 'Jonathan Graehl', 'Kevin Knight', 'Daniel Marcu', 'Steve DeNeefe', 'Wei Wang', 'Ignacio Thayer', 'New York', 'Marina Rey.'], 'venue': 'Proceedings of the 21st In-', 'citeRegEx': 'Galley et al\\.,? 2006', 'shortCiteRegEx': 'Galley et al\\.', 'year': 2006}, {'title': 'Discriminative Training of a Neural Network Statistical Parser', 'author': ['James Henderson.'], 'venue': 'Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume. Barcelona, Spain, pages 95–102.', 'citeRegEx': 'Henderson.,? 2004', 'shortCiteRegEx': 'Henderson.', 'year': 2004}, {'title': 'Distributed representations', 'author': ['G E Hinton', 'J L McClelland', 'D E Rumelhart.'], 'venue': 'Parallel Distributed Processing pages 77–109. https://doi.org/10.1146/annurev-psych-120710100344.', 'citeRegEx': 'Hinton et al\\.,? 1986', 'shortCiteRegEx': 'Hinton et al\\.', 'year': 1986}, {'title': 'Improving neural networks by preventing co-adaptation of feature detectors', 'author': ['Geoffrey E. Hinton', 'Nitish Srivastava', 'Alex Krizhevsky', 'Ilya Sutskever', 'Ruslan R. Salakhutdinov.'], 'venue': 'arXiv: 1207.0580 pages 1–18.', 'citeRegEx': 'Hinton et al\\.,? 2012', 'shortCiteRegEx': 'Hinton et al\\.', 'year': 2012}, {'title': 'Statistical syntax-directed translation with extended domain of locality', 'author': ['Liang Huang', 'Kevin Knight', 'Aravind Joshi.'], 'venue': 'Proceedings of AMTA. pages 66–73.', 'citeRegEx': 'Huang et al\\.,? 2006', 'shortCiteRegEx': 'Huang et al\\.', 'year': 2006}, {'title': 'Computation of the Probability of Initial Substring Generation by Stochastic Context-Free Grammars', 'author': ['Frederick Jelinek', 'John D Lafferty.'], 'venue': 'Computational Linguistics 17(3):315–324. http://www.aclweb.org/anthology/J91-3004.', 'citeRegEx': 'Jelinek and Lafferty.,? 1991', 'shortCiteRegEx': 'Jelinek and Lafferty.', 'year': 1991}, {'title': 'Statistical phrase-based translation', 'author': ['Philipp Koehn', 'Franz Josef Och', 'Daniel Marcu.'], 'venue': 'Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology', 'citeRegEx': 'Koehn et al\\.,? 2003', 'shortCiteRegEx': 'Koehn et al\\.', 'year': 2003}, {'title': 'Treeto-String Alignment Template for Statistical Machine Translation', 'author': ['Yang Liu', 'Qun Liu', 'Shouxun Lin.'], 'venue': 'Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for', 'citeRegEx': 'Liu et al\\.,? 2006', 'shortCiteRegEx': 'Liu et al\\.', 'year': 2006}, {'title': 'Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models', 'author': ['Minh-Thang Luong', 'Christopher D Manning.'], 'venue': 'CoRR abs/1604.0. http://arxiv.org/abs/1604.00788.', 'citeRegEx': 'Luong and Manning.,? 2016', 'shortCiteRegEx': 'Luong and Manning.', 'year': 2016}, {'title': 'Effective Approaches to Attentionbased Neural Machine Translation', 'author': ['Thang Luong', 'Hieu Pham', 'Christopher D Manning.'], 'venue': 'Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for', 'citeRegEx': 'Luong et al\\.,? 2015a', 'shortCiteRegEx': 'Luong et al\\.', 'year': 2015}, {'title': 'Addressing the Rare Word Problem in Neural Machine Translation', 'author': ['Thang Luong', 'Ilya Sutskever', 'Quoc Le', 'Oriol Vinyals', 'Wojciech Zaremba.'], 'venue': 'Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-', 'citeRegEx': 'Luong et al\\.,? 2015b', 'shortCiteRegEx': 'Luong et al\\.', 'year': 2015}, {'title': 'Building a large annotated corpus of English : the Penn Treebank', 'author': ['Mitchell Marcus', 'Beatrice Santorini', 'Mary Marcinkiewicz.'], 'venue': 'Computational Linguistics 19(2):313–330. http://dblp.unitrier.de/db/journals/coling/coling19.html#MarcusSM94.', 'citeRegEx': 'Marcus et al\\.,? 1993', 'shortCiteRegEx': 'Marcus et al\\.', 'year': 1993}, {'title': 'Neural Transformation Machine: {A} New Architecture for Sequence-to-Sequence Learning', 'author': ['Fandong Meng', 'Zhengdong Lu', 'Zhaopeng Tu', 'Hang Li', 'Qun Liu.'], 'venue': 'CoRR abs/1506.0. http://arxiv.org/abs/1506.06442.', 'citeRegEx': 'Meng et al\\.,? 2015', 'shortCiteRegEx': 'Meng et al\\.', 'year': 2015}, {'title': 'Forest-based Translation Rule Extraction', 'author': ['Haitao Mi', 'Liang Huang.'], 'venue': 'Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Honolulu, Hawaii, pages 206–', 'citeRegEx': 'Mi and Huang.,? 2008', 'shortCiteRegEx': 'Mi and Huang.', 'year': 2008}, {'title': 'Forest-Based Translation', 'author': ['Haitao Mi', 'Liang Huang', 'Qun Liu.'], 'venue': 'Proceedings of ACL-08: HLT . Association for Computational Linguistics, Columbus, Ohio, pages 192–199. http://www.aclweb.org/anthology/P/P08/P08-1023.', 'citeRegEx': 'Mi et al\\.,? 2008', 'shortCiteRegEx': 'Mi et al\\.', 'year': 2008}, {'title': 'Recurrent neural network based language model', 'author': ['Tomas Mikolov', 'Martin Karafiát', 'Lukas Burget', 'Jan Cernock\\‘y', 'Sanjeev Khudanpur'], 'venue': 'In INTERSPEECH', 'citeRegEx': 'Mikolov et al\\.,? \\Q2010\\E', 'shortCiteRegEx': 'Mikolov et al\\.', 'year': 2010}, {'title': 'BLEU : a Method for Automatic Evaluation of Machine Translation', 'author': ['Kishore Papineni', 'Salim Roukos', 'Todd Ward', 'Weijing Zhu.'], 'venue': 'Computational Linguistics (July):311–318.', 'citeRegEx': 'Papineni et al\\.,? 2002', 'shortCiteRegEx': 'Papineni et al\\.', 'year': 2002}, {'title': 'Neural Machine Translation of Rare Words with Subword Units', 'author': ['Rico Sennrich', 'Barry Haddow', 'Alexandra Birch.'], 'venue': 'CoRR abs/1508.0. http://arxiv.org/abs/1508.07909.', 'citeRegEx': 'Sennrich et al\\.,? 2015', 'shortCiteRegEx': 'Sennrich et al\\.', 'year': 2015}, {'title': 'Syntactically Guided Neural Machine Translation', 'author': ['Felix Stahlberg', 'Eva Hasler', 'Aurelien Waite', 'Bill Byrne.'], 'venue': 'CoRR abs/1605.0. http://arxiv.org/abs/1605.04569.', 'citeRegEx': 'Stahlberg et al\\.,? 2016', 'shortCiteRegEx': 'Stahlberg et al\\.', 'year': 2016}, {'title': 'Sequence to Sequence Learning with Neural Networks', 'author': ['Ilya Sutskever', 'Oriol Vinyals', 'Quoc V Le.'], 'venue': 'CoRR abs/1409.3. http://arxiv.org/abs/1409.3215.', 'citeRegEx': 'Sutskever et al\\.,? 2014', 'shortCiteRegEx': 'Sutskever et al\\.', 'year': 2014}, {'title': 'A Latent Variable Model for Generative Dependency Parsing', 'author': ['Ivan Titov', 'James Henderson.'], 'venue': 'Proceedings of the Tenth International Conference on Parsing Technologies. Association for Computational Linguis-', 'citeRegEx': 'Titov and Henderson.,? 2007', 'shortCiteRegEx': 'Titov and Henderson.', 'year': 2007}, {'title': 'Blocks and Fuel: Frameworks for deep learning', 'author': ['Bart van Merriënboer', 'Dzmitry Bahdanau', 'Vincent Dumoulin', 'Dmitriy Serdyuk', 'David Warde-Farley', 'Jan Chorowski', 'Yoshua Bengio.'], 'venue': 'CoRR abs/1506.0. http://arxiv.org/abs/1506.00619.', 'citeRegEx': 'Merriënboer et al\\.,? 2015', 'shortCiteRegEx': 'Merriënboer et al\\.', 'year': 2015}, {'title': 'A Novel Dependency-to-String Model for Statistical Machine Translation', 'author': ['Jun Xie', 'Haitao Mi', 'Qun Liu.'], 'venue': 'Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis-', 'citeRegEx': 'Xie et al\\.,? 2011', 'shortCiteRegEx': 'Xie et al\\.', 'year': 2011}, {'title': 'Recurrent Neural Network Regularization', 'author': ['Wojciech Zaremba', 'Ilya Sutskever', 'Oriol Vinyals.'], 'venue': 'CoRR abs/1409.2. http://arxiv.org/abs/1409.2329.', 'citeRegEx': 'Zaremba et al\\.,? 2014', 'shortCiteRegEx': 'Zaremba et al\\.', 'year': 2014}, {'title': 'ADADELTA: An Adaptive Learning Rate Method', 'author': ['Matthew D. Zeiler.'], 'venue': 'arXiv abs/1212.5:6. http://arxiv.org/abs/1212.5701.', 'citeRegEx': 'Zeiler.,? 2012', 'shortCiteRegEx': 'Zeiler.', 'year': 2012}, {'title': 'Phrase-Based Statistical Machine Translation', 'author': ['Richard Zens', 'Franz Josef Och', 'Hermann Ney.'], 'venue': 'pages 18–32.', 'citeRegEx': 'Zens et al\\.,? 2002', 'shortCiteRegEx': 'Zens et al\\.', 'year': 2002}]  \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [{'title': 'Domain-adversarial neural networks', 'author': ['Hana Ajakan', 'Pascal Germain', 'Hugo Larochelle', 'François Laviolette', 'Mario Marchand.'], 'venue': 'arXiv preprint arXiv:1412.4446 .', 'citeRegEx': 'Ajakan et al\\.,? 2014', 'shortCiteRegEx': 'Ajakan et al\\.', 'year': 2014}, {'title': 'A theory of learning from different domains', 'author': ['Shai Ben-David', 'John Blitzer', 'Koby Crammer', 'Alex Kulesza', 'Fernando Pereira', 'Jennifer Wortman Vaughan.'], 'venue': 'Machine learning 79(1-2):151–175.', 'citeRegEx': 'Ben.David et al\\.,? 2010', 'shortCiteRegEx': 'Ben.David et al\\.', 'year': 2010}, {'title': 'Analysis of representations for domain adaptation. Advances in neural information processing systems 19:137', 'author': ['Shai Ben-David', 'John Blitzer', 'Koby Crammer', 'Fernando Pereira'], 'venue': None, 'citeRegEx': 'Ben.David et al\\.,? \\Q2007\\E', 'shortCiteRegEx': 'Ben.David et al\\.', 'year': 2007}, {'title': 'Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification', 'author': ['John Blitzer', 'Mark Dredze', 'Fernando Pereira'], 'venue': 'In ACL', 'citeRegEx': 'Blitzer et al\\.,? \\Q2007\\E', 'shortCiteRegEx': 'Blitzer et al\\.', 'year': 2007}, {'title': 'Domain separation networks', 'author': ['Konstantinos Bousmalis', 'George Trigeorgis', 'Nathan Silberman', 'Dilip Krishnan', 'Dumitru Erhan.'], 'venue': 'Advances in Neural Information Processing Systems. pages 343– 351.', 'citeRegEx': 'Bousmalis et al\\.,? 2016', 'shortCiteRegEx': 'Bousmalis et al\\.', 'year': 2016}, {'title': 'Adversarial deep averaging networks for cross-lingual sentiment classification', 'author': ['Xilun Chen', 'Yu Sun', 'Ben Athiwaratkun', 'Claire Cardie', 'Kilian Weinberger.'], 'venue': 'arXiv preprint arXiv:1606.01614 .', 'citeRegEx': 'Chen et al\\.,? 2016', 'shortCiteRegEx': 'Chen et al\\.', 'year': 2016}, {'title': 'Empirical evaluation of gated recurrent neural networks on sequence modeling', 'author': ['Junyoung Chung', 'Caglar Gulcehre', 'KyungHyun Cho', 'Yoshua Bengio.'], 'venue': 'arXiv preprint arXiv:1412.3555 .', 'citeRegEx': 'Chung et al\\.,? 2014', 'shortCiteRegEx': 'Chung et al\\.', 'year': 2014}, {'title': 'A unified architecture for natural language processing: Deep neural networks with multitask learning', 'author': ['Ronan Collobert', 'Jason Weston.'], 'venue': 'Proceedings of ICML.', 'citeRegEx': 'Collobert and Weston.,? 2008', 'shortCiteRegEx': 'Collobert and Weston.', 'year': 2008}, {'title': 'Natural language processing (almost) from scratch', 'author': ['Ronan Collobert', 'Jason Weston', 'Léon Bottou', 'Michael Karlen', 'Koray Kavukcuoglu', 'Pavel Kuksa.'], 'venue': 'The Journal of Machine Learning Research 12:2493–2537.', 'citeRegEx': 'Collobert et al\\.,? 2011', 'shortCiteRegEx': 'Collobert et al\\.', 'year': 2011}, {'title': 'Finding structure in time', 'author': ['Jeffrey L Elman.'], 'venue': 'Cognitive science 14(2):179–211.', 'citeRegEx': 'Elman.,? 1990', 'shortCiteRegEx': 'Elman.', 'year': 1990}, {'title': 'Unsupervised domain adaptation by backpropagation', 'author': ['Yaroslav Ganin', 'Victor Lempitsky.'], 'venue': 'Proceedings of the 32nd International Conference on Machine Learning (ICML-15). pages 1180–1189.', 'citeRegEx': 'Ganin and Lempitsky.,? 2015', 'shortCiteRegEx': 'Ganin and Lempitsky.', 'year': 2015}, {'title': 'Domain adaptation for large-scale sentiment classification: A deep learning approach', 'author': ['Xavier Glorot', 'Antoine Bordes', 'Yoshua Bengio.'], 'venue': 'Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 513–520.', 'citeRegEx': 'Glorot et al\\.,? 2011', 'shortCiteRegEx': 'Glorot et al\\.', 'year': 2011}, {'title': 'Generative adversarial nets', 'author': ['Ian Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Aaron Courville', 'Yoshua Bengio.'], 'venue': 'Advances in Neural Information Processing Systems. pages 2672–2680.', 'citeRegEx': 'Goodfellow et al\\.,? 2014', 'shortCiteRegEx': 'Goodfellow et al\\.', 'year': 2014}, {'title': 'Generating sequences with recurrent neural networks', 'author': ['Alex Graves.'], 'venue': 'arXiv preprint arXiv:1308.0850 .', 'citeRegEx': 'Graves.,? 2013', 'shortCiteRegEx': 'Graves.', 'year': 2013}, {'title': 'Long short-term memory', 'author': ['Sepp Hochreiter', 'Jürgen Schmidhuber.'], 'venue': 'Neural computation 9(8):1735–1780.', 'citeRegEx': 'Hochreiter and Schmidhuber.,? 1997', 'shortCiteRegEx': 'Hochreiter and Schmidhuber.', 'year': 1997}, {'title': 'Factorized latent spaces with structured sparsity', 'author': ['Yangqing Jia', 'Mathieu Salzmann', 'Trevor Darrell.'], 'venue': 'Advances in Neural Information Processing Systems. pages 982–990.', 'citeRegEx': 'Jia et al\\.,? 2010', 'shortCiteRegEx': 'Jia et al\\.', 'year': 2010}, {'title': 'An empirical exploration of recurrent network architectures', 'author': ['Rafal Jozefowicz', 'Wojciech Zaremba', 'Ilya Sutskever.'], 'venue': 'Proceedings of The 32nd International Conference on Machine Learning.', 'citeRegEx': 'Jozefowicz et al\\.,? 2015', 'shortCiteRegEx': 'Jozefowicz et al\\.', 'year': 2015}, {'title': 'A convolutional neural network for modelling sentences', 'author': ['Nal Kalchbrenner', 'Edward Grefenstette', 'Phil Blunsom.'], 'venue': 'Proceedings of ACL.', 'citeRegEx': 'Kalchbrenner et al\\.,? 2014', 'shortCiteRegEx': 'Kalchbrenner et al\\.', 'year': 2014}, {'title': 'Recurrent neural network for text classification with multi-task learning', 'author': ['Pengfei Liu', 'Xipeng Qiu', 'Xuanjing Huang.'], 'venue': 'Proceedings of International Joint Conference on Artificial Intelligence. https://arxiv.org/abs/1605.05101.', 'citeRegEx': 'Liu et al\\.,? 2016', 'shortCiteRegEx': 'Liu et al\\.', 'year': 2016}, {'title': 'Representation learning using multi-task deep neural networks for semantic classification and information retrieval', 'author': ['Xiaodong Liu', 'Jianfeng Gao', 'Xiaodong He', 'Li Deng', 'Kevin Duh', 'Ye-Yi Wang.'], 'venue': 'NAACL.', 'citeRegEx': 'Liu et al\\.,? 2015', 'shortCiteRegEx': 'Liu et al\\.', 'year': 2015}, {'title': 'Multi-task sequence to sequence learning', 'author': ['Minh-Thang Luong', 'Quoc V Le', 'Ilya Sutskever', 'Oriol Vinyals', 'Lukasz Kaiser.'], 'venue': 'arXiv preprint arXiv:1511.06114 .', 'citeRegEx': 'Luong et al\\.,? 2015', 'shortCiteRegEx': 'Luong et al\\.', 'year': 2015}, {'title': 'Learning word vectors for sentiment analysis', 'author': ['Andrew L Maas', 'Raymond E Daly', 'Peter T Pham', 'Dan Huang', 'Andrew Y Ng', 'Christopher Potts.'], 'venue': 'Proceedings of the ACL. pages 142–150.', 'citeRegEx': 'Maas et al\\.,? 2011', 'shortCiteRegEx': 'Maas et al\\.', 'year': 2011}, {'title': 'Cross-stitch networks for multi-task learning', 'author': ['Ishan Misra', 'Abhinav Shrivastava', 'Abhinav Gupta', 'Martial Hebert.'], 'venue': 'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 3994–4003.', 'citeRegEx': 'Misra et al\\.,? 2016', 'shortCiteRegEx': 'Misra et al\\.', 'year': 2016}, {'title': 'Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales', 'author': ['Bo Pang', 'Lillian Lee.'], 'venue': 'Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational', 'citeRegEx': 'Pang and Lee.,? 2005', 'shortCiteRegEx': 'Pang and Lee.', 'year': 2005}, {'title': 'Image-text multi-modal representation learning by adversarial backpropagation', 'author': ['Gwangbeen Park', 'Woobin Im.'], 'venue': 'arXiv preprint arXiv:1612.08354 .', 'citeRegEx': 'Park and Im.,? 2016', 'shortCiteRegEx': 'Park and Im.', 'year': 2016}, {'title': 'Glove: Global vectors for word representation', 'author': ['Jeffrey Pennington', 'Richard Socher', 'Christopher D Manning.'], 'venue': 'Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12:1532–1543.', 'citeRegEx': 'Pennington et al\\.,? 2014', 'shortCiteRegEx': 'Pennington et al\\.', 'year': 2014}, {'title': 'Factorized orthogonal latent spaces', 'author': ['Mathieu Salzmann', 'Carl Henrik Ek', 'Raquel Urtasun', 'Trevor Darrell.'], 'venue': 'AISTATS. pages 701–708.', 'citeRegEx': 'Salzmann et al\\.,? 2010', 'shortCiteRegEx': 'Salzmann et al\\.', 'year': 2010}, {'title': 'Recursive deep models for semantic compositionality over a sentiment treebank', 'author': ['Richard Socher', 'Alex Perelygin', 'Jean Y Wu', 'Jason Chuang', 'Christopher D Manning', 'Andrew Y Ng', 'Christopher Potts.'], 'venue': 'Proceedings of EMNLP.', 'citeRegEx': 'Socher et al\\.,? 2013', 'shortCiteRegEx': 'Socher et al\\.', 'year': 2013}, {'title': 'Sequence to sequence learning with neural networks', 'author': ['Ilya Sutskever', 'Oriol Vinyals', 'Quoc VV Le.'], 'venue': 'Advances in Neural Information Processing Systems. pages 3104–3112.', 'citeRegEx': 'Sutskever et al\\.,? 2014', 'shortCiteRegEx': 'Sutskever et al\\.', 'year': 2014}, {'title': 'Unsupervised cross-domain image generation', 'author': ['Yaniv Taigman', 'Adam Polyak', 'Lior Wolf.'], 'venue': 'arXiv preprint arXiv:1611.02200 .', 'citeRegEx': 'Taigman et al\\.,? 2016', 'shortCiteRegEx': 'Taigman et al\\.', 'year': 2016}, {'title': 'Facial landmark detection by deep multi-task learning', 'author': ['Zhanpeng Zhang', 'Ping Luo', 'Chen Change Loy', 'Xiaoou Tang.'], 'venue': 'European Conference on Computer Vision. Springer, pages 94–108.', 'citeRegEx': 'Zhang et al\\.,? 2014', 'shortCiteRegEx': 'Zhang et al\\.', 'year': 2014}]  \n",
            "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [{'title': 'Don’t count, predict! a systematic comparison of context-counting vs', 'author': ['Marco Baroni', 'Georgiana Dinu', 'Germán Kruszewski.'], 'venue': 'context-predicting semantic vectors. In ACL (1). pages 238–247.', 'citeRegEx': 'Baroni et al\\.,? 2014', 'shortCiteRegEx': 'Baroni et al\\.', 'year': 2014}, {'title': 'Computational interpretations of the gricean maxims in the generation of referring expressions', 'author': ['Robert Dale', 'Ehud Reiter.'], 'venue': 'Cognitive Science 19(2):233–263.', 'citeRegEx': 'Dale and Reiter.,? 1995', 'shortCiteRegEx': 'Dale and Reiter.', 'year': 1995}, {'title': 'Visual dialog', 'author': ['Abhishek Das', 'Satwik Kottur', 'Khushi Gupta', 'Avi Singh', 'Deshraj Yadav', 'José M.F. Moura', 'Devi Parikh', 'Dhruv Batra.'], 'venue': 'CoRR abs/1611.08669. http://arxiv.org/abs/1611.08669.', 'citeRegEx': 'Das et al\\.,? 2016', 'shortCiteRegEx': 'Das et al\\.', 'year': 2016}, {'title': 'ImageNet: A Large-Scale Hierarchical Image Database', 'author': ['Jia Deng', 'W. Dong', 'Richard Socher', 'L.-J. Li', 'K. Li', 'L. Fei-Fei.'], 'venue': 'CVPR09.', 'citeRegEx': 'Deng et al\\.,? 2009', 'shortCiteRegEx': 'Deng et al\\.', 'year': 2009}, {'title': 'What do you know about an alligator when you know the company it keeps? Semantics and Pragmatics 9(17):1–63', 'author': ['Katrin Erk.'], 'venue': 'https://doi.org/10.3765/sp.9.17.', 'citeRegEx': 'Erk.,? 2016', 'shortCiteRegEx': 'Erk.', 'year': 2016}, {'title': 'Visual information in semantic representation', 'author': ['Yansong Feng', 'Mirella Lapata.'], 'venue': 'Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for', 'citeRegEx': 'Feng and Lapata.,? 2010', 'shortCiteRegEx': 'Feng and Lapata.', 'year': 2010}, {'title': 'Devise: A deep visualsemantic embedding model', 'author': ['Andrea Frome', 'Greg S Corrado', 'Jon Shlens', 'Samy Bengio', 'Jeff Dean', 'Marc Aurelio Ranzato', 'Tomas Mikolov.'], 'venue': 'C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q.', 'citeRegEx': 'Frome et al\\.,? 2013', 'shortCiteRegEx': 'Frome et al\\.', 'year': 2013}, {'title': 'From the virtual to the real world: Referring to objects in real-world spatial scenes', 'author': ['Dimitra Gkatzia', 'Verena Rieser', 'Phil Bartie', 'William Mackaness.'], 'venue': 'Proceedings of EMNLP 2015. Association for Computational Linguistics.', 'citeRegEx': 'Gkatzia et al\\.,? 2015', 'shortCiteRegEx': 'Gkatzia et al\\.', 'year': 2015}, {'title': 'The IAPR TC-12 benchmark: a new evaluation resource for visual information systems', 'author': ['Michael Grubinger', 'Paul Clough', 'Henning Müller', 'Thomas Deselaers.'], 'venue': 'Proceedings of the International Conference on Language Resources and Evaluation', 'citeRegEx': 'Grubinger et al\\.,? 2006', 'shortCiteRegEx': 'Grubinger et al\\.', 'year': 2006}, {'title': 'Natural language object retrieval', 'author': ['Ronghang Hu', 'Huazhe Xu', 'Marcus Rohrbach', 'Jiashi Feng', 'Kate Saenko', 'Trevor Darrell.'], 'venue': 'CoRR abs/1511.04164. http://arxiv.org/abs/1511.04164.', 'citeRegEx': 'Hu et al\\.,? 2015', 'shortCiteRegEx': 'Hu et al\\.', 'year': 2015}, {'title': 'Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics', 'author': ['Douwe Kiela', 'Léon Bottou.'], 'venue': 'Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-14).', 'citeRegEx': 'Kiela and Bottou.,? 2014', 'shortCiteRegEx': 'Kiela and Bottou.', 'year': 2014}, {'title': 'Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes', 'author': ['Satwik Kottur', 'Ramakrishna Vedantam', 'José MF Moura', 'Devi Parikh.'], 'venue': 'Proceedings of the IEEE Conference on Computer Vision and Pattern', 'citeRegEx': 'Kottur et al\\.,? 2016', 'shortCiteRegEx': 'Kottur et al\\.', 'year': 2016}, {'title': 'Computational generation of referring expressions: A survey', 'author': ['Emiel Krahmer', 'Kees Van Deemter.'], 'venue': 'Computational Linguistics 38(1):173–218.', 'citeRegEx': 'Krahmer and Deemter.,? 2012', 'shortCiteRegEx': 'Krahmer and Deemter.', 'year': 2012}, {'title': 'Learning to detect unseen object classes by between-class attribute transfer', 'author': ['Christoph H Lampert', 'Hannes Nickisch', 'Stefan Harmeling.'], 'venue': 'IEEE Computer Vision and Pattern Recognition. IEEE, pages 951–958.', 'citeRegEx': 'Lampert et al\\.,? 2009', 'shortCiteRegEx': 'Lampert et al\\.', 'year': 2009}, {'title': 'Attribute-based classification for zero-shot visual object categorization', 'author': ['Christoph H. Lampert', 'Hannes Nickisch', 'Stefan Harmeling.'], 'venue': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 36(3):453–465.', 'citeRegEx': 'Lampert et al\\.,? 2013', 'shortCiteRegEx': 'Lampert et al\\.', 'year': 2013}, {'title': 'Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world', 'author': ['Angeliki Lazaridou', 'Elia Bruni', 'Marco Baroni.'], 'venue': 'Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics', 'citeRegEx': 'Lazaridou et al\\.,? 2014', 'shortCiteRegEx': 'Lazaridou et al\\.', 'year': 2014}, {'title': 'Hubness and pollution: Delving into cross-space mapping for zero-shot learning', 'author': ['Angeliki Lazaridou', 'Georgiana Dinu', 'Marco Baroni.'], 'venue': 'Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-', 'citeRegEx': 'Lazaridou et al\\.,? 2015a', 'shortCiteRegEx': 'Lazaridou et al\\.', 'year': 2015}, {'title': 'Combining language and vision with a multimodal skip-gram model', 'author': ['Angeliki Lazaridou', 'Nghia The Pham', 'Marco Baroni.'], 'venue': 'Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-', 'citeRegEx': 'Lazaridou et al\\.,? 2015b', 'shortCiteRegEx': 'Lazaridou et al\\.', 'year': 2015}, {'title': 'The time course of lexical access', 'author': ['Willem JM Levelt', 'Herbert Schriefers', 'Dirk Vorberg', 'Antje S Meyer', 'Thomas Pechmann', 'Jaap Havinga'], 'venue': None, 'citeRegEx': 'Levelt et al\\.,? \\Q1991\\E', 'shortCiteRegEx': 'Levelt et al\\.', 'year': 1991}, {'title': 'Generation and comprehension of unambiguous object descriptions', 'author': ['Junhua Mao', 'Jonathan Huang', 'Alexander Toshev', 'Oana Camburu', 'Alan L. Yuille', 'Kevin Murphy.'], 'venue': 'ArXiv / CoRR abs/1511.02283. http://arxiv.org/abs/1511.02283.', 'citeRegEx': 'Mao et al\\.,? 2015', 'shortCiteRegEx': 'Mao et al\\.', 'year': 2015}, {'title': 'Distributed representations of words and phrases and their compositionality', 'author': ['Tomas Mikolov', 'Ilya Sutskever', 'Kai Chen', 'Greg S Corrado', 'Jeff Dean.'], 'venue': 'C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-', 'citeRegEx': 'Mikolov et al\\.,? 2013', 'shortCiteRegEx': 'Mikolov et al\\.', 'year': 2013}, {'title': 'Zero-shot learning by convex combination of semantic embeddings', 'author': ['Mohammad Norouzi', 'Tomas Mikolov', 'Samy Bengio', 'Yoram Singer', 'Jonathon Shlens', 'Andrea Frome', 'Greg S Corrado', 'Jeffrey Dean'], 'venue': None, 'citeRegEx': 'Norouzi et al\\.,? \\Q2013\\E', 'shortCiteRegEx': 'Norouzi et al\\.', 'year': 2013}, {'title': 'Learning to name objects', 'author': ['Vicente Ordonez', 'Wei Liu', 'Jia Deng', 'Yejin Choi', 'Alexander C. Berg', 'Tamara L. Berg.'], 'venue': 'Commun. ACM 59(3):108–115.', 'citeRegEx': 'Ordonez et al\\.,? 2016', 'shortCiteRegEx': 'Ordonez et al\\.', 'year': 2016}, {'title': 'Principles of Categorization', 'author': ['Eleanor Rosch.'], 'venue': 'Eleanor Rosch and Barbara B. Lloyd, editors, Cognition and Categorization, Lawrence Erlbaum, Hillsdale, N.J., USA, pages 27—-48.', 'citeRegEx': 'Rosch.,? 1978', 'shortCiteRegEx': 'Rosch.', 'year': 1978}, {'title': 'Grounding words in perception and action: Computational insights', 'author': ['Deb Roy.'], 'venue': 'Trends in Cognitive Sciene 9(8):389–396.', 'citeRegEx': 'Roy.,? 2005', 'shortCiteRegEx': 'Roy.', 'year': 2005}, {'title': 'A trainable spoken language understanding system for visual object selection', 'author': ['Deb Roy', 'Peter Gorniak', 'Niloy Mukherjee', 'Josh Juster.'], 'venue': 'Proceedings of the International Conference on Speech and Language Processing 2002 (ICSLP 2002). Col-', 'citeRegEx': 'Roy et al\\.,? 2002', 'shortCiteRegEx': 'Roy et al\\.', 'year': 2002}, {'title': 'Learning visually-grounded words and syntax for a scene description task', 'author': ['Deb K. Roy.'], 'venue': 'Computer Speech and Language 16(3).', 'citeRegEx': 'Roy.,? 2002', 'shortCiteRegEx': 'Roy.', 'year': 2002}, {'title': 'Resolving references to objects in photographs using the words-as-classifiers model', 'author': ['David Schlangen', 'Sina Zarriess', 'Casey Kennington.'], 'venue': 'Proceedings of the 54rd Annual Meeting of the Association for Computational Linguistics (ACL 2016).', 'citeRegEx': 'Schlangen et al\\.,? 2016', 'shortCiteRegEx': 'Schlangen et al\\.', 'year': 2016}, {'title': 'Learning grounded meaning representations with autoencoders', 'author': ['Carina Silberer', 'Mirella Lapata.'], 'venue': 'Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-', 'citeRegEx': 'Silberer and Lapata.,? 2014', 'shortCiteRegEx': 'Silberer and Lapata.', 'year': 2014}, {'title': 'Zero-shot learning through cross-modal transfer', 'author': ['Richard Socher', 'Milind Ganjoo', 'Christopher D Manning', 'Andrew Ng.'], 'venue': 'Advances in neural information processing systems. pages 935–943.', 'citeRegEx': 'Socher et al\\.,? 2013', 'shortCiteRegEx': 'Socher et al\\.', 'year': 2013}, {'title': 'Going deeper with convolutions', 'author': ['Christian Szegedy', 'Wei Liu', 'Yangqing Jia', 'Pierre Sermanet', 'Scott Reed', 'Dragomir Anguelov', 'Dumitru Erhan', 'Vincent Vanhoucke', 'Andrew Rabinovich.'], 'venue': 'CVPR 2015. Boston, MA, USA.', 'citeRegEx': 'Szegedy et al\\.,? 2015', 'shortCiteRegEx': 'Szegedy et al\\.', 'year': 2015}, {'title': 'From frequency to meaning: Vector space models of semantics', 'author': ['Peter D Turney', 'Patrick Pantel.'], 'venue': 'Journal of artificial intelligence research 37(1):141–188.', 'citeRegEx': 'Turney and Pantel.,? 2010', 'shortCiteRegEx': 'Turney and Pantel.', 'year': 2010}, {'title': 'Easy things first: Installments improve referring expression generation for objects in photographs', 'author': ['Sina Zarrieß', 'David Schlangen.'], 'venue': 'Proceedings of ACL 2016. Berlin, Germany.', 'citeRegEx': 'Zarrieß and Schlangen.,? 2016', 'shortCiteRegEx': 'Zarrieß and Schlangen.', 'year': 2016}]  \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [{'title': 'Modern information retrieval, volume 463', 'author': ['Ricardo Baeza-Yates', 'Berthier Ribeiro-Neto'], 'venue': None, 'citeRegEx': 'Baeza.Yates and Ribeiro.Neto,? \\Q1999\\E', 'shortCiteRegEx': 'Baeza.Yates and Ribeiro.Neto', 'year': 1999}, {'title': 'Empirical evaluation of gated recurrent neural networks on sequence modeling', 'author': ['Junyoung Chung', 'Caglar Gulcehre', 'KyungHyun Cho', 'Yoshua Bengio.'], 'venue': 'arXiv preprint arXiv:1412.3555 .', 'citeRegEx': 'Chung et al\\.,? 2014', 'shortCiteRegEx': 'Chung et al\\.', 'year': 2014}, {'title': 'Measuring nominal scale agreement among many raters', 'author': ['Joseph L Fleiss.'], 'venue': 'Psychological bulletin 76(5):378.', 'citeRegEx': 'Fleiss.,? 1971', 'shortCiteRegEx': 'Fleiss.', 'year': 1971}, {'title': 'Towards an open-domain conversational system fully based on natural language', 'author': ['Ryuichiro Higashinaka', 'Kenji Imamura', 'Toyomi Meguro', 'Chiaki Miyazaki', 'Nozomi Kobayashi', 'Hiroaki Sugiyama', 'Toru Hirano', 'Toshiro Makino', 'Yoshihiro Matsuo'], 'venue': None, 'citeRegEx': 'Higashinaka et al\\.,? \\Q2014\\E', 'shortCiteRegEx': 'Higashinaka et al\\.', 'year': 2014}, {'title': 'Convolutional neural network architectures for matching natural language sentences', 'author': ['Baotian Hu', 'Zhengdong Lu', 'Hang Li', 'Qingcai Chen.'], 'venue': 'Advances in Neural Information Processing Systems. pages 2042–2050.', 'citeRegEx': 'Hu et al\\.,? 2014', 'shortCiteRegEx': 'Hu et al\\.', 'year': 2014}, {'title': 'An information retrieval approach to short text conversation', 'author': ['Zongcheng Ji', 'Zhengdong Lu', 'Hang Li.'], 'venue': 'arXiv preprint arXiv:1408.6988 .', 'citeRegEx': 'Ji et al\\.,? 2014', 'shortCiteRegEx': 'Ji et al\\.', 'year': 2014}, {'title': 'Improved deep learning baselines for ubuntu corpus dialogs', 'author': ['Rudolf Kadlec', 'Martin Schmid', 'Jan Kleindienst.'], 'venue': 'arXiv preprint arXiv:1510.03753 .', 'citeRegEx': 'Kadlec et al\\.,? 2015', 'shortCiteRegEx': 'Kadlec et al\\.', 'year': 2015}, {'title': 'Adam: A method for stochastic optimization', 'author': ['Diederik Kingma', 'Jimmy Ba.'], 'venue': 'arXiv preprint arXiv:1412.6980 .', 'citeRegEx': 'Kingma and Ba.,? 2014', 'shortCiteRegEx': 'Kingma and Ba.', 'year': 2014}, {'title': 'A diversity-promoting objective function for neural conversation models', 'author': ['Jiwei Li', 'Michel Galley', 'Chris Brockett', 'Jianfeng Gao', 'Bill Dolan.'], 'venue': 'arXiv preprint arXiv:1510.03055 .', 'citeRegEx': 'Li et al\\.,? 2015', 'shortCiteRegEx': 'Li et al\\.', 'year': 2015}, {'title': 'A persona-based neural conversation model', 'author': ['Jiwei Li', 'Michel Galley', 'Chris Brockett', 'Jianfeng Gao', 'Bill Dolan.'], 'venue': 'arXiv preprint arXiv:1603.06155 .', 'citeRegEx': 'Li et al\\.,? 2016', 'shortCiteRegEx': 'Li et al\\.', 'year': 2016}, {'title': 'The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems', 'author': ['Ryan Lowe', 'Nissan Pow', 'Iulian Serban', 'Joelle Pineau.'], 'venue': 'arXiv preprint arXiv:1506.08909 .', 'citeRegEx': 'Lowe et al\\.,? 2015', 'shortCiteRegEx': 'Lowe et al\\.', 'year': 2015}, {'title': 'Distributed representations of words and phrases and their compositionality', 'author': ['Tomas Mikolov', 'Ilya Sutskever', 'Kai Chen', 'Greg S Corrado', 'Jeff Dean.'], 'venue': 'Advances in neural information processing systems. pages 3111–3119.', 'citeRegEx': 'Mikolov et al\\.,? 2013', 'shortCiteRegEx': 'Mikolov et al\\.', 'year': 2013}, {'title': 'Data-driven response generation in social media', 'author': ['Alan Ritter', 'Colin Cherry', 'William B Dolan.'], 'venue': 'Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 583–593.', 'citeRegEx': 'Ritter et al\\.,? 2011', 'shortCiteRegEx': 'Ritter et al\\.', 'year': 2011}, {'title': 'Building end-to-end dialogue systems using generative hierarchical neural network models', 'author': ['Iulian V Serban', 'Alessandro Sordoni', 'Yoshua Bengio', 'Aaron Courville', 'Joelle Pineau.'], 'venue': 'arXiv preprint arXiv:1507.04808 .', 'citeRegEx': 'Serban et al\\.,? 2015', 'shortCiteRegEx': 'Serban et al\\.', 'year': 2015}, {'title': 'Multiresolution recurrent neural networks: An application to dialogue response generation', 'author': ['Iulian Vlad Serban', 'Tim Klinger', 'Gerald Tesauro', 'Kartik Talamadupula', 'Bowen Zhou', 'Yoshua Bengio', 'Aaron Courville.'], 'venue': 'arXiv preprint', 'citeRegEx': 'Serban et al\\.,? 2016a', 'shortCiteRegEx': 'Serban et al\\.', 'year': 2016}, {'title': 'A hierarchical latent variable encoder-decoder model for generating dialogues', 'author': ['Iulian Vlad Serban', 'Alessandro Sordoni', 'Ryan Lowe', 'Laurent Charlin', 'Joelle Pineau', 'Aaron Courville', 'Yoshua Bengio.'], 'venue': 'arXiv preprint arXiv:1605.06069 .', 'citeRegEx': 'Serban et al\\.,? 2016b', 'shortCiteRegEx': 'Serban et al\\.', 'year': 2016}, {'title': 'Neural responding machine for short-text conversation', 'author': ['Lifeng Shang', 'Zhengdong Lu', 'Hang Li.'], 'venue': 'ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers. pages 1577–1586.', 'citeRegEx': 'Shang et al\\.,? 2015', 'shortCiteRegEx': 'Shang et al\\.', 'year': 2015}, {'title': 'Reasoning with neural tensor networks for knowledge base completion', 'author': ['Richard Socher', 'Danqi Chen', 'Christopher D Manning', 'Andrew Ng.'], 'venue': 'Advances in Neural Information Processing Systems. pages 926–934.', 'citeRegEx': 'Socher et al\\.,? 2013', 'shortCiteRegEx': 'Socher et al\\.', 'year': 2013}, {'title': 'Lstmbased deep learning models for non-factoid answer selection', 'author': ['Ming Tan', 'Bing Xiang', 'Bowen Zhou.'], 'venue': 'arXiv preprint arXiv:1511.04108 .', 'citeRegEx': 'Tan et al\\.,? 2015', 'shortCiteRegEx': 'Tan et al\\.', 'year': 2015}, {'title': 'Theano: A Python framework for fast computation of mathematical expressions', 'author': ['Theano Development Team.'], 'venue': 'arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.', 'citeRegEx': 'Team.,? 2016', 'shortCiteRegEx': 'Team.', 'year': 2016}, {'title': 'A neural conversational model', 'author': ['Oriol Vinyals', 'Quoc Le.'], 'venue': 'arXiv preprint arXiv:1506.05869 .', 'citeRegEx': 'Vinyals and Le.,? 2015', 'shortCiteRegEx': 'Vinyals and Le.', 'year': 2015}, {'title': 'The trec-8 question answering track report', 'author': ['Ellen M Voorhees'], 'venue': 'Trec. volume 99, pages 77–', 'citeRegEx': 'Voorhees,? 1999', 'shortCiteRegEx': 'Voorhees', 'year': 1999}, {'title': 'Match-srnn: Modeling the recursive matching structure with spatial rnn', 'author': ['Shengxian Wan', 'Yanyan Lan', 'Jun Xu', 'Jiafeng Guo', 'Liang Pang', 'Xueqi Cheng.'], 'venue': 'arXiv preprint arXiv:1604.04378 .', 'citeRegEx': 'Wan et al\\.,? 2016', 'shortCiteRegEx': 'Wan et al\\.', 'year': 2016}, {'title': 'A dataset for research on short-text conversations', 'author': ['Hao Wang', 'Zhengdong Lu', 'Hang Li', 'Enhong Chen.'], 'venue': 'EMNLP. pages 935–945.', 'citeRegEx': 'Wang et al\\.,? 2013', 'shortCiteRegEx': 'Wang et al\\.', 'year': 2013}, {'title': 'Syntax-based deep matching of short texts', 'author': ['Mingxuan Wang', 'Zhengdong Lu', 'Hang Li', 'Qun Liu.'], 'venue': 'arXiv preprint arXiv:1503.02427 .', 'citeRegEx': 'Wang et al\\.,? 2015', 'shortCiteRegEx': 'Wang et al\\.', 'year': 2015}, {'title': 'Learning natural language inference with lstm', 'author': ['Shuohang Wang', 'Jing Jiang.'], 'venue': 'arXiv preprint arXiv:1512.08849 .', 'citeRegEx': 'Wang and Jiang.,? 2015', 'shortCiteRegEx': 'Wang and Jiang.', 'year': 2015}, {'title': 'Ranking responses oriented to conversational relevance in chat-bots', 'author': ['Bowen Wu', 'Baoxun Wang', 'Hui Xue.'], 'venue': 'COLING16 .', 'citeRegEx': 'Wu et al\\.,? 2016a', 'shortCiteRegEx': 'Wu et al\\.', 'year': 2016}, {'title': 'Topic augmented neural network for short text conversation', 'author': ['Yu Wu', 'Wei Wu', 'Zhoujun Li', 'Ming Zhou.'], 'venue': 'CoRR abs/1605.00090. http://arxiv.org/abs/1605.00090.', 'citeRegEx': 'Wu et al\\.,? 2016b', 'shortCiteRegEx': 'Wu et al\\.', 'year': 2016}, {'title': 'Topic augmented neural response generation with a joint attention mechanism', 'author': ['Chen Xing', 'Wei Wu', 'Yu Wu', 'Jie Liu', 'Yalou Huang', 'Ming Zhou', 'Wei-Ying Ma.'], 'venue': 'arXiv preprint arXiv:1606.08340 .', 'citeRegEx': 'Xing et al\\.,? 2016', 'shortCiteRegEx': 'Xing et al\\.', 'year': 2016}, {'title': 'Incorporating loose-structured knowledge into lstm with recall gate for conversation modeling', 'author': ['Zhen Xu', 'Bingquan Liu', 'Baoxun Wang', 'Chengjie Sun', 'Xiaolong Wang.'], 'venue': 'arXiv preprint arXiv:1605.05110 .', 'citeRegEx': 'Xu et al\\.,? 2016', 'shortCiteRegEx': 'Xu et al\\.', 'year': 2016}, {'title': 'Learning to respond with deep neural networks for retrieval-based human-computer conversation system', 'author': ['Rui Yan', 'Yiping Song', 'Hua Wu.'], 'venue': 'Proceedings of the 39th International ACM SIGIR conference on Research and De-', 'citeRegEx': 'Yan et al\\.,? 2016', 'shortCiteRegEx': 'Yan et al\\.', 'year': 2016}, {'title': 'Hierarchical attention networks for document classification', 'author': ['Zichao Yang', 'Diyi Yang', 'Chris Dyer', 'Xiaodong He', 'Alex Smola', 'Eduard Hovy.'], 'venue': 'Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-', 'citeRegEx': 'Yang et al\\.,? 2016', 'shortCiteRegEx': 'Yang et al\\.', 'year': 2016}, {'title': 'The hidden information state model: A practical framework for pomdp-based spoken dialogue management', 'author': ['Steve Young', 'Milica Gašić', 'Simon Keizer', 'François Mairesse', 'Jost Schatzmann', 'Blaise Thomson', 'Kai Yu.'], 'venue': 'Computer Speech & Language', 'citeRegEx': 'Young et al\\.,? 2010', 'shortCiteRegEx': 'Young et al\\.', 'year': 2010}, {'title': 'Multiview response selection for human-computer conversation', 'author': ['Xiangyang Zhou', 'Daxiang Dong', 'Hua Wu', 'Shiqi Zhao', 'R Yan', 'D Yu', 'Xuan Liu', 'H Tian.'], 'venue': 'EMNLP16 .', 'citeRegEx': 'Zhou et al\\.,? 2016', 'shortCiteRegEx': 'Zhou et al\\.', 'year': 2016}]  \n",
            "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ...  \n",
            "132  [{'title': 'The Uralic Languages', 'author': ['Daniel Abondolo.'], 'venue': 'Routledge.', 'citeRegEx': 'Abondolo.,? 2015', 'shortCiteRegEx': 'Abondolo.', 'year': 2015}, {'title': 'Improving sequence to sequence learning for morphological inflection generation: The BIU-MIT systems for the SIGMORPHON 2016 shared task for morphological reinflection', 'author': ['Roee Aharoni', 'Yoav Goldberg', 'Yonatan Belinkov.'], 'venue': 'SIGMORPHON.', 'citeRegEx': 'Aharoni et al\\.,? 2016', 'shortCiteRegEx': 'Aharoni et al\\.', 'year': 2016}, {'title': 'Semi-supervised learning of morphological paradigms and lexicons', 'author': ['Malin Ahlberg', 'Markus Forsberg', 'Mans Hulden.'], 'venue': 'EACL.', 'citeRegEx': 'Ahlberg et al\\.,? 2014', 'shortCiteRegEx': 'Ahlberg et al\\.', 'year': 2014}, {'title': 'Neural machine translation by jointly learning to align and translate', 'author': ['Dzmitry Bahdanau', 'Kyunghyun Cho', 'Yoshua Bengio.'], 'venue': 'ICLR.', 'citeRegEx': 'Bahdanau et al\\.,? 2015', 'shortCiteRegEx': 'Bahdanau et al\\.', 'year': 2015}, {'title': 'Cross-lingual morphological tagging for low-resource languages', 'author': ['Jan Buys', 'Jan A Botha.'], 'venue': 'ACL.', 'citeRegEx': 'Buys and Botha.,? 2016', 'shortCiteRegEx': 'Buys and Botha.', 'year': 2016}, {'title': 'Multitask learning', 'author': ['Rich Caruana.'], 'venue': 'Machine Learning .', 'citeRegEx': 'Caruana.,? 1997', 'shortCiteRegEx': 'Caruana.', 'year': 1997}, {'title': 'On the properties of neural machine translation: Encoder-decoder approaches', 'author': ['Kyunghyun Cho', 'Bart Van Merriënboer', 'Dzmitry Bahdanau', 'Yoshua Bengio.'], 'venue': 'arXiv preprint arXiv:1409.1259 .', 'citeRegEx': 'Cho et al\\.,? 2014', 'shortCiteRegEx': 'Cho et al\\.', 'year': 2014}, {'title': 'An empirical comparison of simple domain adaptation methods for neural machine translation', 'author': ['Chenhui Chu', 'Raj Dabre', 'Sadao Kurohashi.'], 'venue': 'arXiv preprint 1701.03214 .', 'citeRegEx': 'Chu et al\\.,? 2017', 'shortCiteRegEx': 'Chu et al\\.', 'year': 2017}, {'title': 'Unsupervised structure prediction with non-parallel multilingual guidance', 'author': ['Shay B Cohen', 'Dipanjan Das', 'Noah A Smith.'], 'venue': 'EMNLP.', 'citeRegEx': 'Cohen et al\\.,? 2011', 'shortCiteRegEx': 'Cohen et al\\.', 'year': 2011}, {'title': 'Natural language processing (almost) from scratch', 'author': ['Ronan Collobert', 'Jason Weston', 'Léon Bottou', 'Michael Karlen', 'Koray Kavukcuoglu', 'Pavel Kuksa.'], 'venue': 'JMLR 12(Aug):2493–2537.', 'citeRegEx': 'Collobert et al\\.,? 2011', 'shortCiteRegEx': 'Collobert et al\\.', 'year': 2011}, {'title': 'The Slavonic Languages', 'author': ['Greville Corbett', 'Bernard Comrie.'], 'venue': 'Routledge.', 'citeRegEx': 'Corbett and Comrie.,? 2003', 'shortCiteRegEx': 'Corbett and Comrie.', 'year': 2003}, {'title': 'The SIGMORPHON 2016 shared task— morphological reinflection', 'author': ['Ryan Cotterell', 'Christo Kirov', 'John Sylak-Glassman', 'David Yarowsky', 'Jason Eisner', 'Mans Hulden.'], 'venue': 'SIGMORPHON.', 'citeRegEx': 'Cotterell et al\\.,? 2016a', 'shortCiteRegEx': 'Cotterell et al\\.', 'year': 2016}, {'title': 'Modeling word forms using latent underlying morphs and phonology', 'author': ['Ryan Cotterell', 'Nanyun Peng', 'Jason Eisner.'], 'venue': 'TACL 3:433–447.', 'citeRegEx': 'Cotterell et al\\.,? 2015', 'shortCiteRegEx': 'Cotterell et al\\.', 'year': 2015}, {'title': 'Morphological smoothing and extrapolation of word embeddings', 'author': ['Ryan Cotterell', 'Hinrich Schütze', 'Jason Eisner.'], 'venue': 'ACL.', 'citeRegEx': 'Cotterell et al\\.,? 2016b', 'shortCiteRegEx': 'Cotterell et al\\.', 'year': 2016}, {'title': 'Analysis of morph-based speech recognition and the modeling of out-of', 'author': ['Mathias Creutz', 'Teemu Hirsimäki', 'Mikko Kurimo', 'Antti Puurula', 'Janne Pylkkönen', 'Vesa Siivola', 'Matti Varjokallio', 'Ebru Arisoy', 'Murat Saraçlar', 'Andreas Stolcke'], 'venue': None, 'citeRegEx': 'Creutz et al\\.,? \\Q2007\\E', 'shortCiteRegEx': 'Creutz et al\\.', 'year': 2007}, {'title': 'Multi-task learning for multiple language translation', 'author': ['Daxiang Dong', 'Hua Wu', 'Wei He', 'Dianhai Yu', 'Haifeng Wang.'], 'venue': 'ACL-IJCNLP.', 'citeRegEx': 'Dong et al\\.,? 2015', 'shortCiteRegEx': 'Dong et al\\.', 'year': 2015}, {'title': 'Supervised learning of complete morphological paradigms', 'author': ['Greg Durrett', 'John DeNero.'], 'venue': 'NAACL.', 'citeRegEx': 'Durrett and DeNero.,? 2013', 'shortCiteRegEx': 'Durrett and DeNero.', 'year': 2013}, {'title': 'Generalizing word lattice translation', 'author': ['Christopher Dyer', 'Smaranda Muresan', 'Philip Resnik.'], 'venue': 'ACL.', 'citeRegEx': 'Dyer et al\\.,? 2008', 'shortCiteRegEx': 'Dyer et al\\.', 'year': 2008}, {'title': 'Morphological inflection generation using character sequence to sequence learning', 'author': ['Manaal Faruqui', 'Yulia Tsvetkov', 'Graham Neubig', 'Chris Dyer.'], 'venue': 'arXiv preprint arXiv:1512.06110 .', 'citeRegEx': 'Faruqui et al\\.,? 2015', 'shortCiteRegEx': 'Faruqui et al\\.', 'year': 2015}, {'title': 'Morphological inflection generation using character sequence to sequence learning', 'author': ['Manaal Faruqui', 'Yulia Tsvetkov', 'Graham Neubig', 'Chris Dyer.'], 'venue': 'NAACL.', 'citeRegEx': 'Faruqui et al\\.,? 2016', 'shortCiteRegEx': 'Faruqui et al\\.', 'year': 2016}, {'title': 'Multi-way, multilingual neural machine translation with a shared attention mechanism', 'author': ['Orhan Firat', 'KyungHyun Cho', 'Yoshua Bengio.'], 'venue': 'CoRR abs/1601.01073.', 'citeRegEx': 'Firat et al\\.,? 2016', 'shortCiteRegEx': 'Firat et al\\.', 'year': 2016}, {'title': 'The use of machine translation tools for cross-lingual text mining', 'author': ['Blaz Fortuna', 'John Shawe-Taylor.'], 'venue': 'ICML Workshop on Learning with Multiple Views.', 'citeRegEx': 'Fortuna and Shawe.Taylor.,? 2005', 'shortCiteRegEx': 'Fortuna and Shawe.Taylor.', 'year': 2005}, {'title': 'Speech recognition with deep recurrent neural networks', 'author': ['Alex Graves', 'Abdel-rahman Mohamed', 'Geoffrey Hinton.'], 'venue': 'IEEE.', 'citeRegEx': 'Graves et al\\.,? 2013', 'shortCiteRegEx': 'Graves et al\\.', 'year': 2013}, {'title': 'Framewise phoneme classification with bidirectional lstm and other neural network architectures', 'author': ['Alex Graves', 'Jürgen Schmidhuber.'], 'venue': 'Neural Networks 18(5):602–610.', 'citeRegEx': 'Graves and Schmidhuber.,? 2005', 'shortCiteRegEx': 'Graves and Schmidhuber.', 'year': 2005}, {'title': 'Toward multilingual neural machine translation with universal encoder and decoder', 'author': ['Thanh-Le Ha', 'Jan Niehues', 'Alexander Waibel.'], 'venue': 'arXiv preprint arXiv:1611.04798 .', 'citeRegEx': 'Ha et al\\.,? 2016', 'shortCiteRegEx': 'Ha et al\\.', 'year': 2016}, {'title': 'The Romance languages', 'author': ['Martin Harris', 'Nigel Vincent.'], 'venue': 'Routledge.', 'citeRegEx': 'Harris and Vincent.,? 2003', 'shortCiteRegEx': 'Harris and Vincent.', 'year': 2003}, {'title': 'The Semitic Languages', 'author': ['Robert Hetzron.'], 'venue': 'Routledge.', 'citeRegEx': 'Hetzron.,? 2013', 'shortCiteRegEx': 'Hetzron.', 'year': 2013}, {'title': 'Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers', 'author': ['Jui-Ting Huang', 'Jinyu Li', 'Dong Yu', 'Li Deng', 'n Gong'], 'venue': None, 'citeRegEx': 'Huang et al\\.,? \\Q2013\\E', 'shortCiteRegEx': 'Huang et al\\.', 'year': 2013}, {'title': 'Neural morphological analysis: Encodingdecoding canonical segments', 'author': ['Katharina Kann', 'Ryan Cotterell', 'Hinrich Schütze.'], 'venue': 'EMNLP.', 'citeRegEx': 'Kann et al\\.,? 2016', 'shortCiteRegEx': 'Kann et al\\.', 'year': 2016}, {'title': 'Singlemodel encoder-decoder with explicit morphological representation for reinflection', 'author': ['Katharina Kann', 'Hinrich Schütze.'], 'venue': 'ACL.', 'citeRegEx': 'Kann and Schütze.,? 2016a', 'shortCiteRegEx': 'Kann and Schütze.', 'year': 2016}, {'title': 'MED: The LMU system for the SIGMORPHON 2016 shared task on morphological reinflection', 'author': ['Katharina Kann', 'Hinrich Schütze.'], 'venue': 'ACL.', 'citeRegEx': 'Kann and Schütze.,? 2016b', 'shortCiteRegEx': 'Kann and Schütze.', 'year': 2016}, {'title': 'Very-large scale parsing and normalization of wiktionary morphological paradigms', 'author': ['Christo Kirov', 'John Sylak-Glassman', 'Roger Que', 'David Yarowsky.'], 'venue': 'LREC.', 'citeRegEx': 'Kirov et al\\.,? 2016', 'shortCiteRegEx': 'Kirov et al\\.', 'year': 2016}, {'title': 'A simple way to initialize recurrent networks of rectified linear units', 'author': ['Quoc V Le', 'Navdeep Jaitly', 'Geoffrey E Hinton.'], 'venue': 'CoRR abs/1504.00941.', 'citeRegEx': 'Le et al\\.,? 2015', 'shortCiteRegEx': 'Le et al\\.', 'year': 2015}, {'title': 'Ethnologue: Languages of the World. SIL International, Dallas, Texas, 16 edition', 'author': ['M Paul Lewis', 'editor'], 'venue': None, 'citeRegEx': 'Lewis and editor.,? \\Q2009\\E', 'shortCiteRegEx': 'Lewis and editor.', 'year': 2009}, {'title': 'Multi-task sequence to sequence learning', 'author': ['Minh-Thang Luong', 'Quoc V Le', 'Ilya Sutskever', 'Oriol Vinyals', 'Lukasz Kaiser.'], 'venue': 'ICLR.', 'citeRegEx': 'Luong et al\\.,? 2016', 'shortCiteRegEx': 'Luong et al\\.', 'year': 2016}, {'title': 'Morphological segmentation for keyword spotting', 'author': ['Karthik Narasimhan', 'Damianos Karakos', 'Richard Schwartz', 'Stavros Tsakalidis', 'Regina Barzilay.'], 'venue': 'EMNLP.', 'citeRegEx': 'Narasimhan et al\\.,? 2014', 'shortCiteRegEx': 'Narasimhan et al\\.', 'year': 2014}, {'title': 'Selective sharing for multilingual dependency parsing', 'author': ['Tahira Naseem', 'Regina Barzilay', 'Amir Globerson.'], 'venue': 'ACL.', 'citeRegEx': 'Naseem et al\\.,? 2012', 'shortCiteRegEx': 'Naseem et al\\.', 'year': 2012}, {'title': 'Inflection generation as discriminative string transduction', 'author': ['Garrett Nicolai', 'Colin Cherry', 'Grzegorz Kondrak.'], 'venue': 'NAACL.', 'citeRegEx': 'Nicolai et al\\.,? 2015', 'shortCiteRegEx': 'Nicolai et al\\.', 'year': 2015}, {'title': 'Cross-language text classification', 'author': ['J Scott Olsson', 'Douglas W Oard', 'Jan Hajič.'], 'venue': 'ACM SIGIR conference on Research and development in information retrieval.', 'citeRegEx': 'Olsson et al\\.,? 2005', 'shortCiteRegEx': 'Olsson et al\\.', 'year': 2005}, {'title': 'Crosslinguistic projection of role-semantic information', 'author': ['Sebastian Padó', 'Mirella Lapata.'], 'venue': 'HLT/EMNLP.', 'citeRegEx': 'Padó and Lapata.,? 2005', 'shortCiteRegEx': 'Padó and Lapata.', 'year': 2005}, {'title': 'A universal part-of-speech tagset', 'author': ['Slav Petrov', 'Dipanjan Das', 'Ryan McDonald.'], 'venue': 'LREC.', 'citeRegEx': 'Petrov et al\\.,? 2012', 'shortCiteRegEx': 'Petrov et al\\.', 'year': 2012}, {'title': 'Semimarkov conditional random fields for information extraction', 'author': ['Sunita Sarawagi', 'William W Cohen.'], 'venue': 'NIPS.', 'citeRegEx': 'Sarawagi and Cohen.,? 2004', 'shortCiteRegEx': 'Sarawagi and Cohen.', 'year': 2004}, {'title': 'A graphbased lattice dependency parser for joint morphological segmentation and syntactic analysis', 'author': ['Wolfgang Seeker', 'Özlem Çetinoğlu.'], 'venue': 'TACL 3:359–373.', 'citeRegEx': 'Seeker and Çetinoğlu.,? 2015', 'shortCiteRegEx': 'Seeker and Çetinoğlu.', 'year': 2015}, {'title': 'Cross language text classification by model translation and semi-supervised learning', 'author': ['Lei Shi', 'Rada Mihalcea', 'Mingjun Tian.'], 'venue': 'EMNLP.', 'citeRegEx': 'Shi et al\\.,? 2010', 'shortCiteRegEx': 'Shi et al\\.', 'year': 2010}, {'title': 'Crosslingual propagation for morphological analysis', 'author': ['Benjamin Snyder', 'Regina Barzilay.'], 'venue': 'AAAI.', 'citeRegEx': 'Snyder and Barzilay.,? 2008a', 'shortCiteRegEx': 'Snyder and Barzilay.', 'year': 2008}, {'title': 'Unsupervised multilingual learning for morphological segmentation', 'author': ['Benjamin Snyder', 'Regina Barzilay.'], 'venue': 'ACL-HLT .', 'citeRegEx': 'Snyder and Barzilay.,? 2008b', 'shortCiteRegEx': 'Snyder and Barzilay.', 'year': 2008}, {'title': 'Data point selection for crosslanguage adaptation of dependency parsers', 'author': ['Anders Søgaard.'], 'venue': 'ACLHLT .', 'citeRegEx': 'Søgaard.,? 2011', 'shortCiteRegEx': 'Søgaard.', 'year': 2011}, {'title': 'Dropout: a simple way to prevent neural networks from overfitting', 'author': ['Nitish Srivastava', 'Geoffrey E Hinton', 'Alex Krizhevsky', 'Ilya Sutskever', 'Ruslan Salakhutdinov.'], 'venue': 'Journal of Machine Learning Research 15(1):1929–1958.', 'citeRegEx': 'Srivastava et al\\.,? 2014', 'shortCiteRegEx': 'Srivastava et al\\.', 'year': 2014}, {'title': 'Sequence to sequence learning with neural networks', 'author': ['Ilya Sutskever', 'Oriol Vinyals', 'Quoc V Le.'], 'venue': 'NIPS.', 'citeRegEx': 'Sutskever et al\\.,? 2014', 'shortCiteRegEx': 'Sutskever et al\\.', 'year': 2014}, {'title': 'The composition and use of the universal morphological feature schema (unimorph schema)', 'author': ['John Sylak-Glassman.'], 'venue': 'Technical report, Department of Computer Science, Johns Hopkins University.', 'citeRegEx': 'Sylak.Glassman.,? 2016', 'shortCiteRegEx': 'Sylak.Glassman.', 'year': 2016}, {'title': 'A language-independent feature schema for inflectional morphology', 'author': ['John Sylak-Glassman', 'Christo Kirov', 'David Yarowsky', 'Roger Que.'], 'venue': 'ACLIJCNLP.', 'citeRegEx': 'Sylak.Glassman et al\\.,? 2015', 'shortCiteRegEx': 'Sylak.Glassman et al\\.', 'year': 2015}, {'title': 'Grammar as a foreign language', 'author': ['Oriol Vinyals', 'Łukasz Kaiser', 'Terry Koo', 'Slav Petrov', 'Ilya Sutskever', 'Geoffrey Hinton.'], 'venue': 'NIPS.', 'citeRegEx': 'Vinyals et al\\.,? 2015', 'shortCiteRegEx': 'Vinyals et al\\.', 'year': 2015}, {'title': 'Cross-lingual projected expectation regularization for weakly supervised learning', 'author': ['Mengqiu Wang', 'Christopher D Manning.'], 'venue': 'TACL 2:55–66.', 'citeRegEx': 'Wang and Manning.,? 2014a', 'shortCiteRegEx': 'Wang and Manning.', 'year': 2014}, {'title': 'Cross-lingual pseudo-projected expectation regularization for weakly supervised learning', 'author': ['Mengqiu Wang', 'Christopher D Manning.'], 'venue': 'TACL 2:55–', 'citeRegEx': 'Wang and Manning.,? 2014b', 'shortCiteRegEx': 'Wang and Manning.', 'year': 2014}, {'title': 'Corpus-level fine-grained entity typing using contextual information', 'author': ['Yadollah Yaghoobzadeh', 'Hinrich Schütze.'], 'venue': 'EMNLP.', 'citeRegEx': 'Yaghoobzadeh and Schütze.,? 2015', 'shortCiteRegEx': 'Yaghoobzadeh and Schütze.', 'year': 2015}, {'title': 'Inducing multilingual text analysis tools via robust projection across aligned corpora', 'author': ['David Yarowsky', 'Grace Ngai', 'Richard Wicentowski.'], 'venue': 'HLT .', 'citeRegEx': 'Yarowsky et al\\.,? 2001', 'shortCiteRegEx': 'Yarowsky et al\\.', 'year': 2001}, {'title': 'ADADELTA: an adaptive learning rate method', 'author': ['Matthew D Zeiler.'], 'venue': 'CoRR abs/1212.5701.', 'citeRegEx': 'Zeiler.,? 2012', 'shortCiteRegEx': 'Zeiler.', 'year': 2012}, {'title': 'Multi-source neural translation', 'author': ['Barret Zoph', 'Kevin Knight.'], 'venue': 'arXiv preprint arXiv:1601.00710 .', 'citeRegEx': 'Zoph and Knight.,? 2016', 'shortCiteRegEx': 'Zoph and Knight.', 'year': 2016}]  \n",
            "133                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [{'title': 'Affective computing and sentiment analysis', 'author': ['Erik Cambria.'], 'venue': 'IEEE Intelligent Systems 31(2):102– 107.', 'citeRegEx': 'Cambria.,? 2016', 'shortCiteRegEx': 'Cambria.', 'year': 2016}, {'title': 'Multimodal human emotion/expression recognition', 'author': ['Lawrence S Chen', 'Thomas S Huang', 'Tsutomu Miyasato', 'Ryohei Nakatsu.'], 'venue': 'Proceedings of the Third IEEE International Conference on Automatic Face and Gesture Recognition. IEEE,', 'citeRegEx': 'Chen et al\\.,? 1998', 'shortCiteRegEx': 'Chen et al\\.', 'year': 1998}, {'title': 'Semantic audio-visual data fusion for automatic emotion recognition', 'author': ['Dragos Datcu', 'L Rothkrantz.'], 'venue': 'Euromedia’2008 .', 'citeRegEx': 'Datcu and Rothkrantz.,? 2008', 'shortCiteRegEx': 'Datcu and Rothkrantz.', 'year': 2008}, {'title': 'Facial emotion recognition using multi-modal information', 'author': ['Liyanage C De Silva', 'Tsutomu Miyasato', 'Ryohei Nakatsu.'], 'venue': 'Proceedings of ICICS. IEEE, volume 1, pages 397–401.', 'citeRegEx': 'Silva et al\\.,? 1997', 'shortCiteRegEx': 'Silva et al\\.', 'year': 1997}, {'title': 'Adaptive subgradient methods for online learning and stochastic optimization', 'author': ['John Duchi', 'Elad Hazan', 'Yoram Singer.'], 'venue': 'Journal of Machine Learning Research 12(Jul):2121–2159.', 'citeRegEx': 'Duchi et al\\.,? 2011', 'shortCiteRegEx': 'Duchi et al\\.', 'year': 2011}, {'title': 'Universal facial expressions of emotion', 'author': ['Paul Ekman.'], 'venue': 'Culture and Personality: Contemporary Readings/Chicago .', 'citeRegEx': 'Ekman.,? 1974', 'shortCiteRegEx': 'Ekman.', 'year': 1974}, {'title': 'On-line emotion recognition in a 3-d activation-valence-time continuum using acoustic and linguistic cues', 'author': ['Florian Eyben', 'Martin Wöllmer', 'Alex Graves', 'Björn Schuller', 'Ellen Douglas-Cowie', 'Roddy Cowie.'], 'venue': 'Journal on Multimodal User In-', 'citeRegEx': 'Eyben et al\\.,? 2010a', 'shortCiteRegEx': 'Eyben et al\\.', 'year': 2010}, {'title': 'Opensmile: the munich versatile and fast open-source audio feature extractor', 'author': ['Florian Eyben', 'Martin Wöllmer', 'Björn Schuller.'], 'venue': 'Proceedings of the 18th ACM international conference on Multimedia. ACM, pages 1459–1462.', 'citeRegEx': 'Eyben et al\\.,? 2010b', 'shortCiteRegEx': 'Eyben et al\\.', 'year': 2010}, {'title': 'Long Short-Term Memory in Recurrent Neural Networks', 'author': ['Felix Gers.'], 'venue': 'Ph.D. thesis, Universität Hannover.', 'citeRegEx': 'Gers.,? 2001', 'shortCiteRegEx': 'Gers.', 'year': 2001}, {'title': '3d convolutional neural networks for human action recognition', 'author': ['Shuiwang Ji', 'Wei Xu', 'Ming Yang', 'Kai Yu.'], 'venue': 'IEEE transactions on pattern analysis and machine intelligence 35(1):221–231.', 'citeRegEx': 'Ji et al\\.,? 2013', 'shortCiteRegEx': 'Ji et al\\.', 'year': 2013}, {'title': 'Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis', 'author': ['Loic Kessous', 'Ginevra Castellano', 'George Caridakis.'], 'venue': 'Journal on Multimodal User Interfaces 3(1-2):33–48.', 'citeRegEx': 'Kessous et al\\.,? 2010', 'shortCiteRegEx': 'Kessous et al\\.', 'year': 2010}, {'title': 'Audio-visual emotion recognition using gaussian mixture models for face and voice', 'author': ['Angeliki Metallinou', 'Sungbok Lee', 'Shrikanth Narayanan.'], 'venue': 'Tenth IEEE International Symposium on ISM 2008. IEEE, pages 250–257.', 'citeRegEx': 'Metallinou et al\\.,? 2008', 'shortCiteRegEx': 'Metallinou et al\\.', 'year': 2008}, {'title': 'Efficient estimation of word representations in vector space', 'author': ['Tomas Mikolov', 'Kai Chen', 'Greg Corrado', 'Jeffrey Dean.'], 'venue': 'arXiv preprint arXiv:1301.3781 .', 'citeRegEx': 'Mikolov et al\\.,? 2013', 'shortCiteRegEx': 'Mikolov et al\\.', 'year': 2013}, {'title': 'Thumbs up?: sentiment classification using machine learning techniques', 'author': ['Bo Pang', 'Lillian Lee', 'Shivakumar Vaithyanathan.'], 'venue': 'Proceedings of ACL. Association for Computational Linguistics, pages 79–86.', 'citeRegEx': 'Pang et al\\.,? 2002', 'shortCiteRegEx': 'Pang et al\\.', 'year': 2002}, {'title': 'Utterance-level multimodal sentiment analysis', 'author': ['Verónica Pérez-Rosas', 'Rada Mihalcea', 'LouisPhilippe Morency.'], 'venue': 'ACL (1). pages 973– 982.', 'citeRegEx': 'Pérez.Rosas et al\\.,? 2013', 'shortCiteRegEx': 'Pérez.Rosas et al\\.', 'year': 2013}, {'title': 'A review of affective computing: From unimodal analysis to multimodal fusion', 'author': ['Soujanya Poria', 'Erik Cambria', 'Rajiv Bajpai', 'Amir Hussain.'], 'venue': 'Information Fusion .', 'citeRegEx': 'Poria et al\\.,? 2017', 'shortCiteRegEx': 'Poria et al\\.', 'year': 2017}, {'title': 'Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis', 'author': ['Soujanya Poria', 'Erik Cambria', 'Alexander Gelbukh.'], 'venue': 'Proceedings of EMNLP. pages 2539–2544.', 'citeRegEx': 'Poria et al\\.,? 2015', 'shortCiteRegEx': 'Poria et al\\.', 'year': 2015}, {'title': 'Ensemble of SVM trees for multimodal emotion recognition', 'author': ['V. Rozgic', 'Sankaranarayanan Ananthakrishnan', 'Shirin Saleem', 'Rohit Kumar', 'Rohit Prasad.'], 'venue': 'Proceedings of APSIPA ASC. IEEE, pages 1–4.', 'citeRegEx': 'Rozgic et al\\.,? 2012a', 'shortCiteRegEx': 'Rozgic et al\\.', 'year': 2012}, {'title': 'Ensemble of svm trees for multimodal emotion recognition', 'author': ['Viktor Rozgic', 'Sankaranarayanan Ananthakrishnan', 'Shirin Saleem', 'Rohit Kumar', 'Rohit Prasad.'], 'venue': 'Signal & Information Processing Association Annual Summit and Conference', 'citeRegEx': 'Rozgic et al\\.,? 2012b', 'shortCiteRegEx': 'Rozgic et al\\.', 'year': 2012}, {'title': 'Recognizing affect from linguistic information in 3d continuous space', 'author': ['Björn Schuller.'], 'venue': 'IEEE Transactions on Affective Computing 2(4):192–205.', 'citeRegEx': 'Schuller.,? 2011', 'shortCiteRegEx': 'Schuller.', 'year': 2011}, {'title': 'Recursive deep models for semantic compositionality over a sentiment treebank', 'author': ['Richard Socher', 'Alex Perelygin', 'Jean Y Wu', 'Jason Chuang', 'Christopher D Manning', 'Andrew Y Ng', 'Christopher Potts.'], 'venue': 'Proceedings of EMNL). Citeseer, volume', 'citeRegEx': 'Socher et al\\.,? 2013', 'shortCiteRegEx': 'Socher et al\\.', 'year': 2013}, {'title': 'Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies', 'author': ['Martin Wöllmer', 'Florian Eyben', 'Stephan Reiter', 'Björn W Schuller', 'Cate Cox', 'Ellen Douglas-Cowie', 'Roddy Cowie'], 'venue': None, 'citeRegEx': 'Wöllmer et al\\.,? \\Q2008\\E', 'shortCiteRegEx': 'Wöllmer et al\\.', 'year': 2008}, {'title': 'Youtube movie reviews: Sentiment analysis in an audio-visual context', 'author': ['Martin Wollmer', 'Felix Weninger', 'Timo Knaup', 'Bjorn Schuller', 'Congkai Sun', 'Kenji Sagae', 'LouisPhilippe Morency.'], 'venue': 'IEEE Intelligent Systems 28(3):46–53.', 'citeRegEx': 'Wollmer et al\\.,? 2013', 'shortCiteRegEx': 'Wollmer et al\\.', 'year': 2013}, {'title': 'Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels', 'author': ['Chung-Hsien Wu', 'Wei-Bin Liang.'], 'venue': 'IEEE Transactions on Affective Computing 2(1):10–21.', 'citeRegEx': 'Wu and Liang.,? 2011', 'shortCiteRegEx': 'Wu and Liang.', 'year': 2011}, {'title': 'Multimodal sentiment intensity analysis in videos: Facial gestures and verbal', 'author': ['Amir Zadeh', 'Rowan Zellers', 'Eli Pincus', 'LouisPhilippe Morency'], 'venue': None, 'citeRegEx': 'Zadeh et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Zadeh et al\\.', 'year': 2016}, {'title': 'Attentionbased bidirectional long short-term memory networks for relation classification', 'author': ['Peng Zhou', 'Wei Shi', 'Jun Tian', 'Zhenyu Qi', 'Bingchen Li', 'Hongwei Hao', 'Bo Xu.'], 'venue': 'The 54th Annual Meeting of the Association for Computational Lin-', 'citeRegEx': 'Zhou et al\\.,? 2016', 'shortCiteRegEx': 'Zhou et al\\.', 'year': 2016}]  \n",
            "134                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [{'title': 'A Convolutional Attention Network for Extreme Summarization of Source Code', 'author': ['M. Allamanis', 'H. Peng', 'C. Sutton.'], 'venue': 'ArXiv e-prints .', 'citeRegEx': 'Allamanis et al\\.,? 2016', 'shortCiteRegEx': 'Allamanis et al\\.', 'year': 2016}, {'title': 'Neural machine translation by jointly learning to align and translate', 'author': ['Dzmitry Bahdanau', 'Kyunghyun Cho', 'Yoshua Bengio.'], 'venue': 'arXiv preprint arXiv:1409.0473 .', 'citeRegEx': 'Bahdanau et al\\.,? 2014', 'shortCiteRegEx': 'Bahdanau et al\\.', 'year': 2014}, {'title': 'Opinion expression mining by exploiting keyphrase extraction', 'author': ['Gábor Berend.'], 'venue': 'IJCNLP. Citeseer, pages 1162–1170.', 'citeRegEx': 'Berend.,? 2011', 'shortCiteRegEx': 'Berend.', 'year': 2011}, {'title': 'Learning phrase representations using rnn encoder-decoder for statistical machine translation', 'author': ['Kyunghyun Cho', 'Bart Van Merriënboer', 'Caglar Gulcehre', 'Dzmitry Bahdanau', 'Fethi Bougares', 'Holger Schwenk', 'Yoshua Bengio.'], 'venue': 'arXiv preprint', 'citeRegEx': 'Cho et al\\.,? 2014', 'shortCiteRegEx': 'Cho et al\\.', 'year': 2014}, {'title': 'Domain-specific keyphrase extraction', 'author': ['Eibe Frank', 'Gordon W Paynter', 'Ian H Witten', 'Carl Gutwin', 'Craig G Nevill-Manning'], 'venue': None, 'citeRegEx': 'Frank et al\\.,? \\Q1999\\E', 'shortCiteRegEx': 'Frank et al\\.', 'year': 1999}, {'title': 'Lstm recurrent networks learn simple context-free and contextsensitive languages', 'author': ['Felix A Gers', 'E Schmidhuber.'], 'venue': 'IEEE Transactions on Neural Networks 12(6):1333–1340.', 'citeRegEx': 'Gers and Schmidhuber.,? 2001', 'shortCiteRegEx': 'Gers and Schmidhuber.', 'year': 2001}, {'title': 'Extracting keyphrases from research papers using citation networks', 'author': ['Sujatha Das Gollapalli', 'Cornelia Caragea.'], 'venue': 'Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence. AAAI Press, AAAI’14, pages 1629–1635.', 'citeRegEx': 'Gollapalli and Caragea.,? 2014', 'shortCiteRegEx': 'Gollapalli and Caragea.', 'year': 2014}, {'title': 'Extracting key terms from noisy and multitheme documents', 'author': ['Maria Grineva', 'Maxim Grinev', 'Dmitry Lizorkin.'], 'venue': 'Proceedings of the 18th International Conference on World Wide Web. ACM, New York, NY, USA, WWW ’09, pages 661–670.', 'citeRegEx': 'Grineva et al\\.,? 2009', 'shortCiteRegEx': 'Grineva et al\\.', 'year': 2009}, {'title': 'Incorporating copying mechanism in sequence-to-sequence learning', 'author': ['Jiatao Gu', 'Zhengdong Lu', 'Hang Li', 'Victor OK Li.'], 'venue': 'arXiv preprint arXiv:1603.06393 .', 'citeRegEx': 'Gu et al\\.,? 2016', 'shortCiteRegEx': 'Gu et al\\.', 'year': 2016}, {'title': 'Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art', 'author': ['Kazi Saidul Hasan', 'Vincent Ng.'], 'venue': 'Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Lin-', 'citeRegEx': 'Hasan and Ng.,? 2010', 'shortCiteRegEx': 'Hasan and Ng.', 'year': 2010}, {'title': 'Long short-term memory', 'author': ['Sepp Hochreiter', 'Jürgen Schmidhuber.'], 'venue': 'Neural computation 9(8):1735–1780.', 'citeRegEx': 'Hochreiter and Schmidhuber.,? 1997', 'shortCiteRegEx': 'Hochreiter and Schmidhuber.', 'year': 1997}, {'title': 'Improved automatic keyword extraction given more linguistic knowledge', 'author': ['Anette Hulth.'], 'venue': 'Proceedings of the 2003 conference on Empirical methods in natural language processing. Association for Computational Linguistics, pages 216–223.', 'citeRegEx': 'Hulth.,? 2003', 'shortCiteRegEx': 'Hulth.', 'year': 2003}, {'title': 'A study on automatically extracted keywords in text categorization', 'author': ['Anette Hulth', 'Beáta B Megyesi.'], 'venue': 'Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu-', 'citeRegEx': 'Hulth and Megyesi.,? 2006', 'shortCiteRegEx': 'Hulth and Megyesi.', 'year': 2006}, {'title': 'Phrasier: a system for interactive document retrieval using keyphrases', 'author': ['Steve Jones', 'Mark S Staveley.'], 'venue': 'Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, pages', 'citeRegEx': 'Jones and Staveley.,? 1999', 'shortCiteRegEx': 'Jones and Staveley.', 'year': 1999}, {'title': 'Automatic hypertext keyphrase detection', 'author': ['Daniel Kelleher', 'Saturnino Luz.'], 'venue': 'Proceedings of the 19th International Joint Conference on Artificial Intelligence. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, IJCAI’05, pages 1608–1609.', 'citeRegEx': 'Kelleher and Luz.,? 2005', 'shortCiteRegEx': 'Kelleher and Luz.', 'year': 2005}, {'title': 'Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles', 'author': ['Su Nam Kim', 'Olena Medelyan', 'Min-Yen Kan', 'Timothy Baldwin.'], 'venue': 'Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computa-', 'citeRegEx': 'Kim et al\\.,? 2010', 'shortCiteRegEx': 'Kim et al\\.', 'year': 2010}, {'title': 'Adam: A method for stochastic optimization', 'author': ['Diederik Kingma', 'Jimmy Ba.'], 'venue': 'arXiv preprint arXiv:1412.6980 .', 'citeRegEx': 'Kingma and Ba.,? 2014', 'shortCiteRegEx': 'Kingma and Ba.', 'year': 2014}, {'title': 'Large dataset for keyphrases extraction', 'author': ['Mikalai Krapivin', 'Aliaksandr Autayeu', 'Maurizio Marchese.'], 'venue': 'Technical Report DISI-09-055, DISI, Trento, Italy.', 'citeRegEx': 'Krapivin et al\\.,? 2008', 'shortCiteRegEx': 'Krapivin et al\\.', 'year': 2008}, {'title': 'Unsupervised Keyphrase Extraction: Introducing New Kinds of Words to Keyphrases', 'author': ['Tho Thi Ngoc Le', 'Minh Le Nguyen', 'Akira Shimazu'], 'venue': None, 'citeRegEx': 'Le et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Le et al\\.', 'year': 2016}, {'title': 'Automatic keyphrase extraction by bridging vocabulary gap', 'author': ['Zhiyuan Liu', 'Xinxiong Chen', 'Yabin Zheng', 'Maosong Sun.'], 'venue': 'Proceedings of the Fifteenth Conference on Computational Natural Language Learning. Association for Computational', 'citeRegEx': 'Liu et al\\.,? 2011', 'shortCiteRegEx': 'Liu et al\\.', 'year': 2011}, {'title': 'Automatic keyphrase extraction via topic decomposition', 'author': ['Zhiyuan Liu', 'Wenyi Huang', 'Yabin Zheng', 'Maosong Sun.'], 'venue': 'Proceedings of the 2010 conference on empirical methods in natural language processing. Association for Compu-', 'citeRegEx': 'Liu et al\\.,? 2010', 'shortCiteRegEx': 'Liu et al\\.', 'year': 2010}, {'title': 'Clustering to find exemplar terms for keyphrase extraction', 'author': ['Zhiyuan Liu', 'Peng Li', 'Yabin Zheng', 'Maosong Sun.'], 'venue': 'Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1. Association', 'citeRegEx': 'Liu et al\\.,? 2009', 'shortCiteRegEx': 'Liu et al\\.', 'year': 2009}, {'title': 'Humb: Automatic key term extraction from scientific articles in grobid', 'author': ['Patrice Lopez', 'Laurent Romary.'], 'venue': 'Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics, Strouds-', 'citeRegEx': 'Lopez and Romary.,? 2010', 'shortCiteRegEx': 'Lopez and Romary.', 'year': 2010}, {'title': 'Sequence level training with recurrent neural networks', 'author': ['Michael Auli', 'Wojciech Zaremba'], 'venue': None, 'citeRegEx': 'Ranzato et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Ranzato et al\\.', 'year': 2016}, {'title': 'Keyword extraction from a single document using word co-occurrence statistical information', 'author': ['Yutaka Matsuo', 'Mitsuru Ishizuka.'], 'venue': 'International Journal on Artificial Intelligence Tools 13(01):157– 169.', 'citeRegEx': 'Matsuo and Ishizuka.,? 2004', 'shortCiteRegEx': 'Matsuo and Ishizuka.', 'year': 2004}, {'title': 'Human-competitive tagging using automatic keyphrase extraction', 'author': ['Olena Medelyan', 'Eibe Frank', 'Ian H Witten.'], 'venue': 'Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3. Association', 'citeRegEx': 'Medelyan et al\\.,? 2009a', 'shortCiteRegEx': 'Medelyan et al\\.', 'year': 2009}, {'title': 'Human-competitive tagging using automatic keyphrase extraction', 'author': ['Olena Medelyan', 'Eibe Frank', 'Ian H. Witten.'], 'venue': 'Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3.', 'citeRegEx': 'Medelyan et al\\.,? 2009b', 'shortCiteRegEx': 'Medelyan et al\\.', 'year': 2009}, {'title': 'Topic indexing with wikipedia', 'author': ['Olena Medelyan', 'Ian H Witten', 'David Milne.'], 'venue': 'Proceedings of the AAAI WikiAI workshop. volume 1, pages 19–24.', 'citeRegEx': 'Medelyan et al\\.,? 2008', 'shortCiteRegEx': 'Medelyan et al\\.', 'year': 2008}, {'title': 'Textrank: Bringing order into texts', 'author': ['Rada Mihalcea', 'Paul Tarau.'], 'venue': 'Association for Computational Linguistics.', 'citeRegEx': 'Mihalcea and Tarau.,? 2004', 'shortCiteRegEx': 'Mihalcea and Tarau.', 'year': 2004}, {'title': 'Keyphrase extraction in scientific publications', 'author': ['Thuy Dung Nguyen', 'Min-Yen Kan.'], 'venue': 'International Conference on Asian Digital Libraries. Springer, pages 317–326.', 'citeRegEx': 'Nguyen and Kan.,? 2007', 'shortCiteRegEx': 'Nguyen and Kan.', 'year': 2007}, {'title': 'A neural attention model for abstractive sentence summarization', 'author': ['Alexander M. Rush', 'Sumit Chopra', 'Jason Weston.'], 'venue': 'Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon,', 'citeRegEx': 'Rush et al\\.,? 2015', 'shortCiteRegEx': 'Rush et al\\.', 'year': 2015}, {'title': 'Building end-to-end dialogue systems using generative hierarchical neural network models', 'author': ['Iulian V Serban', 'Alessandro Sordoni', 'Yoshua Bengio', 'Aaron Courville', 'Joelle Pineau.'], 'venue': 'Proceedings of the 30th AAAI Conference on Artificial Intelligence', 'citeRegEx': 'Serban et al\\.,? 2016', 'shortCiteRegEx': 'Serban et al\\.', 'year': 2016}, {'title': 'Minimum risk training for neural machine translation', 'author': ['Shiqi Shen', 'Yong Cheng', 'Zhongjun He', 'Wei He', 'Hua Wu', 'Maosong Sun', 'Yang Liu'], 'venue': None, 'citeRegEx': 'Shen et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Shen et al\\.', 'year': 2016}, {'title': 'Kpspotter: a flexible information gain-based keyphrase extraction system', 'author': ['Min Song', 'Il-Yeol Song', 'Xiaohua Hu.'], 'venue': 'Proceedings of the 5th ACM international workshop on Web information and data management. ACM, pages 50–53.', 'citeRegEx': 'Song et al\\.,? 2003', 'shortCiteRegEx': 'Song et al\\.', 'year': 2003}, {'title': 'Sequence to sequence learning with neural networks', 'author': ['Ilya Sutskever', 'Oriol Vinyals', 'Quoc V Le.'], 'venue': 'Advances in neural information processing systems. pages 3104–3112.', 'citeRegEx': 'Sutskever et al\\.,? 2014', 'shortCiteRegEx': 'Sutskever et al\\.', 'year': 2014}, {'title': 'A language model approach to keyphrase extraction', 'author': ['Takashi Tomokiyo', 'Matthew Hurst.'], 'venue': 'Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment - Volume 18.', 'citeRegEx': 'Tomokiyo and Hurst.,? 2003', 'shortCiteRegEx': 'Tomokiyo and Hurst.', 'year': 2003}, {'title': 'Grammar as a foreign language', 'author': ['Oriol Vinyals', 'Łukasz Kaiser', 'Terry Koo', 'Slav Petrov', 'Ilya Sutskever', 'Geoffrey Hinton.'], 'venue': 'Advances in Neural Information Processing Systems. pages 2773–2781.', 'citeRegEx': 'Vinyals et al\\.,? 2015', 'shortCiteRegEx': 'Vinyals et al\\.', 'year': 2015}, {'title': 'Single document keyphrase extraction using neighborhood knowledge', 'author': ['Xiaojun Wan', 'Jianguo Xiao'], 'venue': None, 'citeRegEx': 'Wan and Xiao.,? \\Q2008\\E', 'shortCiteRegEx': 'Wan and Xiao.', 'year': 2008}, {'title': 'PTR: Phrase-Based Topical Ranking for Automatic Keyphrase Extraction in Scientific Publications', 'author': ['Minmei Wang', 'Bo Zhao', 'Yihua Huang'], 'venue': None, 'citeRegEx': 'Wang et al\\.,? \\Q2016\\E', 'shortCiteRegEx': 'Wang et al\\.', 'year': 2016}, {'title': 'Kea: Practical automatic keyphrase extraction', 'author': ['Ian H Witten', 'Gordon W Paynter', 'Eibe Frank', 'Carl Gutwin', 'Craig G Nevill-Manning.'], 'venue': 'Proceedings of the fourth ACM conference on Digital libraries. ACM, pages 254–255.', 'citeRegEx': 'Witten et al\\.,? 1999', 'shortCiteRegEx': 'Witten et al\\.', 'year': 1999}, {'title': 'Efficient summarization with read-again and copy mechanism', 'author': ['Wenyuan Zeng', 'Wenjie Luo', 'Sanja Fidler', 'Raquel Urtasun.'], 'venue': 'arXiv preprint arXiv:1611.03382 .', 'citeRegEx': 'Zeng et al\\.,? 2016', 'shortCiteRegEx': 'Zeng et al\\.', 'year': 2016}, {'title': 'World wide web site summarization', 'author': ['Yongzheng Zhang', 'Nur Zincir-Heywood', 'Evangelos Milios.'], 'venue': 'Web Intelligence and Agent Systems: An International Journal 2(1):39–53.', 'citeRegEx': 'Zhang et al\\.,? 2004', 'shortCiteRegEx': 'Zhang et al\\.', 'year': 2004}]  \n",
            "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [{'title': 'Neural machine translation by jointly learning to align and translate', 'author': ['Dzmitry Bahdanau', 'Kyunghyun Cho', 'Yoshua Bengio.'], 'venue': 'arXiv preprint arXiv:1409.0473 .', 'citeRegEx': 'Bahdanau et al\\.,? 2014', 'shortCiteRegEx': 'Bahdanau et al\\.', 'year': 2014}, {'title': 'Class-based n-gram models of natural language', 'author': ['Peter F Brown', 'Peter V Desouza', 'Robert L Mercer', 'Vincent J Della Pietra', 'Jenifer C Lai.'], 'venue': 'Computational linguistics 18(4):467–479.', 'citeRegEx': 'Brown et al\\.,? 1992', 'shortCiteRegEx': 'Brown et al\\.', 'year': 1992}, {'title': 'Strategies for training large vocabulary neural language models', 'author': ['Wenlin Chen', 'David Grangier', 'Michael Auli.'], 'venue': 'Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for', 'citeRegEx': 'Chen et al\\.,? 2016', 'shortCiteRegEx': 'Chen et al\\.', 'year': 2016}, {'title': 'Variablelength word encodings for neural translation models', 'author': ['Rohan Chitnis', 'John DeNero.'], 'venue': 'Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational', 'citeRegEx': 'Chitnis and DeNero.,? 2015', 'shortCiteRegEx': 'Chitnis and DeNero.', 'year': 2015}, {'title': 'Solving multiclass learning problems via errorcorrecting output codes', 'author': ['Thomas G. Dietterich', 'Ghulum Bakiri.'], 'venue': 'Journal of Artificial Intelligence Research 2:263–286.', 'citeRegEx': 'Dietterich and Bakiri.,? 1995', 'shortCiteRegEx': 'Dietterich and Bakiri.', 'year': 1995}, {'title': 'Multilabel classification with error-correcting codes', 'author': ['Chun-Sung Ferng', 'Hsuan-Tien Lin.'], 'venue': 'Journal of Machine Learning Research 20:281–295.', 'citeRegEx': 'Ferng and Lin.,? 2011', 'shortCiteRegEx': 'Ferng and Lin.', 'year': 2011}, {'title': 'Multilabel classification using error-correcting codes of hard or soft bits', 'author': ['Chun-Sung Ferng', 'Hsuan-Tien Lin.'], 'venue': 'IEEE transactions on neural networks and learning systems 24(11):1888–1900.', 'citeRegEx': 'Ferng and Lin.,? 2013', 'shortCiteRegEx': 'Ferng and Lin.', 'year': 2013}, {'title': 'Learning to forget: Continual prediction with LSTM', 'author': ['Felix A Gers', 'Jürgen Schmidhuber', 'Fred Cummins.'], 'venue': 'Neural computation 12(10):2451–2471.', 'citeRegEx': 'Gers et al\\.,? 2000', 'shortCiteRegEx': 'Gers et al\\.', 'year': 2000}, {'title': 'Notes on digital coding', 'author': ['Marcel J.E. Golay.'], 'venue': 'Proceedings of the Institute of Radio Engineers 37:657.', 'citeRegEx': 'Golay.,? 1949', 'shortCiteRegEx': 'Golay.', 'year': 1949}, {'title': 'A method for the construction of minimum-redundancy codes', 'author': ['David A. Huffman.'], 'venue': 'Proceedings of the Institute of Radio Engineers 40(9):1098–1101.', 'citeRegEx': 'Huffman.,? 1952', 'shortCiteRegEx': 'Huffman.', 'year': 1952}, {'title': 'Adam: A method for stochastic optimization', 'author': ['Diederik Kingma', 'Jimmy Ba.'], 'venue': 'arXiv preprint arXiv:1412.6980 .', 'citeRegEx': 'Kingma and Ba.,? 2014', 'shortCiteRegEx': 'Kingma and Ba.', 'year': 2014}, {'title': 'On nearest-neighbor error-correcting output codes with application to all-pairs multiclass support vector machines', 'author': ['Aldebaro Klautau', 'Nikola Jevtić', 'Alon Orlitsky.'], 'venue': 'Journal of Machine Learning Research 4(April):1–15.', 'citeRegEx': 'Klautau et al\\.,? 2003', 'shortCiteRegEx': 'Klautau et al\\.', 'year': 2003}, {'title': 'Multilabel classification using error correction codes', 'author': ['Abbas Z Kouzani.'], 'venue': 'International Symposium on Intelligence Computation and Applications. Springer, pages 444–454.', 'citeRegEx': 'Kouzani.,? 2010', 'shortCiteRegEx': 'Kouzani.', 'year': 2010}, {'title': 'Multilabel classification by bch code and random forests', 'author': ['Abbas Z Kouzani', 'Gulisong Nasireding.'], 'venue': 'International journal of recent trends in engineering 2(1):113–116.', 'citeRegEx': 'Kouzani and Nasireding.,? 2009', 'shortCiteRegEx': 'Kouzani and Nasireding.', 'year': 2009}, {'title': 'Character-based neural machine translation', 'author': ['Wang Ling', 'Isabel Trancoso', 'Chris Dyer', 'Alan W Black.'], 'venue': 'arXiv preprint arXiv:1511.04586 .', 'citeRegEx': 'Ling et al\\.,? 2015', 'shortCiteRegEx': 'Ling et al\\.', 'year': 2015}, {'title': 'Using svm and error-correcting codes for multiclass dialog act classification in meeting corpus', 'author': ['Yang Liu.'], 'venue': 'INTERSPEECH.', 'citeRegEx': 'Liu.,? 2006', 'shortCiteRegEx': 'Liu.', 'year': 2006}, {'title': 'Effective approaches to attention-based neural machine translation', 'author': ['Thang Luong', 'Hieu Pham', 'Christopher D. Manning.'], 'venue': 'Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Compu-', 'citeRegEx': 'Luong et al\\.,? 2015', 'shortCiteRegEx': 'Luong et al\\.', 'year': 2015}, {'title': 'Distributed representations of words and phrases and their compositionality', 'author': ['Tomas Mikolov', 'Ilya Sutskever', 'Kai Chen', 'Greg S Corrado', 'Jeff Dean.'], 'venue': 'Advances in neural information processing systems. pages 3111–3119.', 'citeRegEx': 'Mikolov et al\\.,? 2013', 'shortCiteRegEx': 'Mikolov et al\\.', 'year': 2013}, {'title': 'A fast and simple algorithm for training neural probabilistic language models', 'author': ['Andriy Mnih', 'Yee Whye Teh.'], 'venue': 'Proceedings of the 29th International Conference on Machine Learning.', 'citeRegEx': 'Mnih and Teh.,? 2012', 'shortCiteRegEx': 'Mnih and Teh.', 'year': 2012}, {'title': 'Hierarchical probabilistic neural network language model', 'author': ['Frederic Morin', 'Yoshua Bengio.'], 'venue': 'Proceedings of Tenth International Workshop on Artificial Intelligence and Statistics. volume 5, pages 246–252.', 'citeRegEx': 'Morin and Bengio.,? 2005', 'shortCiteRegEx': 'Morin and Bengio.', 'year': 2005}, {'title': 'Aspec: Asian scientific paper excerpt corpus', 'author': ['Toshiaki Nakazawa', 'Manabu Yaguchi', 'Kiyotaka Uchimoto', 'Masao Utiyama', 'Eiichiro Sumita', 'Sadao Kurohashi', 'Hitoshi Isahara.'], 'venue': 'Nicoletta Calzolari (Conference Chair), Khalid Choukri,', 'citeRegEx': 'Nakazawa et al\\.,? 2016', 'shortCiteRegEx': 'Nakazawa et al\\.', 'year': 2016}, {'title': 'Dynet: The dynamic neural network toolkit', 'author': ['Lingpeng Kong', 'Adhiguna Kuncoro', 'Gaurav Kumar', 'Chaitanya Malaviya', 'Paul Michel', 'Yusuke Oda', 'Matthew Richardson', 'Naomi Saphra', 'Swabha Swayamdipta', 'Pengcheng Yin.'], 'venue': 'arXiv preprint', 'citeRegEx': 'Kong et al\\.,? 2017', 'shortCiteRegEx': 'Kong et al\\.', 'year': 2017}, {'title': 'Pointwise prediction for robust, adaptable japanese morphological analysis', 'author': ['Graham Neubig', 'Yosuke Nakata', 'Shinsuke Mori.'], 'venue': 'Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language', 'citeRegEx': 'Neubig et al\\.,? 2011', 'shortCiteRegEx': 'Neubig et al\\.', 'year': 2011}, {'title': 'Bleu: a method for automatic evaluation of machine translation', 'author': ['Kishore Papineni', 'Salim Roukos', 'Todd Ward', 'Wei-Jing Zhu.'], 'venue': 'Proceedings of 40th Annual Meeting of the Association for Computational Linguis-', 'citeRegEx': 'Papineni et al\\.,? 2002', 'shortCiteRegEx': 'Papineni et al\\.', 'year': 2002}, {'title': 'Neural machine translation of rare words with subword units', 'author': ['Rico Sennrich', 'Barry Haddow', 'Alexandra Birch.'], 'venue': 'Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for', 'citeRegEx': 'Sennrich et al\\.,? 2016', 'shortCiteRegEx': 'Sennrich et al\\.', 'year': 2016}, {'title': 'A mathematical theory of communication', 'author': ['Claude E. Shannon.'], 'venue': 'Bell System Technical Journal 27(3):379–423.', 'citeRegEx': 'Shannon.,? 1948', 'shortCiteRegEx': 'Shannon.', 'year': 1948}, {'title': 'Dropout: a simple way to prevent neural networks from overfitting', 'author': ['Nitish Srivastava', 'Geoffrey E Hinton', 'Alex Krizhevsky', 'Ilya Sutskever', 'Ruslan Salakhutdinov.'], 'venue': 'Journal of Machine Learning Research 15(1):1929–1958.', 'citeRegEx': 'Srivastava et al\\.,? 2014', 'shortCiteRegEx': 'Srivastava et al\\.', 'year': 2014}, {'title': 'Sequence to sequence learning with neural networks', 'author': ['Ilya Sutskever', 'Oriol Vinyals', 'Quoc V Le.'], 'venue': 'Advances in neural information processing systems. pages 3104–3112.', 'citeRegEx': 'Sutskever et al\\.,? 2014', 'shortCiteRegEx': 'Sutskever et al\\.', 'year': 2014}, {'title': 'Building a bilingual travel conversation database for speech translation research', 'author': ['Toshiyuki Takezawa.'], 'venue': 'Proc. of the 2nd international workshop on East-Asian resources and evaluation conference on language resources and evaluation. pages 17–20.', 'citeRegEx': 'Takezawa.,? 1999', 'shortCiteRegEx': 'Takezawa.', 'year': 1999}, {'title': 'Error bounds for convolutional codes and an asymptotically optimum decoding algorithm', 'author': ['Andrew Viterbi.'], 'venue': 'IEEE transactions on Information Theory 13(2):260–269.', 'citeRegEx': 'Viterbi.,? 1967', 'shortCiteRegEx': 'Viterbi.', 'year': 1967}, {'title': 'Human behavior and the principle of least effort', 'author': ['George. K. Zipf'], 'venue': None, 'citeRegEx': 'Zipf.,? \\Q1949\\E', 'shortCiteRegEx': 'Zipf.', 'year': 1949}]  \n",
            "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [{'title': 'Improving parsing and pp attachment performance with sense information', 'author': ['Eneko Agirre.'], 'venue': 'ACL. Citeseer.', 'citeRegEx': 'Agirre.,? 2008', 'shortCiteRegEx': 'Agirre.', 'year': 2008}, {'title': 'Neural machine translation by jointly learning to align and translate', 'author': ['Dzmitry Bahdanau', 'Kyunghyun Cho', 'Yoshua Bengio.'], 'venue': 'CoRR abs/1409.0473.', 'citeRegEx': 'Bahdanau et al\\.,? 2014', 'shortCiteRegEx': 'Bahdanau et al\\.', 'year': 2014}, {'title': 'A linear dynamical system model for text', 'author': ['David Belanger', 'Sham M. Kakade.'], 'venue': 'ICML.', 'citeRegEx': 'Belanger and Kakade.,? 2015', 'shortCiteRegEx': 'Belanger and Kakade.', 'year': 2015}, {'title': 'Exploring compositional architectures and word vector representations for prepositional phrase attachment', 'author': ['Yonatan Belinkov', 'Tao Lei', 'Regina Barzilay', 'Amir Globerson.'], 'venue': 'Transactions of the Association for Computational Linguistics 2:561–572.', 'citeRegEx': 'Belinkov et al\\.,? 2014', 'shortCiteRegEx': 'Belinkov et al\\.', 'year': 2014}, {'title': 'A rule-based approach to prepositional phrase attachment disambiguation', 'author': ['Eric Brill', 'Philip Resnik.'], 'venue': 'Proceedings of the 15th conference on Computational linguistics-Volume 2. Association for Computational Linguistics, pages 1198–1204.', 'citeRegEx': 'Brill and Resnik.,? 1994', 'shortCiteRegEx': 'Brill and Resnik.', 'year': 1994}, {'title': 'A fast and accurate dependency parser using neural networks', 'author': ['Danqi Chen', 'Christopher D Manning.'], 'venue': 'EMNLP. pages 740–750.', 'citeRegEx': 'Chen and Manning.,? 2014', 'shortCiteRegEx': 'Chen and Manning.', 'year': 2014}, {'title': 'A unified model for word sense representation and disambiguation', 'author': ['Xinxiong Chen', 'Zhiyuan Liu', 'Maosong Sun.'], 'venue': 'EMNLP. pages 1025–1035.', 'citeRegEx': 'Chen et al\\.,? 2014', 'shortCiteRegEx': 'Chen et al\\.', 'year': 2014}, {'title': 'Keras', 'author': ['François Chollet.'], 'venue': 'https://github. com/fchollet/keras.', 'citeRegEx': 'Chollet.,? 2015', 'shortCiteRegEx': 'Chollet.', 'year': 2015}, {'title': 'Retrofitting word vectors to semantic lexicons', 'author': ['Manaal Faruqui', 'Jesse Dodge', 'Sujay Kumar Jauhar', 'Chris Dyer', 'Eduard H. Hovy', 'Noah A. Smith.'], 'venue': 'NAACL.', 'citeRegEx': 'Faruqui et al\\.,? 2015', 'shortCiteRegEx': 'Faruqui et al\\.', 'year': 2015}, {'title': 'Improving word representations via global context and multiple word prototypes', 'author': ['Eric H Huang', 'Richard Socher', 'Christopher D Manning', 'Andrew Y Ng.'], 'venue': 'Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-', 'citeRegEx': 'Huang et al\\.,? 2012', 'shortCiteRegEx': 'Huang et al\\.', 'year': 2012}, {'title': 'Ontologically grounded multi-sense representation learning for semantic vector space models', 'author': ['Sujay Kumar Jauhar', 'Chris Dyer', 'Eduard H. Hovy.'], 'venue': 'NAACL.', 'citeRegEx': 'Jauhar et al\\.,? 2015', 'shortCiteRegEx': 'Jauhar et al\\.', 'year': 2015}, {'title': 'A large-scale classification of english verbs', 'author': ['Karin Kipper', 'Anna Korhonen', 'Neville Ryant', 'Martha Palmer.'], 'venue': 'Language Resources and Evaluation 42(1):21–40.', 'citeRegEx': 'Kipper et al\\.,? 2008', 'shortCiteRegEx': 'Kipper et al\\.', 'year': 2008}, {'title': 'Neural architectures for named entity recognition', 'author': ['Guillaume Lample', 'Miguel Ballesteros', 'Sandeep Subramanian', 'Kazuya Kawakami', 'Chris Dyer.'], 'venue': 'NAACL.', 'citeRegEx': 'Lample et al\\.,? 2016', 'shortCiteRegEx': 'Lample et al\\.', 'year': 2016}, {'title': 'Low-rank tensors for scoring dependency structures', 'author': ['Tao Lei', 'Yuan Zhang', 'Regina Barzilay', 'Tommi Jaakkola.'], 'venue': 'ACL. Association for Computational Linguistics.', 'citeRegEx': 'Lei et al\\.,? 2014', 'shortCiteRegEx': 'Lei et al\\.', 'year': 2014}, {'title': 'Distributed representations of words and phrases and their compositionality', 'author': ['Tomas Mikolov', 'Ilya Sutskever', 'Kai Chen', 'Gregory S. Corrado', 'Jeffrey Dean.'], 'venue': 'CoRR abs/1310.4546.', 'citeRegEx': 'Mikolov et al\\.,? 2013', 'shortCiteRegEx': 'Mikolov et al\\.', 'year': 2013}, {'title': 'Wordnet: a lexical database for english', 'author': ['George A Miller.'], 'venue': 'Communications of the ACM 38(11):39–', 'citeRegEx': 'Miller.,? 1995', 'shortCiteRegEx': 'Miller.', 'year': 1995}, {'title': 'Efficient non-parametric estimation of multiple embeddings per word in vector space', 'author': ['Arvind Neelakantan', 'Jeevan Shankar', 'Alexandre Passos', 'Andrew McCallum.'], 'venue': 'arXiv preprint arXiv:1504.06654 .', 'citeRegEx': 'Neelakantan et al\\.,? 2015', 'shortCiteRegEx': 'Neelakantan et al\\.', 'year': 2015}, {'title': 'Glove: Global vectors for word representation', 'author': ['Jeffrey Pennington', 'Richard Socher', 'Christopher D Manning.'], 'venue': 'EMNLP.', 'citeRegEx': 'Pennington et al\\.,? 2014', 'shortCiteRegEx': 'Pennington et al\\.', 'year': 2014}, {'title': 'A maximum entropy model for prepositional phrase attachment', 'author': ['Adwait Ratnaparkhi', 'Jeff Reynar', 'Salim Roukos.'], 'venue': 'Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, pages 250–255.', 'citeRegEx': 'Ratnaparkhi et al\\.,? 1994', 'shortCiteRegEx': 'Ratnaparkhi et al\\.', 'year': 1994}, {'title': 'Multi-prototype vector-space models of word meaning', 'author': ['Joseph Reisinger', 'Raymond J Mooney.'], 'venue': 'Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.', 'citeRegEx': 'Reisinger and Mooney.,? 2010', 'shortCiteRegEx': 'Reisinger and Mooney.', 'year': 2010}, {'title': 'Semantic classes and syntactic ambiguity', 'author': ['Philip Resnik.'], 'venue': 'Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics.', 'citeRegEx': 'Resnik.,? 1993', 'shortCiteRegEx': 'Resnik.', 'year': 1993}, {'title': 'Autoextend: Extending word embeddings to embeddings for synsets and lexemes', 'author': ['Sascha Rothe', 'Hinrich Schütze.'], 'venue': 'ACL.', 'citeRegEx': 'Rothe and Schütze.,? 2015', 'shortCiteRegEx': 'Rothe and Schütze.', 'year': 2015}, {'title': 'Word representations via gaussian embedding', 'author': ['Luke Vilnis', 'Andrew McCallum.'], 'venue': 'CoRR abs/1412.6623.', 'citeRegEx': 'Vilnis and McCallum.,? 2014', 'shortCiteRegEx': 'Vilnis and McCallum.', 'year': 2014}, {'title': 'Improving lexical embeddings with semantic knowledge', 'author': ['Mo Yu', 'Mark Dredze.'], 'venue': 'ACL.', 'citeRegEx': 'Yu and Dredze.,? 2014', 'shortCiteRegEx': 'Yu and Dredze.', 'year': 2014}, {'title': 'Selectional preferences for semantic role classification', 'author': ['Benat Zapirain', 'Eneko Agirre', 'Lluis Marquez', 'Mihai Surdeanu.'], 'venue': 'Computational Linguistics 39(3):631–663.', 'citeRegEx': 'Zapirain et al\\.,? 2013', 'shortCiteRegEx': 'Zapirain et al\\.', 'year': 2013}]  \n",
            "\n",
            "[137 rows x 12 columns]\n",
            "Datos guardados en CSV: /content/drive/MyDrive/PeerRead-master/data/acl_primer_ciclo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtener estado de aceptacion"
      ],
      "metadata": {
        "id": "OpSGsekgAeL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ruta del archivo\n",
        "archivo_ruta = '/content/drive/MyDrive/PeerRead-master/data/acl_accepted.txt'\n",
        "\n",
        "# Lista para almacenar los datos\n",
        "data = []\n",
        "\n",
        "# Leer el archivo\n",
        "with open(archivo_ruta, 'r') as archivo:\n",
        "    for linea in archivo:\n",
        "        # Agregar una nueva fila con 'Title' y 'Accepted' como TRUE\n",
        "        data.append({'Title': linea.strip(), 'Accepted': '1'})\n",
        "\n",
        "# Crear un DataFrame de Pandas\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Eliminar filas duplicadas basadas en la columna 'Title'\n",
        "df = df.drop_duplicates(subset='Title')\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame\n",
        "print(df)\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV\n",
        "df.to_csv('/content/drive/MyDrive/PeerRead-master/data/reviews_acl.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6Mf4BWv-N1z",
        "outputId": "17481ab8-958f-4453-a38a-a01a062f0cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                              Title  \\\n",
            "0                                               Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging   \n",
            "1                                                                         Finding Optimal 1-Endpoint-Crossing Trees   \n",
            "2                                                                           Grounding Action Descriptions in Videos   \n",
            "3                                         Branch and Bound Algorithm for Dependency Parsing with Non-local Features   \n",
            "4                                Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions   \n",
            "...                                                                                                             ...   \n",
            "19738                                                       Cross-Lingual Syntactic Transfer with Limited Resources   \n",
            "19739                                     Overcoming Language Variation in Sentiment Analysis with Social Attention   \n",
            "19740  Semantic Specialization of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints   \n",
            "19741                               Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding   \n",
            "19742                       Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation   \n",
            "\n",
            "      Accepted  \n",
            "0            1  \n",
            "1            1  \n",
            "2            1  \n",
            "3            1  \n",
            "4            1  \n",
            "...        ...  \n",
            "19738        1  \n",
            "19739        1  \n",
            "19740        1  \n",
            "19741        1  \n",
            "19742        1  \n",
            "\n",
            "[10406 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combinar csv"
      ],
      "metadata": {
        "id": "Z6fFy4WTH1dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Especifica la ruta de tus archivos CSV en tu Google Drive\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "\n",
        "archivo1 = os.path.join(ruta_carpeta_principal, \"acl_miguel.csv\")\n",
        "archivo2 = os.path.join(ruta_carpeta_principal, \"reviews_acl.csv\")\n",
        "\n",
        "# Carga los archivos CSV en DataFrames\n",
        "df1 = pd.read_csv(archivo1)\n",
        "df2 = pd.read_csv(archivo2)\n",
        "\n",
        "# Realiza la unión en base a la columna 'Title'\n",
        "result = pd.merge(df1, df2, on='Title', how='outer')\n",
        "\n",
        "# Llena las columnas 'Accepted' nulas con 'FALSE'\n",
        "result['Accepted'] = result['Accepted'].fillna('0')\n",
        "#result = result.dropna(subset=['Accepted'])\n",
        "result = result.dropna(subset=['Archivo'])\n",
        "\n",
        "print(result)\n",
        "# Guarda el resultado en un nuevo archivo CSV\n",
        "result.to_csv('/content/drive/MyDrive/PeerRead-master/data/acl_datos_miguel.csv', index=False)\n"
      ],
      "metadata": {
        "id": "uaps5jQ2-PIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d7a8939-9046-47a1-fa4a-03ea6f7ce904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Archivo  \\\n",
            "0    660.pdf   \n",
            "1    371.pdf   \n",
            "2    352.pdf   \n",
            "3     71.pdf   \n",
            "4    343.pdf   \n",
            "..       ...   \n",
            "132  419.pdf   \n",
            "133  182.pdf   \n",
            "134  676.pdf   \n",
            "135  691.pdf   \n",
            "136  104.pdf   \n",
            "\n",
            "                                                                                    Title  \\\n",
            "0                            Automatically Generating Rhythmic Verse with Neural Networks   \n",
            "1                                                        Phrasal Recurrent Neural Network   \n",
            "2                                 Adversarial Multi-task Learning for Text Classification   \n",
            "3                                Cross-lingual Name Tagging and Linking for 282 Languages   \n",
            "4                                          Neural Word Segmentation with Rich Pretraining   \n",
            "..                                                                                    ...   \n",
            "132                        One-Shot Neural Cross-Lingual Transfer for Paradigm Completion   \n",
            "133  Modeling Contextual Relationships Among Utterances for Multimodal Sentiment Analysis   \n",
            "134                                 Neural Machine Translation via Binary Code Prediction   \n",
            "135  Using Ontology-Grounded Token Embeddings To Predict Prepositional Phrase Attachments   \n",
            "136      Bridging Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding   \n",
            "\n",
            "    Accepted  \n",
            "0        1.0  \n",
            "1          0  \n",
            "2        1.0  \n",
            "3        1.0  \n",
            "4        1.0  \n",
            "..       ...  \n",
            "132      1.0  \n",
            "133        0  \n",
            "134      1.0  \n",
            "135        0  \n",
            "136        0  \n",
            "\n",
            "[137 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Especifica la ruta de tus archivos CSV en tu Google Drive\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "\n",
        "archivo1 = os.path.join(ruta_carpeta_principal, \"acl_primer_ciclo.csv\")\n",
        "archivo2 = os.path.join(ruta_carpeta_principal, \"acl_datos_miguel.csv\")\n",
        "\n",
        "# Carga los archivos CSV en DataFrames\n",
        "df1 = pd.read_csv(archivo1)\n",
        "df2 = pd.read_csv(archivo2)\n",
        "\n",
        "# Guarda el número inicial de filas en cada DataFrame\n",
        "num_filas_inicial_df1 = len(df1)\n",
        "print(\"Filas del doc 1:\",num_filas_inicial_df1)\n",
        "num_filas_inicial_df2 = len(df2)\n",
        "print(\"Filas del doc 2:\",num_filas_inicial_df2)\n",
        "\n",
        "# Union de csv con base en la columna 'Title'\n",
        "result = pd.merge(df1, df2, on='Archivo', how='outer')\n",
        "print(\"Número total de elementos al unir:\", len(result))\n",
        "\n",
        "# Calcular la diferencia en el número de filas después de la unión\n",
        "#num_filas_eliminadas_despues_union = 0\n",
        "\n",
        "result['Accepted'] = result['Accepted'].fillna('0')\n",
        "\n",
        "# Dropear filas con valores nulos en columnas\n",
        "#result = result.dropna(subset=['Accepted'])\n",
        "#result = result.dropna(subset=['Contenido'])\n",
        "print(\"Número total de elementos despues de dropear:\", len(result))\n",
        "# Calcular la diferencia en el numero de filas despues de la limpieza\n",
        "\n",
        "#num_filas_eliminadas_despues_limpieza = 0  # No se eliminan filas en esta etapa ya que las líneas están comentadas\n",
        "#print(\"Número total de filas eliminadas en limpieza:\", num_filas_eliminadas_despues_limpieza)\n",
        "\n",
        "#print(result)\n",
        "# Guarda el resultado en un CSV\n",
        "result.to_csv('/content/drive/MyDrive/acl_datos_completos.csv', index=False)\n",
        "\n",
        "\n",
        "# Imprime el numero total de filas eliminadas\n",
        "#print(\"Número total de filas eliminadas:\", num_total_filas_eliminadas)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxhSndNlEeHI",
        "outputId": "ed262a85-ec80-404b-844b-59d3a1271184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filas del doc 1: 137\n",
            "Filas del doc 2: 137\n",
            "Número total de elementos al unir: 137\n",
            "Número total de elementos despues de dropear: 137\n"
          ]
        }
      ]
    }
  ]
}