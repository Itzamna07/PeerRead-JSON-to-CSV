{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Cargar algo de drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5sULhWykeBa",
        "outputId": "2f08194d-4b88-4696-8242-be4fd266f03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creacion de dataset y obtencion de datos"
      ],
      "metadata": {
        "id": "wZrkvuPrxWax"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BUUuiRHkc38",
        "outputId": "afda061f-a181-48bc-f479-ee3dadb66151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error en el archivo: 621.pdf\n",
            "621.pdf\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Machine learning models of music typically break down the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. We explore the use of blocked Gibbs sampling as an analogue to the human approach, and introduce COCONET, a convolutional neural network in the NADE family of generative models (Uria et al., 2016). Despite ostensibly sampling from the same distribution as the NADE ancestral sampling procedure, we find that a blocked Gibbs approach significantly improves sample quality. We provide evidence that this is due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling. We demonstrate the versatility of our method on unconditioned polyphonic music generation. Step 0 Step 1 Step 4 Step 16 Step 64 Ground Truth Figure 1: Ancestral inpainting of a corrupted Bach chorale by COCONET. Colors are used to distinguish the four voices. Grayscale heatmaps show predictions p(xi | xC). The pitch sampled in the current step is indicated by a rectangular outline. The original Bach chorale is shown in the bottom right. Step 0 shows the corrupted Bach chorale. Step 64 shows the result. ∗Work done while at Google Brain. †Work done while at Google Brain.\n",
            "Datos guardados en CSV: /content/drive/MyDrive/PeerRead-master/data/iclr_primer_ciclo.csv\n"
          ]
        }
      ],
      "source": [
        "from pandas.core.indexing import item_from_zerodim\n",
        "import os,glob\n",
        "import pandas as pd\n",
        "import json,re\n",
        "\n",
        "# Ruta de la carpeta principal\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "lista_part=[\"dev/parsed_pdfs/\",\"test/parsed_pdfs/\",\"train/parsed_pdfs/\"]\n",
        "#lista_conf=[\"acl_2017\",\"arxiv.cs.ai_2007-2017\",\"arxiv.cs.cl_2007-2017\",\"arxiv.cs.lg_2007-2017\",\"conll_2016\",\"iclr_2017\",\"nips_2013-2017\",\"\"]\n",
        "lista_conf=[\"iclr_2017\"]\n",
        "rutas_archivos_json = []\n",
        "secc = {}\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = re.sub(r'\\n\\d+\\n', '\\n', texto)\n",
        "    #texto = re.sub(r'[×/]', '', texto)\n",
        "    #texto = re.sub(r'\\b\\d+\\b', '', texto)\n",
        "    #texto = re.sub(r'\\n+', '\\n', texto).strip()\n",
        "    return texto\n",
        "\n",
        "intro_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(introduction)', re.IGNORECASE)\n",
        "conclusion_pattern = re.compile(r'(?:\\d*\\.?\\s*)?(conclusion)', re.IGNORECASE)\n",
        "\n",
        "for confe in lista_conf:\n",
        "  #print(confe)\n",
        "  for particion in lista_part:\n",
        "    #print(particion)\n",
        "    ruta=(ruta_carpeta_principal+\"/\"+confe+\"/\"+particion)\n",
        "    #print(\"Explorando ruta:\", ruta)\n",
        "    archivos=glob.glob(ruta+\"/*.json\")\n",
        "    #print(archivos)\n",
        "    particion_sin_ruta = particion.strip('/').split('/')[0]\n",
        "\n",
        "    for archivo in archivos:\n",
        "      with open(archivo, 'r') as f:\n",
        "        contenido = json.load(f)\n",
        "        #print(contenido.keys())\n",
        "        nombre=contenido[\"name\"]\n",
        "        diccontent=contenido[\"metadata\"]\n",
        "        #print(diccontent)\n",
        "        t = diccontent[\"title\"]\n",
        "        #print(t)\n",
        "        e = diccontent[\"emails\"]\n",
        "        #print(e)\n",
        "        autores=diccontent[\"authors\"]\n",
        "        #print(autores)\n",
        "        dicsecciones=diccontent[\"sections\"]\n",
        "\n",
        "        referencias_archivo = diccontent[\"references\"]\n",
        "\n",
        "        abstract= diccontent[\"abstractText\"]\n",
        "        #print(dicsecciones)\n",
        "        cadena=\"\"\n",
        "        introduccion=\"\"\n",
        "        conclusion=\"\"\n",
        "        contenido=\"\"\n",
        "\n",
        "\n",
        "        #print(dicsecciones)\n",
        "        if dicsecciones!=None:\n",
        "\n",
        "          for item in dicsecciones:\n",
        "            if item['heading'] == None:\n",
        "                cadena = \"\"\n",
        "            else:\n",
        "                cadena = cadena +\" \"+ item['heading'] + \"\\n\"\n",
        "                limpiar_texto(cadena)\n",
        "                cadena+=\" \"# limpiar_texto(cadena)\n",
        "\n",
        "                # Verificar si el heading coincide con el patron de introduccion y concllu\n",
        "                if re.search(intro_pattern, item['heading']):\n",
        "                  introduccion = item['text']\n",
        "                elif re.search(conclusion_pattern, item['heading']):\n",
        "                  conclusion = item['text']\n",
        "                else:\n",
        "                  contenido += f\"**{item['heading']}**\\n\"+ item['text']+ \"\\n\"\n",
        "\n",
        "\n",
        "            texto=item['text'] #re.sub(r'(\\n\\d+)+\\n', '',item['text'])\n",
        "            re.sub(r'(\\n\\d+)+\\n', '',item['text'])\n",
        "            texto = limpiar_texto(texto)\n",
        "            cadena=cadena +\" \"+ texto+\"\\n\"\n",
        "\n",
        "          rutas_archivos_json.append({\n",
        "                      \"Conferencia\":confe,\n",
        "                      \"Particion\":particion_sin_ruta,\n",
        "                      \"Archivo\": nombre,\n",
        "                      \"Title\": t,\n",
        "                      \"Emails\": e,\n",
        "                      \"Autores\": autores,\n",
        "                      \"Texto Completo\": f\"{t} {cadena} Abstract {abstract}\",\n",
        "                      \"Abstract\": abstract,\n",
        "                      \"Introducción\": introduccion,\n",
        "                      \"Contenido\": contenido,\n",
        "                      \"Conclusion\": conclusion,\n",
        "                      \"Referencias\": referencias_archivo\n",
        "                  })\n",
        "\n",
        "          #if(nombre==\"484.pdf\"):\n",
        "            #print(nombre)\n",
        "            #print(cadena)\n",
        "            #print(introduccion)\n",
        "            #print(contenido)\n",
        "            #print(conclusion)\n",
        "            #print(abstract)\n",
        "\n",
        "\n",
        "        else:\n",
        "          print(\"Error en el archivo: \"+nombre)\n",
        "          print(nombre)\n",
        "          print(cadena)\n",
        "          print(introduccion)\n",
        "          print(contenido)\n",
        "          print(conclusion)\n",
        "          print(abstract)\n",
        "          rutas_archivos_json.append({\n",
        "                      \"Conferencia\":confe,\n",
        "                      \"Particion\":particion_sin_ruta,\n",
        "                      \"Archivo\": nombre,\n",
        "                      \"Title\": t,\n",
        "                      \"Emails\": e,\n",
        "                      \"Autores\": autores,\n",
        "                      \"Texto Completo\": \"\",\n",
        "                      \"Abstract\": abstract,\n",
        "                      \"Introducción\": \"\",\n",
        "                      \"Contenido\": \"\",\n",
        "                      \"Conclusion\": \"\",\n",
        "                      \"Referencias\": referencias_archivo\n",
        "                  })\n",
        "\n",
        "df = pd.DataFrame(rutas_archivos_json)\n",
        "#print(df['Texto Completo'])\n",
        "\n",
        "#fila_seleccionada = df[df['Archivo'] == '484.pdf']\n",
        "\n",
        "# Verificar si la fila fue encontrada\n",
        "#if not fila_seleccionada.empty:\n",
        "    # Obtener el texto completo de la fila seleccionada\n",
        "    #texto_completo_484 = fila_seleccionada['Texto Completo'].values[0]\n",
        "\n",
        "    # Imprimir el texto completo\n",
        "    #print(texto_completo_484)\n",
        "#else:\n",
        "    #print(\"No se encontró el archivo '484.pdf' en la columna 'archivo'.\")\n",
        "\n",
        "df.to_csv(ruta_carpeta_principal+\"/\"+\"iclr_primer_ciclo.csv\", sep=',', index=False, encoding='utf-8')\n",
        "print(\"Datos guardados en CSV:\", ruta_carpeta_principal+\"/\"+\"iclr_primer_ciclo.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Obtener estado de aceptacion"
      ],
      "metadata": {
        "id": "yamKcVKYsjFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "lista_part = [\"dev/reviews/\", \"test/reviews/\", \"train/reviews/\"]\n",
        "lista_conf = [\"iclr_2017\"]\n",
        "csv_path = os.path.join(ruta_carpeta_principal, \"reviews_iclr.csv\")\n",
        "\n",
        "# Crear o cargar el DataFrame desde el archivo CSV existente\n",
        "if os.path.exists(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "else:\n",
        "    df = pd.DataFrame(columns=[\"Archivo\", \"Estado\"])\n",
        "\n",
        "dataframes = []  # Lista para almacenar los DataFrames temporales\n",
        "\n",
        "for confe in lista_conf:\n",
        "    for particion in lista_part:\n",
        "        ruta = os.path.join(ruta_carpeta_principal, confe, particion)\n",
        "        archivos = os.listdir(ruta)\n",
        "\n",
        "        for archivo in archivos:\n",
        "            archivo_path = os.path.join(ruta, archivo)\n",
        "\n",
        "            with open(archivo_path, 'r') as f:\n",
        "                contenido = json.load(f)\n",
        "                id = contenido[\"id\"]\n",
        "                id = id+'.pdf'\n",
        "                accepted = contenido[\"accepted\"]\n",
        "\n",
        "                # Verificar si accepted es un booleano y asignar 1 o 0 en consecuencia\n",
        "                if isinstance(accepted, bool):\n",
        "                    accepted_value = \"1\" if accepted else \"0\"\n",
        "                else:\n",
        "                    accepted_value = \"1\" if accepted.lower() == \"verdadero\" else \"0\"\n",
        "\n",
        "                temp_df = pd.DataFrame({\"Archivo\": [id], \"Accepted\": [accepted_value]})\n",
        "                dataframes.append(temp_df)\n",
        "\n",
        "                #if accepted == \"VERDADERO\":\n",
        "                #    temp_df = pd.DataFrame({\"Archivo\": [id], \"Accepted\": [\"1\"]})\n",
        "                #else:\n",
        "                #    temp_df = pd.DataFrame({\"Archivo\": [id], \"Accepted\": [\"0\"]})\n",
        "\n",
        "                #print(id)\n",
        "                #print(accepted)\n",
        "\n",
        "# Concatenar todos los DataFrames temporales en uno solo\n",
        "if dataframes:\n",
        "    df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Guardar el DataFrame actualizado en el archivo CSV\n",
        "df.to_csv(csv_path, sep=',', index=False, encoding='utf-8')\n",
        "print(\"Datos guardados en CSV:\", csv_path)\n"
      ],
      "metadata": {
        "id": "UXNDc9uoAMGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38655f53-d114-43da-f0f9-1045e26a5c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en CSV: /content/drive/MyDrive/PeerRead-master/data/reviews_iclr.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Combinar csvs"
      ],
      "metadata": {
        "id": "XoBAxfePs-nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Especifica la ruta de tus archivos CSV en tu Google Drive\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "\n",
        "archivo1 = os.path.join(ruta_carpeta_principal, \"reviews_iclr.csv\")\n",
        "archivo2 = os.path.join(ruta_carpeta_principal, \"iclr_primer_ciclo.csv\")\n",
        "\n",
        "# Carga los archivos CSV en DataFrames\n",
        "df1 = pd.read_csv(archivo1)\n",
        "df2 = pd.read_csv(archivo2)\n",
        "\n",
        "# Realiza la unión en base a la columna 'client'\n",
        "result = pd.merge(df1, df2, on='Archivo', how='outer')\n",
        "\n",
        "result.to_csv('/content/drive/MyDrive/iclr_datos_completos.csv', index=False)"
      ],
      "metadata": {
        "id": "2EIMiBLvmWIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Especifica la ruta de tus archivos CSV en tu Google Drive\n",
        "ruta_carpeta_principal = '/content/drive/MyDrive/PeerRead-master/data'\n",
        "\n",
        "archivo1 = os.path.join(ruta_carpeta_principal, \"iclr_primer_ciclo.csv\")\n",
        "archivo2 = os.path.join(ruta_carpeta_principal, \"reviews_iclr.csv\")\n",
        "\n",
        "# Carga los archivos CSV en DataFrames\n",
        "df1 = pd.read_csv(archivo1)\n",
        "df2 = pd.read_csv(archivo2)\n",
        "\n",
        "# Guarda el número inicial de filas en cada DataFrame\n",
        "num_filas_inicial_df1 = len(df1)\n",
        "print(\"Filas del doc 1:\",num_filas_inicial_df1)\n",
        "num_filas_inicial_df2 = len(df2)\n",
        "print(\"Filas del doc 2:\",num_filas_inicial_df2)\n",
        "\n",
        "# Union de csv con base en la columna 'Title'\n",
        "result = pd.merge(df1, df2, on='Archivo', how='outer')\n",
        "print(\"Número total de elementos al unir:\", len(result))\n",
        "\n",
        "# Dropear filas con valores nulos en columnas\n",
        "result = result.dropna(subset=['Accepted'])\n",
        "#result = result.dropna(subset=['Contenido'])\n",
        "print(\"Número total de elementos despues de dropear:\", len(result))\n",
        "\n",
        "#print(result)\n",
        "\n",
        "# Guarda el resultado en un CSV\n",
        "result.to_csv('/content/drive/MyDrive/iclr_datos_completos.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y37WDboVbAm",
        "outputId": "a87fa9b0-b6a0-4111-a50d-31eade166aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filas del doc 1: 427\n",
            "Filas del doc 2: 427\n",
            "Número total de elementos al unir: 427\n",
            "Número total de elementos despues de dropear: 427\n"
          ]
        }
      ]
    }
  ]
}